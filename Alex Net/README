- Install tensorflow and python and dstat on each machine
Single Node
- Before training Alexnet on a single node we have to start the tf server in single mode
    python startserver.py -deploy_mode single
- Now you have to train the Alexnet model
    python -m AlexNet.scripts.train --mode single --dataset <dataset_name> --batch_num <batch_num> --batch_size <batch_size>
    
    dataset_name can be flowers or fake_data
    batch_size can take any numeric value
    
Example : python -m AlexNet.scripts.train --mode single --dataset flowers --batch_num 128 --batch_size 64
    
    
Cluster mode
- In cluster mode , choose one of the nodes as parameter server and then run the command,
    python startserver.py -deploy_mode <cluster_configuration> -job_name <job> --task_index <task_number>
    
    cluster_configuration  can take two values :  cluster2 ( 4 nodes cluster configuration ) , cluster ( 2-node cluster configuration)
    job can take two values : ps ( parameter server ) , worker ( worker node )
    task_number can take any value from 0 to max_nodes -1
Example: python startserver.py -deploy_mode cluster -job_name ps --task_index 0
         python startserver.py -deploy_mode cluster -job_name worker --task_index 0
    
    
- Now train the Alexnet model using 
    python -m AlexNet.scripts.train --mode <cluster_configuration> --batch_size <batch_size> --batch_num <batch_num> --dataset <dataset>
    
Example: python -m AlexNet.scripts.train --mode cluster2 --batch_size 32 --batch_num 500 --dataset flowers
Data collection
    We collected CPU and memory usage using 
        dstat --time --cpu --mem --load --net --output <file_name>
We have tried to train Alexnet on GPU using flowers dataset as well.But it was crashing for all batch sizes after some 35 iterations and was showing memory error. When we checked the GPU memory usage , we could see it running on 100%. Unfortunately we did not try this experiment with fake_data.