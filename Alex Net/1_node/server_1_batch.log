018-10-15 17:32:26.176182: step 1648, loss = 3.37 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:32:28.298134: step 1649, loss = 3.31 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:32:30.404930: step 1650, loss = 3.70 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:32:32.534260: step 1651, loss = 3.37 (0.5 examples/sec; 2.125 sec/batch)
2018-10-15 17:32:34.700279: step 1652, loss = 3.75 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:32:36.817002: step 1653, loss = 3.37 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:32:38.983189: step 1654, loss = 3.37 (0.5 examples/sec; 2.164 sec/batch)
2018-10-15 17:32:41.100257: step 1655, loss = 3.37 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:32:43.269211: step 1656, loss = 3.69 (0.5 examples/sec; 2.165 sec/batch)
2018-10-15 17:32:45.370173: step 1657, loss = 3.69 (0.5 examples/sec; 2.097 sec/batch)
2018-10-15 17:32:47.493064: step 1658, loss = 3.69 (0.5 examples/sec; 2.119 sec/batch)
2018-10-15 17:32:49.592418: step 1659, loss = 3.36 (0.5 examples/sec; 2.097 sec/batch)
2018-10-15 17:32:51.677463: step 1660, loss = 3.31 (0.5 examples/sec; 2.081 sec/batch)
2018-10-15 17:32:53.808059: step 1661, loss = 3.36 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:32:55.955690: step 1662, loss = 3.37 (0.5 examples/sec; 2.143 sec/batch)
2018-10-15 17:32:58.105403: step 1663, loss = 3.32 (0.5 examples/sec; 2.145 sec/batch)
2018-10-15 17:33:00.261356: step 1664, loss = 3.75 (0.5 examples/sec; 2.153 sec/batch)
2018-10-15 17:33:02.415032: step 1665, loss = 3.32 (0.5 examples/sec; 2.149 sec/batch)
2018-10-15 17:33:04.525047: step 1666, loss = 3.36 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:33:06.668249: step 1667, loss = 3.37 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:33:08.788036: step 1668, loss = 3.35 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:33:10.936935: step 1669, loss = 3.35 (0.5 examples/sec; 2.145 sec/batch)
2018-10-15 17:33:13.059089: step 1670, loss = 3.37 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:33:15.223088: step 1671, loss = 3.32 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:33:17.342484: step 1672, loss = 3.34 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:33:19.504010: step 1673, loss = 3.35 (0.5 examples/sec; 2.157 sec/batch)
2018-10-15 17:33:21.660243: step 1674, loss = 3.34 (0.5 examples/sec; 2.152 sec/batch)
2018-10-15 17:33:23.795650: step 1675, loss = 3.35 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:33:25.940249: step 1676, loss = 3.32 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:33:28.100247: step 1677, loss = 3.33 (0.5 examples/sec; 2.155 sec/batch)
2018-10-15 17:33:30.244100: step 1678, loss = 3.33 (0.5 examples/sec; 2.140 sec/batch)
2018-10-15 17:33:32.400402: step 1679, loss = 3.33 (0.5 examples/sec; 2.151 sec/batch)
2018-10-15 17:33:34.527630: step 1680, loss = 3.32 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:33:36.729950: step 1681, loss = 3.34 (0.5 examples/sec; 2.198 sec/batch)
2018-10-15 17:33:38.870496: step 1682, loss = 3.30 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:33:40.977419: step 1683, loss = 3.28 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:33:43.164143: step 1684, loss = 3.85 (0.5 examples/sec; 2.183 sec/batch)
2018-10-15 17:33:45.320532: step 1685, loss = 3.71 (0.5 examples/sec; 2.152 sec/batch)
2018-10-15 17:33:47.448157: step 1686, loss = 3.72 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:33:49.551528: step 1687, loss = 3.29 (0.5 examples/sec; 2.099 sec/batch)
2018-10-15 17:33:51.663118: step 1688, loss = 3.84 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:33:53.776954: step 1689, loss = 3.37 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:33:55.921951: step 1690, loss = 3.21 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:33:58.050075: step 1691, loss = 3.71 (0.5 examples/sec; 2.125 sec/batch)
2018-10-15 17:34:00.219751: step 1692, loss = 3.36 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:34:02.338257: step 1693, loss = 3.38 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:34:04.510049: step 1694, loss = 3.39 (0.5 examples/sec; 2.168 sec/batch)
2018-10-15 17:34:06.635187: step 1695, loss = 3.37 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:34:08.785548: step 1696, loss = 3.17 (0.5 examples/sec; 2.146 sec/batch)
2018-10-15 17:34:10.915601: step 1697, loss = 3.37 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:34:13.076507: step 1698, loss = 3.36 (0.5 examples/sec; 2.157 sec/batch)
2018-10-15 17:34:15.194536: step 1699, loss = 3.36 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:34:17.363198: step 1700, loss = 3.38 (0.5 examples/sec; 2.164 sec/batch)
2018-10-15 17:34:20.078087: step 1701, loss = 3.27 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:34:22.217985: step 1702, loss = 3.37 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:34:24.308277: step 1703, loss = 3.27 (0.5 examples/sec; 2.087 sec/batch)
2018-10-15 17:34:26.466138: step 1704, loss = 3.14 (0.5 examples/sec; 2.156 sec/batch)
2018-10-15 17:34:28.597165: step 1705, loss = 3.38 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:34:30.763275: step 1706, loss = 3.15 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:34:32.895656: step 1707, loss = 3.08 (0.5 examples/sec; 2.128 sec/batch)
2018-10-15 17:34:35.079548: step 1708, loss = 3.36 (0.5 examples/sec; 2.180 sec/batch)
2018-10-15 17:34:37.187025: step 1709, loss = 3.33 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:34:39.290436: step 1710, loss = 3.36 (0.5 examples/sec; 2.099 sec/batch)
2018-10-15 17:34:41.408078: step 1711, loss = 4.23 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:34:43.563386: step 1712, loss = 2.93 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:34:45.696543: step 1713, loss = 3.43 (0.5 examples/sec; 2.128 sec/batch)
2018-10-15 17:34:47.891343: step 1714, loss = 4.12 (0.5 examples/sec; 2.190 sec/batch)
2018-10-15 17:34:50.006398: step 1715, loss = 4.12 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:34:52.119331: step 1716, loss = 3.40 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:34:54.229108: step 1717, loss = 3.37 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:34:56.354948: step 1718, loss = 3.35 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:34:58.521434: step 1719, loss = 4.04 (0.5 examples/sec; 2.163 sec/batch)
2018-10-15 17:35:00.639180: step 1720, loss = 3.30 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:35:02.815738: step 1721, loss = 3.84 (0.5 examples/sec; 2.173 sec/batch)
2018-10-15 17:35:04.936426: step 1722, loss = 3.82 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:35:07.044956: step 1723, loss = 3.74 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:35:09.201010: step 1724, loss = 3.34 (0.5 examples/sec; 2.152 sec/batch)
2018-10-15 17:35:11.384605: step 1725, loss = 3.34 (0.5 examples/sec; 2.180 sec/batch)
2018-10-15 17:35:13.541713: step 1726, loss = 3.27 (0.5 examples/sec; 2.153 sec/batch)
2018-10-15 17:35:15.671535: step 1727, loss = 3.33 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:35:17.760719: step 1728, loss = 3.27 (0.5 examples/sec; 2.085 sec/batch)
2018-10-15 17:35:19.860386: step 1729, loss = 3.27 (0.5 examples/sec; 2.096 sec/batch)
2018-10-15 17:35:22.004274: step 1730, loss = 3.73 (0.5 examples/sec; 2.140 sec/batch)
2018-10-15 17:35:24.115481: step 1731, loss = 3.32 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:35:26.267142: step 1732, loss = 3.73 (0.5 examples/sec; 2.148 sec/batch)
2018-10-15 17:35:28.376648: step 1733, loss = 3.32 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:35:30.522149: step 1734, loss = 3.27 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:35:32.656745: step 1735, loss = 3.84 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:35:34.835629: step 1736, loss = 3.27 (0.5 examples/sec; 2.175 sec/batch)
2018-10-15 17:35:36.953565: step 1737, loss = 3.27 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:35:39.106937: step 1738, loss = 3.38 (0.5 examples/sec; 2.149 sec/batch)
2018-10-15 17:35:41.232855: step 1739, loss = 3.72 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:35:43.351926: step 1740, loss = 3.84 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:35:45.554594: step 1741, loss = 3.71 (0.5 examples/sec; 2.198 sec/batch)
2018-10-15 17:35:47.663744: step 1742, loss = 3.26 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:35:49.780740: step 1743, loss = 3.39 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:35:51.879865: step 1744, loss = 3.40 (0.5 examples/sec; 2.096 sec/batch)
2018-10-15 17:35:54.002619: step 1745, loss = 3.26 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:35:56.119278: step 1746, loss = 3.39 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:35:58.317622: step 1747, loss = 3.25 (0.5 examples/sec; 2.194 sec/batch)
2018-10-15 17:36:00.458686: step 1748, loss = 3.39 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:36:02.562932: step 1749, loss = 3.84 (0.5 examples/sec; 2.100 sec/batch)
2018-10-15 17:36:04.672163: step 1750, loss = 3.39 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:36:06.795584: step 1751, loss = 3.25 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:36:08.960941: step 1752, loss = 3.25 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:36:11.058326: step 1753, loss = 3.71 (0.5 examples/sec; 2.094 sec/batch)
2018-10-15 17:36:13.220290: step 1754, loss = 3.24 (0.5 examples/sec; 2.158 sec/batch)
2018-10-15 17:36:15.352821: step 1755, loss = 3.71 (0.5 examples/sec; 2.128 sec/batch)
2018-10-15 17:36:17.496268: step 1756, loss = 3.38 (0.5 examples/sec; 2.139 sec/batch)
2018-10-15 17:36:19.624743: step 1757, loss = 3.38 (0.5 examples/sec; 2.124 sec/batch)
2018-10-15 17:36:21.798679: step 1758, loss = 3.36 (0.5 examples/sec; 2.170 sec/batch)
2018-10-15 17:36:23.915021: step 1759, loss = 3.23 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:36:26.026584: step 1760, loss = 3.23 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:36:28.182557: step 1761, loss = 3.37 (0.5 examples/sec; 2.152 sec/batch)
2018-10-15 17:36:30.285402: step 1762, loss = 3.86 (0.5 examples/sec; 2.099 sec/batch)
2018-10-15 17:36:32.404655: step 1763, loss = 3.37 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:36:34.524761: step 1764, loss = 3.22 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:36:36.632346: step 1765, loss = 3.37 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:36:38.771157: step 1766, loss = 3.36 (0.5 examples/sec; 2.135 sec/batch)
2018-10-15 17:36:40.947242: step 1767, loss = 3.22 (0.5 examples/sec; 2.172 sec/batch)
2018-10-15 17:36:43.074748: step 1768, loss = 3.21 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:36:45.198172: step 1769, loss = 3.21 (0.5 examples/sec; 2.119 sec/batch)
2018-10-15 17:36:47.322312: step 1770, loss = 3.39 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:36:49.434973: step 1771, loss = 3.35 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:36:51.569170: step 1772, loss = 3.20 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:36:53.674424: step 1773, loss = 3.73 (0.5 examples/sec; 2.101 sec/batch)
2018-10-15 17:36:55.841570: step 1774, loss = 3.39 (0.5 examples/sec; 2.164 sec/batch)
2018-10-15 17:36:57.966910: step 1775, loss = 3.87 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:37:00.144479: step 1776, loss = 3.35 (0.5 examples/sec; 2.173 sec/batch)
2018-10-15 17:37:02.321224: step 1777, loss = 3.35 (0.5 examples/sec; 2.172 sec/batch)
2018-10-15 17:37:04.435774: step 1778, loss = 3.40 (0.5 examples/sec; 2.111 sec/batch)
2018-10-15 17:37:06.597309: step 1779, loss = 3.34 (0.5 examples/sec; 2.157 sec/batch)
2018-10-15 17:37:08.726038: step 1780, loss = 3.74 (0.5 examples/sec; 2.125 sec/batch)
2018-10-15 17:37:10.886426: step 1781, loss = 3.34 (0.5 examples/sec; 2.156 sec/batch)
2018-10-15 17:37:13.027865: step 1782, loss = 3.88 (0.5 examples/sec; 2.139 sec/batch)
2018-10-15 17:37:15.157113: step 1783, loss = 3.33 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:37:17.307289: step 1784, loss = 3.20 (0.5 examples/sec; 2.148 sec/batch)
2018-10-15 17:37:19.447893: step 1785, loss = 3.20 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:37:21.589649: step 1786, loss = 3.20 (0.5 examples/sec; 2.139 sec/batch)
2018-10-15 17:37:23.767937: step 1787, loss = 3.20 (0.5 examples/sec; 2.174 sec/batch)
2018-10-15 17:37:25.893777: step 1788, loss = 3.41 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:37:28.034923: step 1789, loss = 3.74 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:37:30.168693: step 1790, loss = 3.88 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:37:32.275906: step 1791, loss = 3.88 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:37:34.429530: step 1792, loss = 3.19 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:37:36.572488: step 1793, loss = 3.19 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:37:38.704844: step 1794, loss = 3.32 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:37:40.906569: step 1795, loss = 3.32 (0.5 examples/sec; 2.198 sec/batch)
2018-10-15 17:37:43.021919: step 1796, loss = 3.42 (0.5 examples/sec; 2.111 sec/batch)
2018-10-15 17:37:45.204801: step 1797, loss = 3.18 (0.5 examples/sec; 2.179 sec/batch)
2018-10-15 17:37:47.344199: step 1798, loss = 3.32 (0.5 examples/sec; 2.135 sec/batch)
2018-10-15 17:37:49.512980: step 1799, loss = 3.32 (0.5 examples/sec; 2.165 sec/batch)
2018-10-15 17:37:51.668830: step 1800, loss = 3.76 (0.5 examples/sec; 2.152 sec/batch)
2018-10-15 17:37:54.302583: step 1801, loss = 3.76 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:37:56.432680: step 1802, loss = 3.88 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:37:58.543619: step 1803, loss = 3.31 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:38:00.666137: step 1804, loss = 3.18 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:38:02.802564: step 1805, loss = 3.44 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:38:04.972918: step 1806, loss = 3.44 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:38:07.105396: step 1807, loss = 3.30 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:38:09.256187: step 1808, loss = 3.30 (0.5 examples/sec; 2.147 sec/batch)
2018-10-15 17:38:11.378796: step 1809, loss = 3.30 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:38:13.543494: step 1810, loss = 3.75 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:38:15.660305: step 1811, loss = 3.29 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:38:17.780215: step 1812, loss = 3.75 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:38:19.899085: step 1813, loss = 3.88 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:38:22.019824: step 1814, loss = 3.19 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:38:24.146343: step 1815, loss = 3.20 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:38:26.291036: step 1816, loss = 3.27 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:38:28.462025: step 1817, loss = 3.27 (0.5 examples/sec; 2.167 sec/batch)
2018-10-15 17:38:30.602993: step 1818, loss = 3.46 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:38:32.698587: step 1819, loss = 3.46 (0.5 examples/sec; 2.092 sec/batch)
2018-10-15 17:38:34.816032: step 1820, loss = 3.20 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:38:36.913712: step 1821, loss = 3.26 (0.5 examples/sec; 2.094 sec/batch)
2018-10-15 17:38:39.075312: step 1822, loss = 3.89 (0.5 examples/sec; 2.158 sec/batch)
2018-10-15 17:38:41.212218: step 1823, loss = 3.26 (0.5 examples/sec; 2.133 sec/batch)
2018-10-15 17:38:43.322121: step 1824, loss = 3.20 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:38:45.441023: step 1825, loss = 3.46 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:38:47.562027: step 1826, loss = 3.20 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:38:49.717262: step 1827, loss = 3.20 (0.5 examples/sec; 2.151 sec/batch)
2018-10-15 17:38:51.831540: step 1828, loss = 3.89 (0.5 examples/sec; 2.110 sec/batch)
2018-10-15 17:38:53.948541: step 1829, loss = 3.20 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:38:56.066619: step 1830, loss = 3.20 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:38:58.168789: step 1831, loss = 3.77 (0.5 examples/sec; 2.098 sec/batch)
2018-10-15 17:39:00.305251: step 1832, loss = 3.19 (0.5 examples/sec; 2.133 sec/batch)
2018-10-15 17:39:02.457986: step 1833, loss = 3.25 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:39:04.566447: step 1834, loss = 3.89 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:39:06.714527: step 1835, loss = 3.25 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:39:08.852384: step 1836, loss = 3.18 (0.5 examples/sec; 2.134 sec/batch)
2018-10-15 17:39:11.028166: step 1837, loss = 3.89 (0.5 examples/sec; 2.171 sec/batch)
2018-10-15 17:39:13.161445: step 1838, loss = 3.78 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:39:15.274763: step 1839, loss = 3.78 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:39:17.392541: step 1840, loss = 3.17 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:39:19.505290: step 1841, loss = 3.17 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:39:21.668105: step 1842, loss = 3.49 (0.5 examples/sec; 2.159 sec/batch)
2018-10-15 17:39:23.768036: step 1843, loss = 3.16 (0.5 examples/sec; 2.097 sec/batch)
2018-10-15 17:39:25.897840: step 1844, loss = 3.88 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:39:28.028451: step 1845, loss = 3.50 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:39:30.160753: step 1846, loss = 3.77 (0.5 examples/sec; 2.128 sec/batch)
2018-10-15 17:39:32.325664: step 1847, loss = 3.88 (0.5 examples/sec; 2.161 sec/batch)
2018-10-15 17:39:34.447589: step 1848, loss = 3.87 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:39:36.555918: step 1849, loss = 3.77 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:39:38.671987: step 1850, loss = 3.28 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:39:40.794435: step 1851, loss = 3.86 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:39:42.956876: step 1852, loss = 3.76 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:39:45.083076: step 1853, loss = 3.15 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:39:47.213599: step 1854, loss = 3.15 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:39:49.323503: step 1855, loss = 3.15 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:39:51.439877: step 1856, loss = 3.85 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:39:53.616367: step 1857, loss = 3.75 (0.5 examples/sec; 2.173 sec/batch)
2018-10-15 17:39:55.733562: step 1858, loss = 3.51 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:39:57.857845: step 1859, loss = 3.75 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:39:59.985050: step 1860, loss = 3.51 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:40:02.112687: step 1861, loss = 3.51 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:40:04.315320: step 1862, loss = 3.15 (0.5 examples/sec; 2.199 sec/batch)
2018-10-15 17:40:06.442550: step 1863, loss = 3.83 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:40:08.601549: step 1864, loss = 3.73 (0.5 examples/sec; 2.155 sec/batch)
2018-10-15 17:40:10.715402: step 1865, loss = 3.51 (0.5 examples/sec; 2.110 sec/batch)
2018-10-15 17:40:12.874315: step 1866, loss = 3.15 (0.5 examples/sec; 2.155 sec/batch)
2018-10-15 17:40:15.014385: step 1867, loss = 3.33 (0.5 examples/sec; 2.135 sec/batch)
2018-10-15 17:40:17.143753: step 1868, loss = 3.50 (0.5 examples/sec; 2.125 sec/batch)
2018-10-15 17:40:19.264563: step 1869, loss = 3.73 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:40:21.384623: step 1870, loss = 3.34 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:40:23.522417: step 1871, loss = 3.34 (0.5 examples/sec; 2.134 sec/batch)
2018-10-15 17:40:25.661056: step 1872, loss = 3.34 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:40:27.813029: step 1873, loss = 3.49 (0.5 examples/sec; 2.148 sec/batch)
2018-10-15 17:40:29.929377: step 1874, loss = 3.49 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:40:32.059631: step 1875, loss = 3.16 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:40:34.195329: step 1876, loss = 3.83 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:40:36.307322: step 1877, loss = 3.33 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:40:38.453952: step 1878, loss = 3.33 (0.5 examples/sec; 2.143 sec/batch)
2018-10-15 17:40:40.582458: step 1879, loss = 3.83 (0.5 examples/sec; 2.125 sec/batch)
2018-10-15 17:40:42.725433: step 1880, loss = 3.17 (0.5 examples/sec; 2.138 sec/batch)
2018-10-15 17:40:44.891250: step 1881, loss = 3.32 (0.5 examples/sec; 2.161 sec/batch)
2018-10-15 17:40:47.026529: step 1882, loss = 3.72 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:40:49.134528: step 1883, loss = 3.48 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:40:51.243802: step 1884, loss = 3.18 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:40:53.367002: step 1885, loss = 3.72 (0.5 examples/sec; 2.119 sec/batch)
2018-10-15 17:40:55.483069: step 1886, loss = 3.72 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:40:57.647378: step 1887, loss = 3.18 (0.5 examples/sec; 2.161 sec/batch)
2018-10-15 17:40:59.771910: step 1888, loss = 3.71 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:41:01.895217: step 1889, loss = 3.18 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:41:04.020201: step 1890, loss = 3.48 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:41:06.142564: step 1891, loss = 3.18 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:41:08.275474: step 1892, loss = 3.83 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:41:10.399256: step 1893, loss = 3.83 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:41:12.573446: step 1894, loss = 3.33 (0.5 examples/sec; 2.170 sec/batch)
2018-10-15 17:41:14.702078: step 1895, loss = 3.33 (0.5 examples/sec; 2.124 sec/batch)
2018-10-15 17:41:16.822149: step 1896, loss = 3.17 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:41:18.939241: step 1897, loss = 3.70 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:41:21.057254: step 1898, loss = 3.49 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:41:23.171606: step 1899, loss = 3.33 (0.5 examples/sec; 2.110 sec/batch)
2018-10-15 17:41:25.279631: step 1900, loss = 3.49 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:41:27.863594: step 1901, loss = 3.82 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:41:29.976236: step 1902, loss = 3.49 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:41:32.087284: step 1903, loss = 3.49 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:41:34.182446: step 1904, loss = 3.48 (0.5 examples/sec; 2.091 sec/batch)
2018-10-15 17:41:36.293953: step 1905, loss = 3.70 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:41:38.405649: step 1906, loss = 3.33 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:41:40.577923: step 1907, loss = 3.19 (0.5 examples/sec; 2.168 sec/batch)
2018-10-15 17:41:42.729478: step 1908, loss = 3.19 (0.5 examples/sec; 2.148 sec/batch)
2018-10-15 17:41:44.855783: step 1909, loss = 3.33 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:41:46.977133: step 1910, loss = 3.19 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:41:49.086412: step 1911, loss = 3.46 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:41:51.192860: step 1912, loss = 3.33 (0.5 examples/sec; 2.102 sec/batch)
2018-10-15 17:41:53.313331: step 1913, loss = 3.33 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:41:55.438850: step 1914, loss = 3.83 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:41:57.563705: step 1915, loss = 3.19 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:41:59.736408: step 1916, loss = 3.19 (0.5 examples/sec; 2.169 sec/batch)
2018-10-15 17:42:01.854126: step 1917, loss = 3.19 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:42:03.970695: step 1918, loss = 3.71 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:42:06.058748: step 1919, loss = 3.71 (0.5 examples/sec; 2.084 sec/batch)
2018-10-15 17:42:08.168493: step 1920, loss = 3.18 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:42:10.293375: step 1921, loss = 3.71 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:42:12.468712: step 1922, loss = 3.47 (0.5 examples/sec; 2.171 sec/batch)
2018-10-15 17:42:14.586444: step 1923, loss = 3.18 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:42:16.701140: step 1924, loss = 3.70 (0.5 examples/sec; 2.111 sec/batch)
2018-10-15 17:42:18.805054: step 1925, loss = 3.70 (0.5 examples/sec; 2.100 sec/batch)
2018-10-15 17:42:20.939494: step 1926, loss = 3.84 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:42:23.120630: step 1927, loss = 3.47 (0.5 examples/sec; 2.177 sec/batch)
2018-10-15 17:42:25.254022: step 1928, loss = 3.69 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:42:27.431499: step 1929, loss = 3.34 (0.5 examples/sec; 2.173 sec/batch)
2018-10-15 17:42:29.565274: step 1930, loss = 3.34 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:42:31.745625: step 1931, loss = 3.17 (0.5 examples/sec; 2.176 sec/batch)
2018-10-15 17:42:33.886418: step 1932, loss = 3.67 (0.5 examples/sec; 2.138 sec/batch)
2018-10-15 17:42:36.014997: step 1933, loss = 3.85 (0.5 examples/sec; 2.124 sec/batch)
2018-10-15 17:42:38.113448: step 1934, loss = 3.34 (0.5 examples/sec; 2.095 sec/batch)
2018-10-15 17:42:40.243341: step 1935, loss = 3.48 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:42:42.431166: step 1936, loss = 3.18 (0.5 examples/sec; 2.183 sec/batch)
2018-10-15 17:42:44.555922: step 1937, loss = 3.18 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:42:46.678482: step 1938, loss = 3.48 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:42:48.799745: step 1939, loss = 3.48 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:42:50.938493: step 1940, loss = 3.35 (0.5 examples/sec; 2.135 sec/batch)
2018-10-15 17:42:53.065552: step 1941, loss = 3.35 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:42:55.229864: step 1942, loss = 3.34 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:42:57.378354: step 1943, loss = 3.18 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:42:59.490532: step 1944, loss = 3.47 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:43:01.600004: step 1945, loss = 3.34 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:43:03.732622: step 1946, loss = 3.18 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:43:05.862303: step 1947, loss = 3.67 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:43:08.032114: step 1948, loss = 3.33 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:43:10.167719: step 1949, loss = 3.18 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:43:12.271576: step 1950, loss = 3.18 (0.5 examples/sec; 2.100 sec/batch)
2018-10-15 17:43:14.376770: step 1951, loss = 3.67 (0.5 examples/sec; 2.101 sec/batch)
2018-10-15 17:43:16.512649: step 1952, loss = 3.18 (0.5 examples/sec; 2.133 sec/batch)
2018-10-15 17:43:18.637605: step 1953, loss = 3.87 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:43:20.755792: step 1954, loss = 3.67 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:43:22.862638: step 1955, loss = 3.67 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:43:24.990867: step 1956, loss = 3.17 (0.5 examples/sec; 2.124 sec/batch)
2018-10-15 17:43:27.167703: step 1957, loss = 3.17 (0.5 examples/sec; 2.173 sec/batch)
2018-10-15 17:43:29.317010: step 1958, loss = 3.16 (0.5 examples/sec; 2.145 sec/batch)
2018-10-15 17:43:31.428228: step 1959, loss = 3.33 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:43:33.512653: step 1960, loss = 3.33 (0.5 examples/sec; 2.080 sec/batch)
2018-10-15 17:43:35.638408: step 1961, loss = 3.33 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:43:37.758116: step 1962, loss = 3.50 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:43:39.912721: step 1963, loss = 3.89 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:43:42.048530: step 1964, loss = 3.15 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:43:44.224444: step 1965, loss = 3.33 (0.5 examples/sec; 2.171 sec/batch)
2018-10-15 17:43:46.399327: step 1966, loss = 3.89 (0.5 examples/sec; 2.171 sec/batch)
2018-10-15 17:43:48.500399: step 1967, loss = 3.89 (0.5 examples/sec; 2.097 sec/batch)
2018-10-15 17:43:50.619271: step 1968, loss = 3.88 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:43:52.754671: step 1969, loss = 3.51 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:43:54.875017: step 1970, loss = 3.87 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:43:57.009107: step 1971, loss = 3.15 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:43:59.172178: step 1972, loss = 3.51 (0.5 examples/sec; 2.161 sec/batch)
2018-10-15 17:44:01.293734: step 1973, loss = 3.51 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:44:03.477029: step 1974, loss = 3.32 (0.5 examples/sec; 2.179 sec/batch)
2018-10-15 17:44:05.638738: step 1975, loss = 3.68 (0.5 examples/sec; 2.158 sec/batch)
2018-10-15 17:44:07.759110: step 1976, loss = 3.16 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:44:09.868789: step 1977, loss = 3.68 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:44:11.995083: step 1978, loss = 3.16 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:44:14.160684: step 1979, loss = 3.33 (0.5 examples/sec; 2.161 sec/batch)
2018-10-15 17:44:16.320097: step 1980, loss = 3.51 (0.5 examples/sec; 2.155 sec/batch)
2018-10-15 17:44:18.492882: step 1981, loss = 3.16 (0.5 examples/sec; 2.169 sec/batch)
2018-10-15 17:44:20.628881: step 1982, loss = 3.85 (0.5 examples/sec; 2.134 sec/batch)
2018-10-15 17:44:22.818346: step 1983, loss = 3.51 (0.5 examples/sec; 2.185 sec/batch)
2018-10-15 17:44:24.990598: step 1984, loss = 3.85 (0.5 examples/sec; 2.168 sec/batch)
2018-10-15 17:44:27.116664: step 1985, loss = 3.15 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:44:29.263168: step 1986, loss = 3.84 (0.5 examples/sec; 2.143 sec/batch)
2018-10-15 17:44:31.386298: step 1987, loss = 3.34 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:44:33.506710: step 1988, loss = 3.15 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:44:35.667142: step 1989, loss = 3.16 (0.5 examples/sec; 2.157 sec/batch)
2018-10-15 17:44:37.781282: step 1990, loss = 3.34 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:44:39.887252: step 1991, loss = 3.34 (0.5 examples/sec; 2.102 sec/batch)
2018-10-15 17:44:41.999537: step 1992, loss = 3.15 (0.5 examples/sec; 2.110 sec/batch)
2018-10-15 17:44:44.172621: step 1993, loss = 3.34 (0.5 examples/sec; 2.169 sec/batch)
2018-10-15 17:44:46.328828: step 1994, loss = 3.51 (0.5 examples/sec; 2.152 sec/batch)
2018-10-15 17:44:48.460076: step 1995, loss = 3.33 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:44:50.614465: step 1996, loss = 3.70 (0.5 examples/sec; 2.152 sec/batch)
2018-10-15 17:44:52.771279: step 1997, loss = 3.33 (0.5 examples/sec; 2.153 sec/batch)
2018-10-15 17:44:54.914968: step 1998, loss = 3.33 (0.5 examples/sec; 2.139 sec/batch)
2018-10-15 17:44:57.079146: step 1999, loss = 3.83 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:44:59.248638: step 2000, loss = 3.83 (0.5 examples/sec; 2.165 sec/batch)
2018-10-15 17:45:01.878174: step 2001, loss = 3.15 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:45:04.030981: step 2002, loss = 3.15 (0.5 examples/sec; 2.149 sec/batch)
2018-10-15 17:45:06.170676: step 2003, loss = 3.31 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:45:08.330966: step 2004, loss = 3.30 (0.5 examples/sec; 2.156 sec/batch)
2018-10-15 17:45:10.479128: step 2005, loss = 3.82 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:45:12.601358: step 2006, loss = 3.15 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:45:14.797430: step 2007, loss = 3.82 (0.5 examples/sec; 2.194 sec/batch)
2018-10-15 17:45:16.961730: step 2008, loss = 3.72 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:45:19.100521: step 2009, loss = 3.81 (0.5 examples/sec; 2.134 sec/batch)
2018-10-15 17:45:21.263981: step 2010, loss = 3.29 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:45:23.408207: step 2011, loss = 3.55 (0.5 examples/sec; 2.142 sec/batch)
2018-10-15 17:45:25.539564: step 2012, loss = 3.80 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:45:27.672985: step 2013, loss = 3.16 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:45:29.844875: step 2014, loss = 3.79 (0.5 examples/sec; 2.169 sec/batch)
2018-10-15 17:45:32.015422: step 2015, loss = 3.16 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:45:34.173165: step 2016, loss = 3.73 (0.5 examples/sec; 2.153 sec/batch)
2018-10-15 17:45:36.291752: step 2017, loss = 3.29 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:45:38.453094: step 2018, loss = 3.78 (0.5 examples/sec; 2.157 sec/batch)
2018-10-15 17:45:40.572920: step 2019, loss = 3.77 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:45:42.716172: step 2020, loss = 3.29 (0.5 examples/sec; 2.139 sec/batch)
2018-10-15 17:45:44.857215: step 2021, loss = 3.29 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:45:46.965679: step 2022, loss = 3.16 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:45:49.070491: step 2023, loss = 3.28 (0.5 examples/sec; 2.101 sec/batch)
2018-10-15 17:45:51.285713: step 2024, loss = 3.75 (0.5 examples/sec; 2.211 sec/batch)
2018-10-15 17:45:53.420632: step 2025, loss = 3.28 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:45:55.568255: step 2026, loss = 3.17 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:45:57.709237: step 2027, loss = 3.17 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:45:59.854426: step 2028, loss = 3.59 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:46:01.985407: step 2029, loss = 3.27 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:46:04.093106: step 2030, loss = 3.75 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:46:06.262491: step 2031, loss = 3.17 (0.5 examples/sec; 2.165 sec/batch)
2018-10-15 17:46:08.383140: step 2032, loss = 3.74 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:46:10.547249: step 2033, loss = 3.74 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:46:12.672914: step 2034, loss = 3.60 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:46:14.826972: step 2035, loss = 3.73 (0.5 examples/sec; 2.152 sec/batch)
2018-10-15 17:46:17.005568: step 2036, loss = 3.17 (0.5 examples/sec; 2.174 sec/batch)
2018-10-15 17:46:19.144872: step 2037, loss = 3.17 (0.5 examples/sec; 2.138 sec/batch)
2018-10-15 17:46:21.307939: step 2038, loss = 3.26 (0.5 examples/sec; 2.159 sec/batch)
2018-10-15 17:46:23.451653: step 2039, loss = 3.76 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:46:25.623242: step 2040, loss = 3.27 (0.5 examples/sec; 2.168 sec/batch)
2018-10-15 17:46:27.792416: step 2041, loss = 3.71 (0.5 examples/sec; 2.167 sec/batch)
2018-10-15 17:46:29.950883: step 2042, loss = 3.28 (0.5 examples/sec; 2.154 sec/batch)
2018-10-15 17:46:32.121356: step 2043, loss = 3.17 (0.5 examples/sec; 2.168 sec/batch)
2018-10-15 17:46:34.285578: step 2044, loss = 3.26 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:46:36.408217: step 2045, loss = 3.26 (0.5 examples/sec; 2.119 sec/batch)
2018-10-15 17:46:38.558613: step 2046, loss = 3.17 (0.5 examples/sec; 2.147 sec/batch)
2018-10-15 17:46:40.691993: step 2047, loss = 3.17 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:46:42.827605: step 2048, loss = 3.25 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:46:44.951398: step 2049, loss = 3.62 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:46:47.093417: step 2050, loss = 3.24 (0.5 examples/sec; 2.138 sec/batch)
2018-10-15 17:46:49.218100: step 2051, loss = 3.63 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:46:51.355373: step 2052, loss = 3.71 (0.5 examples/sec; 2.133 sec/batch)
2018-10-15 17:46:53.483451: step 2053, loss = 3.63 (0.5 examples/sec; 2.124 sec/batch)
2018-10-15 17:46:55.626803: step 2054, loss = 3.78 (0.5 examples/sec; 2.140 sec/batch)
2018-10-15 17:46:57.770355: step 2055, loss = 3.17 (0.5 examples/sec; 2.140 sec/batch)
2018-10-15 17:46:59.882844: step 2056, loss = 3.78 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:47:01.990125: step 2057, loss = 3.62 (0.5 examples/sec; 2.102 sec/batch)
2018-10-15 17:47:04.068852: step 2058, loss = 3.78 (0.5 examples/sec; 2.075 sec/batch)
2018-10-15 17:47:06.228143: step 2059, loss = 3.17 (0.5 examples/sec; 2.155 sec/batch)
2018-10-15 17:47:08.340297: step 2060, loss = 3.24 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:47:10.493405: step 2061, loss = 3.71 (0.5 examples/sec; 2.149 sec/batch)
2018-10-15 17:47:12.615076: step 2062, loss = 3.71 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:47:14.766845: step 2063, loss = 3.17 (0.5 examples/sec; 2.148 sec/batch)
2018-10-15 17:47:16.904709: step 2064, loss = 3.71 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:47:19.070873: step 2065, loss = 3.24 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:47:21.233253: step 2066, loss = 3.18 (0.5 examples/sec; 2.158 sec/batch)
2018-10-15 17:47:23.376602: step 2067, loss = 3.17 (0.5 examples/sec; 2.142 sec/batch)
2018-10-15 17:47:25.522699: step 2068, loss = 3.24 (0.5 examples/sec; 2.142 sec/batch)
2018-10-15 17:47:27.642430: step 2069, loss = 3.70 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:47:29.745616: step 2070, loss = 3.24 (0.5 examples/sec; 2.101 sec/batch)
2018-10-15 17:47:31.854064: step 2071, loss = 3.17 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:47:34.011952: step 2072, loss = 3.24 (0.5 examples/sec; 2.154 sec/batch)
2018-10-15 17:47:36.151984: step 2073, loss = 3.17 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:47:38.292308: step 2074, loss = 3.69 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:47:40.461392: step 2075, loss = 3.79 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:47:42.610509: step 2076, loss = 3.69 (0.5 examples/sec; 2.145 sec/batch)
2018-10-15 17:47:44.758990: step 2077, loss = 3.69 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:47:46.923216: step 2078, loss = 3.64 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:47:49.051297: step 2079, loss = 3.68 (0.5 examples/sec; 2.124 sec/batch)
2018-10-15 17:47:51.227927: step 2080, loss = 3.17 (0.5 examples/sec; 2.172 sec/batch)
2018-10-15 17:47:53.394914: step 2081, loss = 3.65 (0.5 examples/sec; 2.163 sec/batch)
2018-10-15 17:47:55.539932: step 2082, loss = 3.24 (0.5 examples/sec; 2.142 sec/batch)
2018-10-15 17:47:57.687758: step 2083, loss = 3.66 (0.5 examples/sec; 2.143 sec/batch)
2018-10-15 17:47:59.869741: step 2084, loss = 3.65 (0.5 examples/sec; 2.178 sec/batch)
2018-10-15 17:48:02.027506: step 2085, loss = 3.17 (0.5 examples/sec; 2.153 sec/batch)
2018-10-15 17:48:04.145749: step 2086, loss = 3.17 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:48:06.294908: step 2087, loss = 3.65 (0.5 examples/sec; 2.145 sec/batch)
2018-10-15 17:48:08.439584: step 2088, loss = 3.64 (0.5 examples/sec; 2.142 sec/batch)
2018-10-15 17:48:10.605215: step 2089, loss = 3.80 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:48:12.742038: step 2090, loss = 3.64 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:48:14.902535: step 2091, loss = 3.80 (0.5 examples/sec; 2.157 sec/batch)
2018-10-15 17:48:17.051275: step 2092, loss = 3.17 (0.5 examples/sec; 2.145 sec/batch)
2018-10-15 17:48:19.185645: step 2093, loss = 3.63 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:48:21.347718: step 2094, loss = 3.17 (0.5 examples/sec; 2.158 sec/batch)
2018-10-15 17:48:23.476739: step 2095, loss = 3.27 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:48:25.645331: step 2096, loss = 3.17 (0.5 examples/sec; 2.162 sec/batch)
2018-10-15 17:48:27.778943: step 2097, loss = 3.63 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:48:29.922465: step 2098, loss = 3.80 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:48:32.059696: step 2099, loss = 3.63 (0.5 examples/sec; 2.135 sec/batch)
2018-10-15 17:48:34.202578: step 2100, loss = 3.28 (0.5 examples/sec; 2.138 sec/batch)
2018-10-15 17:48:36.795432: step 2101, loss = 3.28 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:48:38.952602: step 2102, loss = 3.80 (0.5 examples/sec; 2.153 sec/batch)
2018-10-15 17:48:41.091606: step 2103, loss = 3.62 (0.5 examples/sec; 2.135 sec/batch)
2018-10-15 17:48:43.226196: step 2104, loss = 3.17 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:48:45.351532: step 2105, loss = 3.79 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:48:47.439132: step 2106, loss = 3.63 (0.5 examples/sec; 2.085 sec/batch)
2018-10-15 17:48:49.570485: step 2107, loss = 3.17 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:48:51.730262: step 2108, loss = 3.29 (0.5 examples/sec; 2.156 sec/batch)
2018-10-15 17:48:53.888461: step 2109, loss = 3.63 (0.5 examples/sec; 2.156 sec/batch)
2018-10-15 17:48:56.015901: step 2110, loss = 3.62 (0.5 examples/sec; 2.125 sec/batch)
2018-10-15 17:48:58.190489: step 2111, loss = 3.62 (0.5 examples/sec; 2.170 sec/batch)
2018-10-15 17:49:00.328433: step 2112, loss = 3.62 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:49:02.493487: step 2113, loss = 3.61 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:49:04.622676: step 2114, loss = 3.18 (0.5 examples/sec; 2.125 sec/batch)
2018-10-15 17:49:06.756399: step 2115, loss = 3.61 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:49:08.913399: step 2116, loss = 3.60 (0.5 examples/sec; 2.153 sec/batch)
2018-10-15 17:49:11.026313: step 2117, loss = 3.18 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:49:13.156824: step 2118, loss = 3.18 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:49:15.277436: step 2119, loss = 3.31 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:49:17.388730: step 2120, loss = 3.58 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:49:19.524653: step 2121, loss = 3.58 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:49:21.660496: step 2122, loss = 3.62 (0.5 examples/sec; 2.133 sec/batch)
2018-10-15 17:49:23.793299: step 2123, loss = 3.62 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:49:25.923857: step 2124, loss = 3.32 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:49:28.029045: step 2125, loss = 3.18 (0.5 examples/sec; 2.101 sec/batch)
2018-10-15 17:49:30.151509: step 2126, loss = 3.18 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:49:32.272129: step 2127, loss = 3.61 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:49:34.407707: step 2128, loss = 3.56 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:49:36.522573: step 2129, loss = 3.61 (0.5 examples/sec; 2.110 sec/batch)
2018-10-15 17:49:38.625864: step 2130, loss = 3.18 (0.5 examples/sec; 2.099 sec/batch)
2018-10-15 17:49:40.734292: step 2131, loss = 3.82 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:49:42.869951: step 2132, loss = 3.18 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:49:45.043090: step 2133, loss = 3.60 (0.5 examples/sec; 2.169 sec/batch)
2018-10-15 17:49:47.160343: step 2134, loss = 3.55 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:49:49.267092: step 2135, loss = 3.59 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:49:51.375888: step 2136, loss = 3.34 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:49:53.500463: step 2137, loss = 3.17 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:49:55.618133: step 2138, loss = 3.17 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:49:57.726724: step 2139, loss = 3.58 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:49:59.837029: step 2140, loss = 3.54 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:50:01.978875: step 2141, loss = 3.17 (0.5 examples/sec; 2.140 sec/batch)
2018-10-15 17:50:04.098911: step 2142, loss = 3.17 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:50:06.224743: step 2143, loss = 3.54 (0.5 examples/sec; 2.122 sec/batch)
2018-10-15 17:50:08.343186: step 2144, loss = 3.58 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:50:10.466505: step 2145, loss = 3.57 (0.5 examples/sec; 2.119 sec/batch)
2018-10-15 17:50:12.562962: step 2146, loss = 3.37 (0.5 examples/sec; 2.094 sec/batch)
2018-10-15 17:50:14.708390: step 2147, loss = 3.54 (0.5 examples/sec; 2.142 sec/batch)
2018-10-15 17:50:16.876376: step 2148, loss = 3.37 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:50:18.981895: step 2149, loss = 3.56 (0.5 examples/sec; 2.102 sec/batch)
2018-10-15 17:50:21.087061: step 2150, loss = 3.37 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:50:23.202747: step 2151, loss = 3.56 (0.5 examples/sec; 2.111 sec/batch)
2018-10-15 17:50:25.322382: step 2152, loss = 3.85 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:50:27.447070: step 2153, loss = 3.17 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:50:29.553826: step 2154, loss = 3.37 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:50:31.720913: step 2155, loss = 3.86 (0.5 examples/sec; 2.163 sec/batch)
2018-10-15 17:50:33.832174: step 2156, loss = 3.17 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:50:35.991981: step 2157, loss = 3.54 (0.5 examples/sec; 2.156 sec/batch)
2018-10-15 17:50:38.130820: step 2158, loss = 3.37 (0.5 examples/sec; 2.134 sec/batch)
2018-10-15 17:50:40.286142: step 2159, loss = 3.54 (0.5 examples/sec; 2.151 sec/batch)
2018-10-15 17:50:42.420391: step 2160, loss = 3.17 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:50:44.536145: step 2161, loss = 3.37 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:50:46.716923: step 2162, loss = 3.55 (0.5 examples/sec; 2.176 sec/batch)
2018-10-15 17:50:48.858378: step 2163, loss = 3.55 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:50:50.963061: step 2164, loss = 3.55 (0.5 examples/sec; 2.101 sec/batch)
2018-10-15 17:50:53.072421: step 2165, loss = 3.86 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:50:55.171895: step 2166, loss = 3.18 (0.5 examples/sec; 2.095 sec/batch)
2018-10-15 17:50:57.381940: step 2167, loss = 3.18 (0.5 examples/sec; 2.208 sec/batch)
2018-10-15 17:50:59.506188: step 2168, loss = 3.18 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:51:01.626869: step 2169, loss = 3.54 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:51:03.739288: step 2170, loss = 3.17 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:51:05.853630: step 2171, loss = 3.53 (0.5 examples/sec; 2.110 sec/batch)
2018-10-15 17:51:08.027160: step 2172, loss = 3.37 (0.5 examples/sec; 2.170 sec/batch)
2018-10-15 17:51:10.131789: step 2173, loss = 3.37 (0.5 examples/sec; 2.100 sec/batch)
2018-10-15 17:51:12.216547: step 2174, loss = 3.17 (0.5 examples/sec; 2.083 sec/batch)
2018-10-15 17:51:14.339727: step 2175, loss = 3.54 (0.5 examples/sec; 2.119 sec/batch)
2018-10-15 17:51:16.465057: step 2176, loss = 3.16 (0.5 examples/sec; 2.123 sec/batch)
2018-10-15 17:51:18.604115: step 2177, loss = 3.54 (0.5 examples/sec; 2.135 sec/batch)
2018-10-15 17:51:20.723044: step 2178, loss = 3.54 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:51:22.843116: step 2179, loss = 3.54 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:51:24.963836: step 2180, loss = 3.88 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:51:27.076545: step 2181, loss = 3.38 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:51:29.246544: step 2182, loss = 3.53 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:51:31.382388: step 2183, loss = 3.16 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:51:33.495941: step 2184, loss = 3.53 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:51:35.604123: step 2185, loss = 3.88 (0.5 examples/sec; 2.106 sec/batch)
2018-10-15 17:51:37.737185: step 2186, loss = 3.16 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:51:39.903603: step 2187, loss = 3.52 (0.5 examples/sec; 2.163 sec/batch)
2018-10-15 17:51:42.044679: step 2188, loss = 3.54 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:51:44.212337: step 2189, loss = 3.39 (0.5 examples/sec; 2.164 sec/batch)
2018-10-15 17:51:46.334324: step 2190, loss = 3.88 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:51:48.496296: step 2191, loss = 3.39 (0.5 examples/sec; 2.158 sec/batch)
2018-10-15 17:51:50.627761: step 2192, loss = 3.87 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:51:52.816903: step 2193, loss = 3.16 (0.5 examples/sec; 2.187 sec/batch)
2018-10-15 17:51:54.929054: step 2194, loss = 3.16 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:51:57.018224: step 2195, loss = 3.16 (0.5 examples/sec; 2.084 sec/batch)
2018-10-15 17:51:59.140656: step 2196, loss = 3.39 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:52:01.249547: step 2197, loss = 3.39 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:52:03.395949: step 2198, loss = 3.55 (0.5 examples/sec; 2.143 sec/batch)
2018-10-15 17:52:05.559453: step 2199, loss = 3.55 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:52:07.712983: step 2200, loss = 3.86 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:52:10.352355: step 2201, loss = 3.55 (0.5 examples/sec; 2.177 sec/batch)
2018-10-15 17:52:12.479076: step 2202, loss = 3.52 (0.5 examples/sec; 2.124 sec/batch)
2018-10-15 17:52:14.614557: step 2203, loss = 3.16 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:52:16.732113: step 2204, loss = 3.52 (0.5 examples/sec; 2.114 sec/batch)
2018-10-15 17:52:18.880957: step 2205, loss = 3.86 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:52:21.016199: step 2206, loss = 3.16 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:52:23.129364: step 2207, loss = 3.39 (0.5 examples/sec; 2.111 sec/batch)
2018-10-15 17:52:25.285767: step 2208, loss = 3.39 (0.5 examples/sec; 2.154 sec/batch)
2018-10-15 17:52:27.416779: step 2209, loss = 3.16 (0.5 examples/sec; 2.127 sec/batch)
2018-10-15 17:52:29.544840: step 2210, loss = 3.85 (0.5 examples/sec; 2.126 sec/batch)
2018-10-15 17:52:31.654084: step 2211, loss = 3.38 (0.5 examples/sec; 2.105 sec/batch)
2018-10-15 17:52:33.786267: step 2212, loss = 3.85 (0.5 examples/sec; 2.128 sec/batch)
2018-10-15 17:52:35.905160: step 2213, loss = 3.55 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:52:38.026025: step 2214, loss = 3.53 (0.5 examples/sec; 2.119 sec/batch)
2018-10-15 17:52:40.152030: step 2215, loss = 3.84 (0.5 examples/sec; 2.124 sec/batch)
2018-10-15 17:52:42.295310: step 2216, loss = 3.84 (0.5 examples/sec; 2.141 sec/batch)
2018-10-15 17:52:44.464052: step 2217, loss = 3.38 (0.5 examples/sec; 2.165 sec/batch)
2018-10-15 17:52:46.615718: step 2218, loss = 3.38 (0.5 examples/sec; 2.149 sec/batch)
2018-10-15 17:52:48.769321: step 2219, loss = 3.36 (0.5 examples/sec; 2.149 sec/batch)
2018-10-15 17:52:50.889725: step 2220, loss = 3.18 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:52:52.995441: step 2221, loss = 3.82 (0.5 examples/sec; 2.102 sec/batch)
2018-10-15 17:52:55.112218: step 2222, loss = 3.34 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:52:57.232744: step 2223, loss = 3.18 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:52:59.381622: step 2224, loss = 3.80 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:53:01.534922: step 2225, loss = 3.60 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:53:03.659488: step 2226, loss = 3.18 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:53:05.834671: step 2227, loss = 3.18 (0.5 examples/sec; 2.171 sec/batch)
2018-10-15 17:53:07.948037: step 2228, loss = 3.18 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:53:10.059943: step 2229, loss = 3.27 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:53:12.233122: step 2230, loss = 3.36 (0.5 examples/sec; 2.171 sec/batch)
2018-10-15 17:53:14.369090: step 2231, loss = 3.24 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:53:16.522771: step 2232, loss = 3.18 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:53:18.641719: step 2233, loss = 3.20 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:53:20.816994: step 2234, loss = 3.79 (0.5 examples/sec; 2.171 sec/batch)
2018-10-15 17:53:22.940625: step 2235, loss = 3.16 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:53:25.099910: step 2236, loss = 3.79 (0.5 examples/sec; 2.156 sec/batch)
2018-10-15 17:53:27.212483: step 2237, loss = 3.02 (0.5 examples/sec; 2.109 sec/batch)
2018-10-15 17:53:29.337830: step 2238, loss = 2.94 (0.5 examples/sec; 2.121 sec/batch)
2018-10-15 17:53:31.450430: step 2239, loss = 3.17 (0.5 examples/sec; 2.108 sec/batch)
2018-10-15 17:53:33.587451: step 2240, loss = 4.29 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:53:35.716246: step 2241, loss = 4.14 (0.5 examples/sec; 2.125 sec/batch)
2018-10-15 17:53:37.856107: step 2242, loss = 4.09 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:53:39.962986: step 2243, loss = 4.16 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:53:42.132699: step 2244, loss = 3.89 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:53:44.266233: step 2245, loss = 3.19 (0.5 examples/sec; 2.129 sec/batch)
2018-10-15 17:53:46.442684: step 2246, loss = 3.58 (0.5 examples/sec; 2.172 sec/batch)
2018-10-15 17:53:48.547121: step 2247, loss = 3.83 (0.5 examples/sec; 2.101 sec/batch)
2018-10-15 17:53:50.667722: step 2248, loss = 3.28 (0.5 examples/sec; 2.116 sec/batch)
2018-10-15 17:53:52.774760: step 2249, loss = 3.18 (0.5 examples/sec; 2.102 sec/batch)
2018-10-15 17:53:54.878041: step 2250, loss = 3.18 (0.5 examples/sec; 2.098 sec/batch)
2018-10-15 17:53:56.964866: step 2251, loss = 3.17 (0.5 examples/sec; 2.085 sec/batch)
2018-10-15 17:53:59.072991: step 2252, loss = 3.77 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:54:01.179573: step 2253, loss = 3.61 (0.5 examples/sec; 2.102 sec/batch)
2018-10-15 17:54:03.285582: step 2254, loss = 3.57 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:54:05.439461: step 2255, loss = 3.35 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:54:07.601467: step 2256, loss = 3.57 (0.5 examples/sec; 2.157 sec/batch)
2018-10-15 17:54:09.716382: step 2257, loss = 3.35 (0.5 examples/sec; 2.111 sec/batch)
2018-10-15 17:54:11.822850: step 2258, loss = 3.16 (0.5 examples/sec; 2.102 sec/batch)
2018-10-15 17:54:13.944548: step 2259, loss = 3.16 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:54:16.051717: step 2260, loss = 3.35 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:54:18.169042: step 2261, loss = 3.62 (0.5 examples/sec; 2.113 sec/batch)
2018-10-15 17:54:20.306924: step 2262, loss = 3.34 (0.5 examples/sec; 2.136 sec/batch)
2018-10-15 17:54:22.422989: step 2263, loss = 3.16 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:54:24.538803: step 2264, loss = 3.62 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:54:26.650700: step 2265, loss = 3.34 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:54:28.785891: step 2266, loss = 3.16 (0.5 examples/sec; 2.131 sec/batch)
2018-10-15 17:54:30.907773: step 2267, loss = 3.15 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:54:33.014199: step 2268, loss = 3.15 (0.5 examples/sec; 2.104 sec/batch)
2018-10-15 17:54:35.122175: step 2269, loss = 3.78 (0.5 examples/sec; 2.103 sec/batch)
2018-10-15 17:54:37.238491: step 2270, loss = 3.63 (0.5 examples/sec; 2.112 sec/batch)
2018-10-15 17:54:39.361138: step 2271, loss = 3.62 (0.5 examples/sec; 2.119 sec/batch)
2018-10-15 17:54:41.493191: step 2272, loss = 3.78 (0.5 examples/sec; 2.128 sec/batch)
2018-10-15 17:54:43.657610: step 2273, loss = 3.62 (0.5 examples/sec; 2.160 sec/batch)
2018-10-15 17:54:45.811591: step 2274, loss = 3.62 (0.5 examples/sec; 2.150 sec/batch)
2018-10-15 17:54:47.953141: step 2275, loss = 3.61 (0.5 examples/sec; 2.137 sec/batch)
2018-10-15 17:54:50.064512: step 2276, loss = 3.14 (0.5 examples/sec; 2.107 sec/batch)
2018-10-15 17:54:52.233664: step 2277, loss = 3.78 (0.5 examples/sec; 2.167 sec/batch)
2018-10-15 17:54:54.403724: step 2278, loss = 3.78 (0.5 examples/sec; 2.166 sec/batch)
2018-10-15 17:54:56.539142: step 2279, loss = 3.35 (0.5 examples/sec; 2.133 sec/batch)
2018-10-15 17:54:58.673552: step 2280, loss = 3.59 (0.5 examples/sec; 2.130 sec/batch)
2018-10-15 17:55:00.820980: step 2281, loss = 3.60 (0.5 examples/sec; 2.143 sec/batch)
2018-10-15 17:55:02.996002: step 2282, loss = 3.15 (0.5 examples/sec; 2.171 sec/batch)
2018-10-15 17:55:05.184695: step 2283, loss = 3.34 (0.5 examples/sec; 2.184 sec/batch)
2018-10-15 17:55:07.326988: step 2284, loss = 3.58 (0.5 examples/sec; 2.138 sec/batch)
2018-10-15 17:55:09.451171: step 2285, loss = 3.60 (0.5 examples/sec; 2.120 sec/batch)
2018-10-15 17:55:11.571163: step 2286, loss = 3.33 (0.5 examples/sec; 2.115 sec/batch)
2018-10-15 17:55:13.703558: step 2287, loss = 3.15 (0.5 examples/sec; 2.128 sec/batch)
2018-10-15 17:55:15.853884: step 2288, loss = 3.76 (0.5 examples/sec; 2.148 sec/batch)
2018-10-15 17:55:18.020870: step 2289, loss = 3.58 (0.5 examples/sec; 2.163 sec/batch)
2018-10-15 17:55:20.158140: step 2290, loss = 3.29 (0.5 examples/sec; 2.133 sec/batch)
2018-10-15 17:55:22.278378: step 2291, loss = 3.26 (0.5 examples/sec; 2.117 sec/batch)
2018-10-15 17:55:24.449387: step 2292, loss = 3.22 (0.5 examples/sec; 2.169 sec/batch)
2018-10-15 17:55:26.585979: step 2293, loss = 3.16 (0.5 examples/sec; 2.132 sec/batch)
2018-10-15 17:55:28.739309: step 2294, loss = 3.07 (0.5 examples/sec; 2.149 sec/batch)
2018-10-15 17:55:30.885432: step 2295, loss = 3.57 (0.5 examples/sec; 2.142 sec/batch)
2018-10-15 17:55:33.024619: step 2296, loss = 3.39 (0.5 examples/sec; 2.135 sec/batch)
2018-10-15 17:55:35.189745: step 2297, loss = 3.55 (0.5 examples/sec; 2.161 sec/batch)
2018-10-15 17:55:37.312515: step 2298, loss = 3.79 (0.5 examples/sec; 2.118 sec/batch)
2018-10-15 17:55:39.461297: step 2299, loss = 3.78 (0.5 examples/sec; 2.144 sec/batch)
2018-10-15 17:55:41.635784: step 2300, loss = 2.84 (0.5 examples/sec; 2.170 sec/batch)
2018-10-15 17:55:44.311996: step 2301, loss = 3.17 (0.5 examples/sec; 2.197 sec/batch)
2018-10-15 17:55:46.477584: step 2302, loss = 3.55 (0.5 examples/sec; 2.161 sec/batch)
2018-10-15 17:55:48.590643: step 2303, loss = 3.48 (0.5 examples/sec; 2.109 sec/batch)
Terminated
