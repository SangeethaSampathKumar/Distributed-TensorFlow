yan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode single --dataset flowers --batch_num 2048
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:worker/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:worker/task:0)
Input batch shape: images: (64, 256, 256, 3) labels: (64,)
num_classes: 5
total_num_examples: 131072
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 13:41:16.712297: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 6d8a3e83ea087240 with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 13:41:23.167203: step 0, loss = 4.07 (16.2 examples/sec; 3.954 sec/batch)
2018-10-15 13:41:26.762017: step 1, loss = 6.09 (21.3 examples/sec; 3.000 sec/batch)
2018-10-15 13:41:30.370326: step 2, loss = 8.83 (17.8 examples/sec; 3.603 sec/batch)
2018-10-15 13:41:33.042500: step 3, loss = 6.85 (24.0 examples/sec; 2.669 sec/batch)
2018-10-15 13:41:35.601551: step 4, loss = 10.59 (25.1 examples/sec; 2.552 sec/batch)
2018-10-15 13:41:38.122825: step 5, loss = 8.99 (25.4 examples/sec; 2.517 sec/batch)
2018-10-15 13:41:40.834974: step 6, loss = 4.23 (23.6 examples/sec; 2.708 sec/batch)
2018-10-15 13:41:43.329870: step 7, loss = 5.94 (25.7 examples/sec; 2.490 sec/batch)
2018-10-15 13:41:45.878079: step 8, loss = 6.67 (25.1 examples/sec; 2.545 sec/batch)
2018-10-15 13:41:48.495104: step 9, loss = 6.77 (24.5 examples/sec; 2.611 sec/batch)
2018-10-15 13:41:53.382958: step 10, loss = 6.05 (13.1 examples/sec; 4.883 sec/batch)
2018-10-15 13:41:56.823679: step 11, loss = 5.38 (18.6 examples/sec; 3.438 sec/batch)
2018-10-15 13:41:59.345478: step 12, loss = 4.53 (25.4 examples/sec; 2.515 sec/batch)
2018-10-15 13:42:02.008842: step 13, loss = 3.80 (24.1 examples/sec; 2.658 sec/batch)
2018-10-15 13:42:04.715873: step 14, loss = 6.47 (23.7 examples/sec; 2.702 sec/batch)
2018-10-15 13:42:07.208978: step 15, loss = 4.27 (25.7 examples/sec; 2.490 sec/batch)
2018-10-15 13:42:09.866778: step 16, loss = 4.94 (24.1 examples/sec; 2.653 sec/batch)
2018-10-15 13:42:12.473256: step 17, loss = 5.07 (24.6 examples/sec; 2.599 sec/batch)
2018-10-15 13:42:15.012375: step 18, loss = 5.04 (25.2 examples/sec; 2.535 sec/batch)
2018-10-15 13:42:17.443868: step 19, loss = 4.21 (26.4 examples/sec; 2.427 sec/batch)
2018-10-15 13:42:20.083644: step 20, loss = 3.97 (24.3 examples/sec; 2.635 sec/batch)
2018-10-15 13:42:22.561793: step 21, loss = 4.71 (25.9 examples/sec; 2.473 sec/batch)
2018-10-15 13:42:25.064169: step 22, loss = 3.84 (25.7 examples/sec; 2.495 sec/batch)
2018-10-15 13:42:28.385480: step 23, loss = 4.18 (19.3 examples/sec; 3.319 sec/batch)
2018-10-15 13:42:30.890817: step 24, loss = 3.59 (25.6 examples/sec; 2.502 sec/batch)
2018-10-15 13:42:33.251218: step 25, loss = 3.77 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 13:42:35.525885: step 26, loss = 4.08 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 13:42:37.822337: step 27, loss = 3.62 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 13:42:40.129007: step 28, loss = 3.51 (27.8 examples/sec; 2.302 sec/batch)
2018-10-15 13:42:42.411578: step 29, loss = 3.77 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 13:42:44.654729: step 30, loss = 3.79 (28.6 examples/sec; 2.238 sec/batch)
2018-10-15 13:42:46.953178: step 31, loss = 3.57 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 13:42:49.242231: step 32, loss = 3.53 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 13:42:51.547661: step 33, loss = 3.72 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 13:42:53.810863: step 34, loss = 3.60 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 13:42:56.074889: step 35, loss = 3.51 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 13:42:58.473809: step 36, loss = 3.64 (26.7 examples/sec; 2.394 sec/batch)
2018-10-15 13:43:00.840339: step 37, loss = 3.75 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 13:43:03.175322: step 38, loss = 3.58 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 13:43:05.459287: step 39, loss = 3.64 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 13:43:07.852942: step 40, loss = 3.56 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 13:43:10.206930: step 41, loss = 3.51 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 13:43:12.453465: step 42, loss = 3.53 (28.5 examples/sec; 2.242 sec/batch)
2018-10-15 13:43:14.733221: step 43, loss = 3.53 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 13:43:17.063718: step 44, loss = 3.58 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 13:43:19.403474: step 45, loss = 3.45 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 13:43:21.820624: step 46, loss = 3.42 (26.5 examples/sec; 2.412 sec/batch)
2018-10-15 13:43:24.086133: step 47, loss = 3.62 (28.3 examples/sec; 2.261 sec/batch)
2018-10-15 13:43:26.406270: step 48, loss = 3.54 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 13:43:28.681761: step 49, loss = 3.61 (28.2 examples/sec; 2.271 sec/batch)
2018-10-15 13:43:30.943810: step 50, loss = 3.66 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 13:43:33.201013: step 51, loss = 3.58 (28.4 examples/sec; 2.253 sec/batch)
2018-10-15 13:43:35.557470: step 52, loss = 3.71 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 13:43:37.950951: step 53, loss = 3.66 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 13:43:40.359776: step 54, loss = 3.58 (26.6 examples/sec; 2.404 sec/batch)
2018-10-15 13:43:42.697823: step 55, loss = 3.74 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 13:43:45.086058: step 56, loss = 3.70 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 13:43:47.463840: step 57, loss = 3.63 (27.0 examples/sec; 2.373 sec/batch)
2018-10-15 13:43:49.790609: step 58, loss = 3.61 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 13:43:52.140741: step 59, loss = 3.47 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 13:43:54.495671: step 60, loss = 3.76 (27.2 examples/sec; 2.351 sec/batch)
2018-10-15 13:43:56.740873: step 61, loss = 3.52 (28.6 examples/sec; 2.240 sec/batch)
2018-10-15 13:43:59.065747: step 62, loss = 3.46 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 13:44:01.408015: step 63, loss = 3.57 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 13:44:03.767586: step 64, loss = 3.58 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 13:44:06.163692: step 65, loss = 3.50 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 13:44:08.425791: step 66, loss = 3.47 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 13:44:10.649977: step 67, loss = 3.53 (28.8 examples/sec; 2.220 sec/batch)
2018-10-15 13:44:12.979266: step 68, loss = 3.57 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 13:44:15.335703: step 69, loss = 3.26 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 13:44:17.669866: step 70, loss = 3.70 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 13:44:19.887000: step 71, loss = 3.57 (28.9 examples/sec; 2.212 sec/batch)
2018-10-15 13:44:22.213355: step 72, loss = 3.66 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 13:44:24.515006: step 73, loss = 3.67 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 13:44:26.815558: step 74, loss = 3.58 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 13:44:29.085917: step 75, loss = 3.70 (28.2 examples/sec; 2.268 sec/batch)
2018-10-15 13:44:31.436521: step 76, loss = 3.69 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 13:44:33.663617: step 77, loss = 3.48 (28.8 examples/sec; 2.222 sec/batch)
2018-10-15 13:44:35.938089: step 78, loss = 3.41 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 13:44:38.166190: step 79, loss = 3.45 (28.8 examples/sec; 2.224 sec/batch)
2018-10-15 13:44:40.547166: step 80, loss = 3.46 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 13:44:42.854808: step 81, loss = 3.41 (27.8 examples/sec; 2.302 sec/batch)
2018-10-15 13:44:45.136681: step 82, loss = 3.31 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 13:44:47.394766: step 83, loss = 3.34 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 13:44:49.854937: step 84, loss = 3.36 (26.1 examples/sec; 2.455 sec/batch)
2018-10-15 13:44:52.153568: step 85, loss = 3.38 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 13:44:54.441943: step 86, loss = 3.60 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 13:44:56.735128: step 87, loss = 3.39 (28.0 examples/sec; 2.288 sec/batch)
2018-10-15 13:44:59.016809: step 88, loss = 3.25 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 13:45:01.386916: step 89, loss = 3.19 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 13:45:03.673951: step 90, loss = 3.47 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 13:45:06.020049: step 91, loss = 3.51 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 13:45:08.293469: step 92, loss = 3.58 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 13:45:10.621047: step 93, loss = 3.28 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 13:45:12.957562: step 94, loss = 3.59 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 13:45:15.209529: step 95, loss = 3.31 (28.5 examples/sec; 2.249 sec/batch)
2018-10-15 13:45:17.565429: step 96, loss = 3.65 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 13:45:19.883272: step 97, loss = 3.61 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 13:45:22.265596: step 98, loss = 3.42 (26.9 examples/sec; 2.378 sec/batch)
2018-10-15 13:45:24.635803: step 99, loss = 3.30 (27.1 examples/sec; 2.366 sec/batch)
2018-10-15 13:45:26.974986: step 100, loss = 3.36 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 13:45:29.710401: step 101, loss = 3.51 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 13:45:32.068602: step 102, loss = 3.51 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 13:45:34.429735: step 103, loss = 3.53 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 13:45:36.812106: step 104, loss = 3.78 (26.9 examples/sec; 2.378 sec/batch)
2018-10-15 13:45:39.167903: step 105, loss = 3.75 (27.2 examples/sec; 2.351 sec/batch)
2018-10-15 13:45:41.525478: step 106, loss = 3.32 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 13:45:43.927261: step 107, loss = 3.60 (26.7 examples/sec; 2.397 sec/batch)
2018-10-15 13:45:46.287565: step 108, loss = 3.55 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 13:45:48.663819: step 109, loss = 3.72 (27.0 examples/sec; 2.371 sec/batch)
2018-10-15 13:45:51.023771: step 110, loss = 3.46 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 13:45:53.383593: step 111, loss = 3.72 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 13:45:55.714600: step 112, loss = 3.81 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 13:45:57.995182: step 113, loss = 3.30 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 13:46:00.252361: step 114, loss = 3.52 (28.4 examples/sec; 2.253 sec/batch)
2018-10-15 13:46:02.541979: step 115, loss = 3.31 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 13:46:04.800987: step 116, loss = 3.45 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 13:46:07.124313: step 117, loss = 3.21 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 13:46:09.413964: step 118, loss = 3.21 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 13:46:11.663034: step 119, loss = 3.33 (28.5 examples/sec; 2.246 sec/batch)
2018-10-15 13:46:13.921500: step 120, loss = 3.46 (28.4 examples/sec; 2.253 sec/batch)
2018-10-15 13:46:16.264187: step 121, loss = 3.56 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 13:46:18.651866: step 122, loss = 3.28 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 13:46:20.979255: step 123, loss = 3.11 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 13:46:23.392312: step 124, loss = 3.34 (26.6 examples/sec; 2.410 sec/batch)
2018-10-15 13:46:25.784801: step 125, loss = 3.38 (26.8 examples/sec; 2.388 sec/batch)
2018-10-15 13:46:28.075480: step 126, loss = 3.18 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 13:46:30.382580: step 127, loss = 3.25 (27.8 examples/sec; 2.304 sec/batch)
2018-10-15 13:46:32.712800: step 128, loss = 3.32 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 13:46:35.020219: step 129, loss = 3.29 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 13:46:37.417734: step 130, loss = 3.33 (26.7 examples/sec; 2.393 sec/batch)
2018-10-15 13:46:39.778982: step 131, loss = 3.07 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 13:46:42.131817: step 132, loss = 3.19 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 13:46:44.498666: step 133, loss = 3.23 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 13:46:46.789479: step 134, loss = 3.09 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 13:46:49.005029: step 135, loss = 3.23 (28.9 examples/sec; 2.213 sec/batch)
2018-10-15 13:46:51.353771: step 136, loss = 3.21 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 13:46:53.708845: step 137, loss = 3.28 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 13:46:56.002684: step 138, loss = 3.21 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 13:46:58.352579: step 139, loss = 3.32 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 13:47:00.745867: step 140, loss = 3.36 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 13:47:03.094087: step 141, loss = 3.11 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 13:47:05.391346: step 142, loss = 3.11 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 13:47:07.734361: step 143, loss = 3.18 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 13:47:10.114348: step 144, loss = 3.31 (26.9 examples/sec; 2.375 sec/batch)
2018-10-15 13:47:12.427364: step 145, loss = 3.33 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 13:47:14.717132: step 146, loss = 3.15 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 13:47:17.058775: step 147, loss = 3.28 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 13:47:19.445358: step 148, loss = 3.17 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 13:47:21.798453: step 149, loss = 3.00 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 13:47:24.068618: step 150, loss = 3.18 (28.3 examples/sec; 2.265 sec/batch)
2018-10-15 13:47:26.428625: step 151, loss = 3.23 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 13:47:28.671101: step 152, loss = 3.22 (28.6 examples/sec; 2.238 sec/batch)
2018-10-15 13:47:30.959288: step 153, loss = 3.01 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 13:47:33.352774: step 154, loss = 3.17 (26.8 examples/sec; 2.389 sec/batch)
2018-10-15 13:47:35.725821: step 155, loss = 2.92 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 13:47:37.974125: step 156, loss = 3.10 (28.5 examples/sec; 2.243 sec/batch)
2018-10-15 13:47:40.266041: step 157, loss = 2.92 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 13:47:42.559655: step 158, loss = 3.14 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 13:47:44.893569: step 159, loss = 3.09 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 13:47:47.232815: step 160, loss = 3.07 (27.4 examples/sec; 2.336 sec/batch)
2018-10-15 13:47:49.553397: step 161, loss = 3.19 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 13:47:51.925751: step 162, loss = 3.21 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 13:47:54.286242: step 163, loss = 3.15 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 13:47:56.562060: step 164, loss = 3.12 (28.2 examples/sec; 2.271 sec/batch)
2018-10-15 13:47:58.893561: step 165, loss = 2.96 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 13:48:01.342514: step 166, loss = 3.20 (26.2 examples/sec; 2.444 sec/batch)
2018-10-15 13:48:03.674220: step 167, loss = 3.15 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 13:48:06.021313: step 168, loss = 3.27 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 13:48:08.406268: step 169, loss = 3.07 (26.9 examples/sec; 2.381 sec/batch)
2018-10-15 13:48:10.724907: step 170, loss = 2.99 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 13:48:13.042799: step 171, loss = 3.26 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 13:48:15.412130: step 172, loss = 3.07 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 13:48:17.810969: step 173, loss = 3.11 (26.7 examples/sec; 2.394 sec/batch)
2018-10-15 13:48:20.138191: step 174, loss = 3.12 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 13:48:22.403309: step 175, loss = 3.02 (28.3 examples/sec; 2.260 sec/batch)
2018-10-15 13:48:24.793691: step 176, loss = 3.17 (26.8 examples/sec; 2.385 sec/batch)
2018-10-15 13:48:27.083211: step 177, loss = 3.43 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 13:48:29.396528: step 178, loss = 2.91 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 13:48:31.702481: step 179, loss = 3.32 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 13:48:33.948527: step 180, loss = 3.04 (28.6 examples/sec; 2.241 sec/batch)
2018-10-15 13:48:36.268306: step 181, loss = 3.16 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 13:48:38.566292: step 182, loss = 3.14 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 13:48:40.899834: step 183, loss = 3.07 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 13:48:43.290126: step 184, loss = 3.13 (26.8 examples/sec; 2.385 sec/batch)
2018-10-15 13:48:45.596759: step 185, loss = 3.08 (27.8 examples/sec; 2.302 sec/batch)
2018-10-15 13:48:47.890481: step 186, loss = 3.28 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 13:48:50.261244: step 187, loss = 3.20 (27.1 examples/sec; 2.366 sec/batch)
2018-10-15 13:48:52.474874: step 188, loss = 3.04 (29.0 examples/sec; 2.209 sec/batch)
2018-10-15 13:48:54.774440: step 189, loss = 3.15 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 13:48:57.006183: step 190, loss = 3.05 (28.7 examples/sec; 2.227 sec/batch)
2018-10-15 13:48:59.354738: step 191, loss = 3.04 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 13:49:01.664769: step 192, loss = 2.89 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 13:49:04.059867: step 193, loss = 3.04 (26.8 examples/sec; 2.390 sec/batch)
2018-10-15 13:49:06.375806: step 194, loss = 3.09 (27.7 examples/sec; 2.311 sec/batch)
2018-10-15 13:49:08.721433: step 195, loss = 3.32 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 13:49:11.050943: step 196, loss = 3.15 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 13:49:13.352165: step 197, loss = 3.13 (27.9 examples/sec; 2.297 sec/batch)
2018-10-15 13:49:15.708424: step 198, loss = 3.04 (27.2 examples/sec; 2.351 sec/batch)
2018-10-15 13:49:18.182736: step 199, loss = 3.20 (25.9 examples/sec; 2.470 sec/batch)
2018-10-15 13:49:20.521347: step 200, loss = 3.15 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 13:49:23.336447: step 201, loss = 3.15 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 13:49:25.753794: step 202, loss = 3.11 (26.5 examples/sec; 2.412 sec/batch)
2018-10-15 13:49:28.126052: step 203, loss = 3.17 (27.0 examples/sec; 2.368 sec/batch)
2018-10-15 13:49:30.503415: step 204, loss = 3.24 (27.0 examples/sec; 2.374 sec/batch)
2018-10-15 13:49:32.824822: step 205, loss = 3.30 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 13:49:35.083800: step 206, loss = 3.08 (28.4 examples/sec; 2.255 sec/batch)
2018-10-15 13:49:37.350268: step 207, loss = 3.01 (28.3 examples/sec; 2.262 sec/batch)
2018-10-15 13:49:39.635826: step 208, loss = 3.16 (28.1 examples/sec; 2.281 sec/batch)
2018-10-15 13:49:41.943763: step 209, loss = 2.91 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 13:49:44.184957: step 210, loss = 3.54 (28.6 examples/sec; 2.236 sec/batch)
2018-10-15 13:49:46.552041: step 211, loss = 3.14 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 13:49:48.884023: step 212, loss = 3.04 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 13:49:51.147409: step 213, loss = 2.98 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 13:49:53.615642: step 214, loss = 3.25 (26.0 examples/sec; 2.463 sec/batch)
2018-10-15 13:49:55.952707: step 215, loss = 3.17 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 13:49:58.239095: step 216, loss = 3.15 (28.1 examples/sec; 2.281 sec/batch)
2018-10-15 13:50:00.504268: step 217, loss = 3.23 (28.3 examples/sec; 2.260 sec/batch)
2018-10-15 13:50:02.891708: step 218, loss = 3.07 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 13:50:05.199659: step 219, loss = 3.04 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 13:50:07.445152: step 220, loss = 3.03 (28.5 examples/sec; 2.243 sec/batch)
2018-10-15 13:50:09.798649: step 221, loss = 3.12 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 13:50:12.143105: step 222, loss = 2.98 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 13:50:14.454251: step 223, loss = 3.10 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 13:50:16.699554: step 224, loss = 3.30 (28.6 examples/sec; 2.240 sec/batch)
2018-10-15 13:50:18.964787: step 225, loss = 3.09 (28.3 examples/sec; 2.260 sec/batch)
2018-10-15 13:50:21.191120: step 226, loss = 2.95 (28.8 examples/sec; 2.221 sec/batch)
2018-10-15 13:50:23.581890: step 227, loss = 3.16 (26.8 examples/sec; 2.386 sec/batch)
2018-10-15 13:50:25.833184: step 228, loss = 3.24 (28.5 examples/sec; 2.246 sec/batch)
2018-10-15 13:50:28.142508: step 229, loss = 3.02 (27.8 examples/sec; 2.304 sec/batch)
2018-10-15 13:50:30.491761: step 230, loss = 3.21 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 13:50:32.736530: step 231, loss = 3.00 (28.6 examples/sec; 2.239 sec/batch)
2018-10-15 13:50:35.090050: step 232, loss = 3.01 (27.3 examples/sec; 2.349 sec/batch)
2018-10-15 13:50:37.434836: step 233, loss = 3.00 (27.3 examples/sec; 2.340 sec/batch)
2018-10-15 13:50:39.746975: step 234, loss = 2.88 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 13:50:42.091213: step 235, loss = 3.07 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 13:50:44.387459: step 236, loss = 2.90 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 13:50:46.703929: step 237, loss = 3.03 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 13:50:49.084442: step 238, loss = 3.01 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 13:50:51.310553: step 239, loss = 2.97 (28.8 examples/sec; 2.222 sec/batch)
2018-10-15 13:50:53.703128: step 240, loss = 3.09 (26.8 examples/sec; 2.388 sec/batch)
2018-10-15 13:50:56.023797: step 241, loss = 3.01 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 13:50:58.306824: step 242, loss = 2.99 (28.1 examples/sec; 2.278 sec/batch)
2018-10-15 13:51:00.643636: step 243, loss = 3.19 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 13:51:03.065155: step 244, loss = 3.05 (26.5 examples/sec; 2.417 sec/batch)
2018-10-15 13:51:05.396216: step 245, loss = 3.03 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 13:51:07.705954: step 246, loss = 3.01 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 13:51:09.974939: step 247, loss = 3.16 (28.3 examples/sec; 2.263 sec/batch)
2018-10-15 13:51:12.428693: step 248, loss = 3.09 (26.1 examples/sec; 2.451 sec/batch)
2018-10-15 13:51:14.747002: step 249, loss = 3.22 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 13:51:16.974671: step 250, loss = 2.97 (28.8 examples/sec; 2.223 sec/batch)
2018-10-15 13:51:19.353304: step 251, loss = 2.95 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 13:51:21.715990: step 252, loss = 3.11 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 13:51:24.068731: step 253, loss = 3.12 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 13:51:26.348443: step 254, loss = 2.93 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 13:51:28.728017: step 255, loss = 3.11 (27.0 examples/sec; 2.374 sec/batch)
2018-10-15 13:51:31.134877: step 256, loss = 3.08 (26.6 examples/sec; 2.404 sec/batch)
2018-10-15 13:51:33.445437: step 257, loss = 3.14 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 13:51:35.756535: step 258, loss = 2.96 (27.7 examples/sec; 2.306 sec/batch)
2018-10-15 13:51:38.167679: step 259, loss = 3.31 (26.6 examples/sec; 2.406 sec/batch)
2018-10-15 13:51:40.533643: step 260, loss = 3.40 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 13:51:42.832321: step 261, loss = 3.10 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 13:51:45.166729: step 262, loss = 3.23 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 13:51:47.580754: step 263, loss = 3.04 (26.6 examples/sec; 2.410 sec/batch)
2018-10-15 13:51:49.938796: step 264, loss = 3.35 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 13:51:52.249633: step 265, loss = 3.03 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 13:51:54.640547: step 266, loss = 3.05 (26.8 examples/sec; 2.386 sec/batch)
2018-10-15 13:51:57.047621: step 267, loss = 3.20 (26.6 examples/sec; 2.402 sec/batch)
2018-10-15 13:51:59.385800: step 268, loss = 2.98 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 13:52:01.663595: step 269, loss = 3.11 (28.2 examples/sec; 2.273 sec/batch)
2018-10-15 13:52:03.999848: step 270, loss = 2.99 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 13:52:06.374194: step 271, loss = 2.95 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 13:52:08.715989: step 272, loss = 3.18 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 13:52:11.026492: step 273, loss = 3.05 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 13:52:13.371607: step 274, loss = 2.88 (27.3 examples/sec; 2.340 sec/batch)
2018-10-15 13:52:15.706447: step 275, loss = 3.17 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 13:52:17.985244: step 276, loss = 3.18 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 13:52:20.317417: step 277, loss = 2.96 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 13:52:22.633760: step 278, loss = 2.80 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 13:52:24.958505: step 279, loss = 3.01 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 13:52:27.227558: step 280, loss = 2.97 (28.3 examples/sec; 2.264 sec/batch)
2018-10-15 13:52:29.555037: step 281, loss = 3.32 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 13:52:31.953204: step 282, loss = 2.96 (26.7 examples/sec; 2.393 sec/batch)
2018-10-15 13:52:34.296150: step 283, loss = 2.96 (27.3 examples/sec; 2.340 sec/batch)
2018-10-15 13:52:36.590271: step 284, loss = 3.04 (27.9 examples/sec; 2.291 sec/batch)
2018-10-15 13:52:39.016938: step 285, loss = 2.94 (26.4 examples/sec; 2.422 sec/batch)
2018-10-15 13:52:41.285995: step 286, loss = 2.88 (28.2 examples/sec; 2.266 sec/batch)
2018-10-15 13:52:43.628459: step 287, loss = 2.91 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 13:52:45.952669: step 288, loss = 2.96 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 13:52:48.282481: step 289, loss = 2.85 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 13:52:50.513451: step 290, loss = 2.91 (28.7 examples/sec; 2.226 sec/batch)
2018-10-15 13:52:52.859647: step 291, loss = 2.96 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 13:52:55.210752: step 292, loss = 3.16 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 13:52:57.569099: step 293, loss = 3.07 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 13:52:59.854732: step 294, loss = 2.79 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 13:53:02.214444: step 295, loss = 3.02 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 13:53:04.478796: step 296, loss = 2.82 (28.3 examples/sec; 2.260 sec/batch)
2018-10-15 13:53:06.826084: step 297, loss = 2.87 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 13:53:09.122186: step 298, loss = 2.94 (27.9 examples/sec; 2.291 sec/batch)
2018-10-15 13:53:11.428644: step 299, loss = 3.06 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 13:53:13.741715: step 300, loss = 2.85 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 13:53:16.512059: step 301, loss = 2.88 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 13:53:18.739838: step 302, loss = 3.05 (28.8 examples/sec; 2.223 sec/batch)
2018-10-15 13:53:21.020363: step 303, loss = 3.21 (28.1 examples/sec; 2.278 sec/batch)
2018-10-15 13:53:23.370807: step 304, loss = 2.98 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 13:53:25.721598: step 305, loss = 2.79 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 13:53:27.984750: step 306, loss = 2.82 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 13:53:30.273408: step 307, loss = 2.99 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 13:53:32.595612: step 308, loss = 2.85 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 13:53:34.875821: step 309, loss = 2.93 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 13:53:37.214922: step 310, loss = 2.65 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 13:53:39.539913: step 311, loss = 2.71 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 13:53:41.839252: step 312, loss = 2.96 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 13:53:44.165106: step 313, loss = 3.02 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 13:53:46.467200: step 314, loss = 2.96 (27.9 examples/sec; 2.298 sec/batch)
2018-10-15 13:53:48.910497: step 315, loss = 2.92 (26.2 examples/sec; 2.438 sec/batch)
2018-10-15 13:53:51.216073: step 316, loss = 3.11 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 13:53:53.480454: step 317, loss = 3.03 (28.3 examples/sec; 2.260 sec/batch)
2018-10-15 13:53:55.815130: step 318, loss = 2.93 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 13:53:58.201262: step 319, loss = 2.90 (26.9 examples/sec; 2.381 sec/batch)
2018-10-15 13:54:00.529257: step 320, loss = 3.01 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 13:54:02.757936: step 321, loss = 2.98 (28.8 examples/sec; 2.224 sec/batch)
2018-10-15 13:54:05.127703: step 322, loss = 2.96 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 13:54:07.436036: step 323, loss = 3.14 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 13:54:09.736547: step 324, loss = 2.93 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 13:54:12.006067: step 325, loss = 2.99 (28.3 examples/sec; 2.264 sec/batch)
2018-10-15 13:54:14.432131: step 326, loss = 3.01 (26.4 examples/sec; 2.421 sec/batch)
2018-10-15 13:54:16.782255: step 327, loss = 3.04 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 13:54:19.122637: step 328, loss = 3.11 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 13:54:21.408955: step 329, loss = 3.00 (28.1 examples/sec; 2.281 sec/batch)
2018-10-15 13:54:23.852594: step 330, loss = 3.00 (26.2 examples/sec; 2.439 sec/batch)
2018-10-15 13:54:26.202598: step 331, loss = 3.07 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 13:54:28.555822: step 332, loss = 3.06 (27.3 examples/sec; 2.349 sec/batch)
2018-10-15 13:54:30.978704: step 333, loss = 3.12 (26.5 examples/sec; 2.418 sec/batch)
2018-10-15 13:54:33.273923: step 334, loss = 3.19 (27.9 examples/sec; 2.290 sec/batch)
2018-10-15 13:54:35.620011: step 335, loss = 2.96 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 13:54:37.922379: step 336, loss = 2.98 (27.9 examples/sec; 2.297 sec/batch)
2018-10-15 13:54:40.250734: step 337, loss = 3.01 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 13:54:42.594433: step 338, loss = 3.04 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 13:54:44.874724: step 339, loss = 2.91 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 13:54:47.225720: step 340, loss = 3.08 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 13:54:49.653032: step 341, loss = 2.88 (26.4 examples/sec; 2.423 sec/batch)
2018-10-15 13:54:52.023536: step 342, loss = 3.10 (27.1 examples/sec; 2.366 sec/batch)
2018-10-15 13:54:54.345468: step 343, loss = 2.87 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 13:54:56.736681: step 344, loss = 3.08 (26.8 examples/sec; 2.386 sec/batch)
2018-10-15 13:54:59.028296: step 345, loss = 2.90 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 13:55:01.413808: step 346, loss = 3.00 (26.9 examples/sec; 2.381 sec/batch)
2018-10-15 13:55:03.816982: step 347, loss = 3.01 (26.7 examples/sec; 2.398 sec/batch)
2018-10-15 13:55:06.074050: step 348, loss = 3.21 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 13:55:08.472587: step 349, loss = 2.99 (26.7 examples/sec; 2.394 sec/batch)
2018-10-15 13:55:10.740338: step 350, loss = 2.90 (28.3 examples/sec; 2.263 sec/batch)
2018-10-15 13:55:13.052403: step 351, loss = 2.96 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 13:55:15.338906: step 352, loss = 2.76 (28.1 examples/sec; 2.281 sec/batch)
2018-10-15 13:55:17.696097: step 353, loss = 2.99 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 13:55:20.074073: step 354, loss = 3.12 (27.0 examples/sec; 2.373 sec/batch)
2018-10-15 13:55:22.410620: step 355, loss = 3.06 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 13:55:24.778110: step 356, loss = 2.88 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 13:55:27.099782: step 357, loss = 2.86 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 13:55:29.421203: step 358, loss = 2.80 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 13:55:31.816488: step 359, loss = 2.93 (26.8 examples/sec; 2.390 sec/batch)
2018-10-15 13:55:34.182667: step 360, loss = 2.94 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 13:55:36.470474: step 361, loss = 3.03 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 13:55:38.885316: step 362, loss = 2.92 (26.6 examples/sec; 2.410 sec/batch)
2018-10-15 13:55:41.335348: step 363, loss = 2.94 (26.2 examples/sec; 2.445 sec/batch)
2018-10-15 13:55:43.709335: step 364, loss = 2.81 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 13:55:46.078459: step 365, loss = 2.87 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 13:55:48.425846: step 366, loss = 3.00 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 13:55:50.738223: step 367, loss = 2.68 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 13:55:53.014087: step 368, loss = 3.08 (28.2 examples/sec; 2.271 sec/batch)
2018-10-15 13:55:55.361292: step 369, loss = 2.92 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 13:55:57.834139: step 370, loss = 2.86 (25.9 examples/sec; 2.470 sec/batch)
2018-10-15 13:56:00.064220: step 371, loss = 3.10 (28.8 examples/sec; 2.225 sec/batch)
2018-10-15 13:56:02.410808: step 372, loss = 2.94 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 13:56:04.776272: step 373, loss = 2.94 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 13:56:07.069990: step 374, loss = 2.94 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 13:56:09.323722: step 375, loss = 2.90 (28.4 examples/sec; 2.251 sec/batch)
2018-10-15 13:56:11.633192: step 376, loss = 2.87 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 13:56:14.015071: step 377, loss = 2.98 (26.9 examples/sec; 2.379 sec/batch)
2018-10-15 13:56:16.374519: step 378, loss = 2.98 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 13:56:18.620840: step 379, loss = 2.85 (28.6 examples/sec; 2.241 sec/batch)
2018-10-15 13:56:20.925729: step 380, loss = 3.03 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 13:56:23.376717: step 381, loss = 2.99 (26.2 examples/sec; 2.446 sec/batch)
2018-10-15 13:56:25.729135: step 382, loss = 2.85 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 13:56:28.033747: step 383, loss = 2.99 (27.8 examples/sec; 2.302 sec/batch)
2018-10-15 13:56:30.370857: step 384, loss = 3.07 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 13:56:32.690562: step 385, loss = 2.86 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 13:56:35.013944: step 386, loss = 3.21 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 13:56:37.342041: step 387, loss = 2.84 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 13:56:39.671293: step 388, loss = 2.96 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 13:56:42.021092: step 389, loss = 2.76 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 13:56:44.351615: step 390, loss = 2.84 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 13:56:46.678777: step 391, loss = 3.01 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 13:56:48.992738: step 392, loss = 3.08 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 13:56:51.252774: step 393, loss = 3.11 (28.4 examples/sec; 2.255 sec/batch)
2018-10-15 13:56:53.589277: step 394, loss = 2.89 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 13:56:56.045948: step 395, loss = 2.86 (26.1 examples/sec; 2.452 sec/batch)
2018-10-15 13:56:58.397063: step 396, loss = 2.80 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 13:57:00.683418: step 397, loss = 2.98 (28.1 examples/sec; 2.282 sec/batch)
2018-10-15 13:57:02.978404: step 398, loss = 2.97 (27.9 examples/sec; 2.290 sec/batch)
2018-10-15 13:57:05.339510: step 399, loss = 3.10 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 13:57:07.687942: step 400, loss = 2.90 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 13:57:10.594995: step 401, loss = 3.04 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 13:57:12.932976: step 402, loss = 2.96 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 13:57:15.305314: step 403, loss = 3.16 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 13:57:17.594822: step 404, loss = 3.01 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 13:57:19.883118: step 405, loss = 2.95 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 13:57:22.278847: step 406, loss = 2.98 (26.7 examples/sec; 2.393 sec/batch)
2018-10-15 13:57:24.650850: step 407, loss = 3.05 (27.0 examples/sec; 2.366 sec/batch)
2018-10-15 13:57:26.967544: step 408, loss = 2.97 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 13:57:29.272618: step 409, loss = 3.07 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 13:57:31.707254: step 410, loss = 3.08 (26.3 examples/sec; 2.430 sec/batch)
2018-10-15 13:57:34.056422: step 411, loss = 3.12 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 13:57:36.387015: step 412, loss = 3.16 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 13:57:38.787632: step 413, loss = 3.01 (26.7 examples/sec; 2.396 sec/batch)
2018-10-15 13:57:41.175849: step 414, loss = 3.04 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 13:57:43.482796: step 415, loss = 2.97 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 13:57:45.767245: step 416, loss = 3.16 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 13:57:48.026468: step 417, loss = 2.96 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 13:57:50.359230: step 418, loss = 2.82 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 13:57:52.659966: step 419, loss = 2.90 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 13:57:54.971495: step 420, loss = 2.93 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 13:57:57.317427: step 421, loss = 3.10 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 13:57:59.643000: step 422, loss = 2.95 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 13:58:01.975849: step 423, loss = 3.05 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 13:58:04.307788: step 424, loss = 2.87 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 13:58:06.619435: step 425, loss = 2.88 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 13:58:08.958349: step 426, loss = 2.87 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 13:58:11.282395: step 427, loss = 2.96 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 13:58:13.584392: step 428, loss = 2.86 (27.9 examples/sec; 2.298 sec/batch)
2018-10-15 13:58:15.959152: step 429, loss = 2.83 (27.0 examples/sec; 2.370 sec/batch)
2018-10-15 13:58:18.191025: step 430, loss = 2.88 (28.7 examples/sec; 2.227 sec/batch)
2018-10-15 13:58:20.577944: step 431, loss = 2.91 (26.9 examples/sec; 2.382 sec/batch)
2018-10-15 13:58:22.867503: step 432, loss = 3.18 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 13:58:25.203606: step 433, loss = 2.92 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 13:58:27.623677: step 434, loss = 3.22 (26.5 examples/sec; 2.415 sec/batch)
2018-10-15 13:58:29.944133: step 435, loss = 3.14 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 13:58:32.272406: step 436, loss = 3.00 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 13:58:34.565970: step 437, loss = 2.88 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 13:58:36.879042: step 438, loss = 2.97 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 13:58:39.295942: step 439, loss = 2.99 (26.5 examples/sec; 2.412 sec/batch)
2018-10-15 13:58:41.575689: step 440, loss = 3.06 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 13:58:43.865760: step 441, loss = 2.94 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 13:58:46.239603: step 442, loss = 2.75 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 13:58:48.504943: step 443, loss = 2.98 (28.3 examples/sec; 2.261 sec/batch)
2018-10-15 13:58:50.848478: step 444, loss = 2.81 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 13:58:53.233795: step 445, loss = 2.82 (26.9 examples/sec; 2.380 sec/batch)
2018-10-15 13:58:55.633482: step 446, loss = 2.83 (26.7 examples/sec; 2.395 sec/batch)
2018-10-15 13:58:57.968092: step 447, loss = 2.78 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 13:59:00.275482: step 448, loss = 2.89 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 13:59:02.665183: step 449, loss = 2.88 (26.8 examples/sec; 2.386 sec/batch)
2018-10-15 13:59:04.902588: step 450, loss = 2.91 (28.7 examples/sec; 2.233 sec/batch)
2018-10-15 13:59:07.230313: step 451, loss = 2.70 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 13:59:09.558139: step 452, loss = 3.03 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 13:59:11.925314: step 453, loss = 3.03 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 13:59:14.250030: step 454, loss = 2.84 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 13:59:16.530127: step 455, loss = 3.02 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 13:59:18.893014: step 456, loss = 2.93 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 13:59:21.212309: step 457, loss = 2.92 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 13:59:23.496358: step 458, loss = 3.13 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 13:59:25.801711: step 459, loss = 3.15 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 13:59:28.069665: step 460, loss = 2.95 (28.3 examples/sec; 2.263 sec/batch)
2018-10-15 13:59:30.388825: step 461, loss = 3.07 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 13:59:32.738045: step 462, loss = 2.73 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 13:59:35.118548: step 463, loss = 2.83 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 13:59:37.516026: step 464, loss = 2.85 (26.8 examples/sec; 2.392 sec/batch)
2018-10-15 13:59:39.862396: step 465, loss = 2.89 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 13:59:42.136825: step 466, loss = 3.03 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 13:59:44.531649: step 467, loss = 2.88 (26.8 examples/sec; 2.390 sec/batch)
2018-10-15 13:59:46.845029: step 468, loss = 3.14 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 13:59:49.150459: step 469, loss = 3.23 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 13:59:51.478566: step 470, loss = 2.94 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 13:59:53.836542: step 471, loss = 3.00 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 13:59:56.185243: step 472, loss = 2.97 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 13:59:58.420443: step 473, loss = 2.91 (28.7 examples/sec; 2.232 sec/batch)
2018-10-15 14:00:00.743857: step 474, loss = 3.00 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 14:00:03.226435: step 475, loss = 2.81 (25.8 examples/sec; 2.477 sec/batch)
2018-10-15 14:00:05.555462: step 476, loss = 2.77 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 14:00:07.807843: step 477, loss = 2.94 (28.5 examples/sec; 2.249 sec/batch)
2018-10-15 14:00:10.210044: step 478, loss = 2.95 (26.7 examples/sec; 2.398 sec/batch)
2018-10-15 14:00:12.617704: step 479, loss = 2.93 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 14:00:14.908566: step 480, loss = 3.06 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 14:00:17.281408: step 481, loss = 2.89 (27.0 examples/sec; 2.370 sec/batch)
2018-10-15 14:00:19.674278: step 482, loss = 2.81 (26.8 examples/sec; 2.388 sec/batch)
2018-10-15 14:00:21.984439: step 483, loss = 3.10 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 14:00:24.288211: step 484, loss = 2.87 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 14:00:26.706361: step 485, loss = 2.65 (26.5 examples/sec; 2.413 sec/batch)
2018-10-15 14:00:29.011009: step 486, loss = 2.87 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 14:00:31.342863: step 487, loss = 2.80 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 14:00:33.648762: step 488, loss = 2.94 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 14:00:35.985189: step 489, loss = 3.00 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 14:00:38.308031: step 490, loss = 2.81 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 14:00:40.585386: step 491, loss = 3.02 (28.2 examples/sec; 2.272 sec/batch)
2018-10-15 14:00:42.924728: step 492, loss = 2.93 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 14:00:45.186933: step 493, loss = 2.84 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 14:00:47.545988: step 494, loss = 3.01 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 14:00:49.809037: step 495, loss = 2.96 (28.3 examples/sec; 2.260 sec/batch)
2018-10-15 14:00:52.175359: step 496, loss = 2.84 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 14:00:54.559799: step 497, loss = 2.81 (26.9 examples/sec; 2.380 sec/batch)
2018-10-15 14:00:56.886441: step 498, loss = 2.64 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:00:59.199960: step 499, loss = 3.09 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 14:01:01.454244: step 500, loss = 2.94 (28.5 examples/sec; 2.249 sec/batch)
2018-10-15 14:01:04.253417: step 501, loss = 2.73 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 14:01:06.632392: step 502, loss = 2.82 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:01:08.974202: step 503, loss = 2.77 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 14:01:11.387366: step 504, loss = 2.78 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 14:01:13.668645: step 505, loss = 2.96 (28.1 examples/sec; 2.278 sec/batch)
2018-10-15 14:01:15.997478: step 506, loss = 2.76 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 14:01:18.402722: step 507, loss = 2.91 (26.6 examples/sec; 2.402 sec/batch)
2018-10-15 14:01:20.787069: step 508, loss = 2.51 (26.9 examples/sec; 2.379 sec/batch)
2018-10-15 14:01:23.099027: step 509, loss = 2.94 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 14:01:25.363054: step 510, loss = 2.77 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 14:01:27.742403: step 511, loss = 2.84 (27.0 examples/sec; 2.374 sec/batch)
2018-10-15 14:01:30.122944: step 512, loss = 2.85 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:01:32.511899: step 513, loss = 2.97 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 14:01:34.864039: step 514, loss = 2.80 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 14:01:37.272345: step 515, loss = 2.80 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 14:01:39.608473: step 516, loss = 2.84 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 14:01:41.916442: step 517, loss = 2.75 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 14:01:44.243480: step 518, loss = 2.76 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 14:01:46.517301: step 519, loss = 2.97 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 14:01:48.798900: step 520, loss = 2.94 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 14:01:51.141860: step 521, loss = 2.88 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 14:01:53.512318: step 522, loss = 2.77 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 14:01:55.943222: step 523, loss = 2.81 (26.4 examples/sec; 2.428 sec/batch)
2018-10-15 14:01:58.237082: step 524, loss = 2.75 (28.0 examples/sec; 2.288 sec/batch)
2018-10-15 14:02:00.592268: step 525, loss = 2.82 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 14:02:02.951049: step 526, loss = 2.84 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 14:02:05.248670: step 527, loss = 2.67 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 14:02:07.548045: step 528, loss = 2.72 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 14:02:09.930176: step 529, loss = 2.88 (26.9 examples/sec; 2.377 sec/batch)
2018-10-15 14:02:12.223584: step 530, loss = 2.70 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 14:02:14.590394: step 531, loss = 3.09 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 14:02:16.970043: step 532, loss = 2.96 (27.0 examples/sec; 2.375 sec/batch)
2018-10-15 14:02:19.361436: step 533, loss = 3.02 (26.8 examples/sec; 2.387 sec/batch)
2018-10-15 14:02:21.741790: step 534, loss = 3.06 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:02:24.120232: step 535, loss = 2.82 (27.0 examples/sec; 2.374 sec/batch)
2018-10-15 14:02:26.421236: step 536, loss = 2.98 (27.8 examples/sec; 2.298 sec/batch)
2018-10-15 14:02:28.700455: step 537, loss = 3.23 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 14:02:31.031922: step 538, loss = 3.16 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 14:02:33.488986: step 539, loss = 3.00 (26.1 examples/sec; 2.452 sec/batch)
2018-10-15 14:02:35.850939: step 540, loss = 2.85 (27.1 examples/sec; 2.357 sec/batch)
2018-10-15 14:02:38.161942: step 541, loss = 3.04 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 14:02:40.480163: step 542, loss = 3.05 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 14:02:42.844110: step 543, loss = 3.09 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 14:02:45.256940: step 544, loss = 2.80 (26.6 examples/sec; 2.410 sec/batch)
2018-10-15 14:02:47.579563: step 545, loss = 2.98 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 14:02:49.927936: step 546, loss = 2.88 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 14:02:52.313556: step 547, loss = 3.03 (26.9 examples/sec; 2.382 sec/batch)
2018-10-15 14:02:54.675021: step 548, loss = 2.72 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 14:02:57.013159: step 549, loss = 2.75 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 14:02:59.351180: step 550, loss = 2.81 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 14:03:01.771867: step 551, loss = 2.76 (26.5 examples/sec; 2.416 sec/batch)
2018-10-15 14:03:04.150633: step 552, loss = 3.05 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:03:06.412486: step 553, loss = 3.10 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 14:03:08.729931: step 554, loss = 2.87 (27.7 examples/sec; 2.315 sec/batch)
2018-10-15 14:03:11.009557: step 555, loss = 2.95 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 14:03:13.390238: step 556, loss = 2.97 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:03:15.648353: step 557, loss = 3.00 (28.4 examples/sec; 2.252 sec/batch)
2018-10-15 14:03:17.937436: step 558, loss = 2.89 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 14:03:20.347465: step 559, loss = 2.88 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 14:03:22.683193: step 560, loss = 2.70 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 14:03:24.939806: step 561, loss = 2.81 (28.4 examples/sec; 2.252 sec/batch)
2018-10-15 14:03:27.203799: step 562, loss = 2.80 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 14:03:29.531853: step 563, loss = 2.81 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 14:03:31.822348: step 564, loss = 2.71 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 14:03:34.082583: step 565, loss = 2.80 (28.4 examples/sec; 2.255 sec/batch)
2018-10-15 14:03:36.463706: step 566, loss = 2.64 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:03:38.834859: step 567, loss = 2.73 (27.0 examples/sec; 2.366 sec/batch)
2018-10-15 14:03:41.205932: step 568, loss = 2.69 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 14:03:43.493361: step 569, loss = 2.83 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 14:03:45.793719: step 570, loss = 2.70 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 14:03:48.147386: step 571, loss = 2.76 (27.3 examples/sec; 2.349 sec/batch)
2018-10-15 14:03:50.418921: step 572, loss = 2.81 (28.2 examples/sec; 2.267 sec/batch)
2018-10-15 14:03:52.746953: step 573, loss = 2.69 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 14:03:55.032116: step 574, loss = 2.91 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 14:03:57.292301: step 575, loss = 2.81 (28.4 examples/sec; 2.255 sec/batch)
2018-10-15 14:03:59.609041: step 576, loss = 3.11 (27.7 examples/sec; 2.311 sec/batch)
2018-10-15 14:04:01.906486: step 577, loss = 2.86 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 14:04:04.295503: step 578, loss = 2.81 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 14:04:06.731050: step 579, loss = 2.87 (26.3 examples/sec; 2.431 sec/batch)
2018-10-15 14:04:09.058812: step 580, loss = 2.65 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:04:11.381263: step 581, loss = 2.97 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 14:04:13.733834: step 582, loss = 3.07 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 14:04:16.107657: step 583, loss = 2.83 (27.0 examples/sec; 2.368 sec/batch)
2018-10-15 14:04:18.380891: step 584, loss = 2.83 (28.2 examples/sec; 2.268 sec/batch)
2018-10-15 14:04:20.680195: step 585, loss = 2.94 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 14:04:22.967350: step 586, loss = 2.99 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 14:04:25.301215: step 587, loss = 3.06 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 14:04:27.624615: step 588, loss = 3.12 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 14:04:29.961158: step 589, loss = 2.70 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 14:04:32.323753: step 590, loss = 2.81 (27.1 examples/sec; 2.357 sec/batch)
2018-10-15 14:04:34.639169: step 591, loss = 2.69 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 14:04:37.011843: step 592, loss = 2.77 (27.0 examples/sec; 2.368 sec/batch)
2018-10-15 14:04:39.373090: step 593, loss = 2.88 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 14:04:41.693538: step 594, loss = 3.02 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 14:04:44.024406: step 595, loss = 3.09 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 14:04:46.366457: step 596, loss = 2.95 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 14:04:48.718116: step 597, loss = 2.57 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 14:04:51.061459: step 598, loss = 3.00 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 14:04:53.424356: step 599, loss = 2.80 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 14:04:55.765494: step 600, loss = 2.67 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 14:04:58.739502: step 601, loss = 2.86 (27.8 examples/sec; 2.299 sec/batch)
2018-10-15 14:05:01.096493: step 602, loss = 2.84 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 14:05:03.527435: step 603, loss = 2.77 (26.4 examples/sec; 2.422 sec/batch)
2018-10-15 14:05:05.829075: step 604, loss = 2.70 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 14:05:08.086542: step 605, loss = 2.88 (28.4 examples/sec; 2.252 sec/batch)
2018-10-15 14:05:10.422412: step 606, loss = 2.89 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 14:05:12.743970: step 607, loss = 3.05 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 14:05:15.151461: step 608, loss = 2.82 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 14:05:17.495460: step 609, loss = 2.97 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 14:05:19.907252: step 610, loss = 2.85 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 14:05:22.249579: step 611, loss = 2.67 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 14:05:24.575815: step 612, loss = 2.94 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:05:26.788973: step 613, loss = 2.95 (29.0 examples/sec; 2.208 sec/batch)
2018-10-15 14:05:29.204001: step 614, loss = 2.68 (26.5 examples/sec; 2.412 sec/batch)
2018-10-15 14:05:31.525701: step 615, loss = 2.85 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 14:05:33.868803: step 616, loss = 2.87 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 14:05:36.303579: step 617, loss = 2.81 (26.3 examples/sec; 2.430 sec/batch)
2018-10-15 14:05:38.748575: step 618, loss = 2.85 (26.2 examples/sec; 2.440 sec/batch)
2018-10-15 14:05:41.038624: step 619, loss = 2.87 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 14:05:43.389024: step 620, loss = 2.96 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 14:05:45.759066: step 621, loss = 3.07 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 14:05:48.081996: step 622, loss = 2.80 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 14:05:50.414495: step 623, loss = 2.80 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 14:05:52.746856: step 624, loss = 2.99 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 14:05:55.065770: step 625, loss = 2.90 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 14:05:57.393732: step 626, loss = 2.90 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 14:05:59.738742: step 627, loss = 2.82 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 14:06:02.026106: step 628, loss = 2.86 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 14:06:04.313883: step 629, loss = 3.01 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 14:06:06.598397: step 630, loss = 2.64 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 14:06:08.912550: step 631, loss = 2.73 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 14:06:11.282264: step 632, loss = 2.59 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 14:06:13.698843: step 633, loss = 2.72 (26.5 examples/sec; 2.411 sec/batch)
2018-10-15 14:06:16.016425: step 634, loss = 2.85 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 14:06:18.335806: step 635, loss = 2.59 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 14:06:20.602044: step 636, loss = 2.56 (28.3 examples/sec; 2.262 sec/batch)
2018-10-15 14:06:22.845288: step 637, loss = 2.69 (28.6 examples/sec; 2.238 sec/batch)
2018-10-15 14:06:25.150847: step 638, loss = 2.91 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 14:06:27.463213: step 639, loss = 2.78 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 14:06:29.916224: step 640, loss = 2.84 (26.1 examples/sec; 2.448 sec/batch)
2018-10-15 14:06:32.253435: step 641, loss = 3.03 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 14:06:34.617352: step 642, loss = 2.71 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 14:06:36.916305: step 643, loss = 2.77 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 14:06:39.320951: step 644, loss = 2.79 (26.7 examples/sec; 2.399 sec/batch)
2018-10-15 14:06:41.662105: step 645, loss = 2.91 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 14:06:43.937168: step 646, loss = 2.75 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 14:06:46.310967: step 647, loss = 2.76 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 14:06:48.551195: step 648, loss = 2.76 (28.6 examples/sec; 2.235 sec/batch)
2018-10-15 14:06:50.860316: step 649, loss = 2.72 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 14:06:53.221280: step 650, loss = 2.73 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 14:06:55.682421: step 651, loss = 2.73 (26.1 examples/sec; 2.456 sec/batch)
2018-10-15 14:06:58.040522: step 652, loss = 2.63 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 14:07:00.297669: step 653, loss = 2.84 (28.4 examples/sec; 2.252 sec/batch)
2018-10-15 14:07:02.612571: step 654, loss = 2.53 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 14:07:04.987004: step 655, loss = 2.69 (27.0 examples/sec; 2.372 sec/batch)
2018-10-15 14:07:07.220059: step 656, loss = 2.64 (28.7 examples/sec; 2.228 sec/batch)
2018-10-15 14:07:09.614868: step 657, loss = 2.75 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 14:07:12.050349: step 658, loss = 2.77 (26.3 examples/sec; 2.433 sec/batch)
2018-10-15 14:07:14.357838: step 659, loss = 2.62 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 14:07:16.616494: step 660, loss = 2.59 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 14:07:18.974065: step 661, loss = 2.73 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 14:07:21.259497: step 662, loss = 2.84 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 14:07:23.643665: step 663, loss = 2.65 (26.9 examples/sec; 2.379 sec/batch)
2018-10-15 14:07:25.991400: step 664, loss = 2.95 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 14:07:28.341759: step 665, loss = 2.93 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 14:07:30.724516: step 666, loss = 3.09 (26.9 examples/sec; 2.378 sec/batch)
2018-10-15 14:07:33.049662: step 667, loss = 2.91 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:07:35.363899: step 668, loss = 2.88 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 14:07:37.668044: step 669, loss = 2.81 (27.8 examples/sec; 2.298 sec/batch)
2018-10-15 14:07:39.962624: step 670, loss = 2.73 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 14:07:42.239204: step 671, loss = 2.72 (28.2 examples/sec; 2.271 sec/batch)
2018-10-15 14:07:44.568640: step 672, loss = 2.77 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 14:07:46.938439: step 673, loss = 2.83 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 14:07:49.286897: step 674, loss = 2.89 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 14:07:51.628437: step 675, loss = 2.79 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 14:07:53.989997: step 676, loss = 3.13 (27.1 examples/sec; 2.357 sec/batch)
2018-10-15 14:07:56.308560: step 677, loss = 2.68 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 14:07:58.647238: step 678, loss = 2.88 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 14:08:00.995507: step 679, loss = 2.84 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 14:08:03.369641: step 680, loss = 3.00 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 14:08:05.796009: step 681, loss = 2.74 (26.4 examples/sec; 2.423 sec/batch)
2018-10-15 14:08:08.250590: step 682, loss = 2.76 (26.1 examples/sec; 2.450 sec/batch)
2018-10-15 14:08:10.537805: step 683, loss = 2.62 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 14:08:12.866483: step 684, loss = 2.84 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 14:08:15.217274: step 685, loss = 3.05 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 14:08:17.419544: step 686, loss = 2.86 (29.1 examples/sec; 2.197 sec/batch)
2018-10-15 14:08:19.696664: step 687, loss = 2.64 (28.2 examples/sec; 2.272 sec/batch)
2018-10-15 14:08:22.180629: step 688, loss = 2.79 (25.8 examples/sec; 2.479 sec/batch)
2018-10-15 14:08:24.538192: step 689, loss = 2.78 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 14:08:26.778119: step 690, loss = 2.57 (28.6 examples/sec; 2.235 sec/batch)
2018-10-15 14:08:29.087417: step 691, loss = 2.82 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 14:08:31.536761: step 692, loss = 2.59 (26.2 examples/sec; 2.446 sec/batch)
2018-10-15 14:08:33.852247: step 693, loss = 2.50 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 14:08:36.195016: step 694, loss = 2.79 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 14:08:38.504295: step 695, loss = 3.06 (27.8 examples/sec; 2.304 sec/batch)
2018-10-15 14:08:40.889159: step 696, loss = 2.72 (26.9 examples/sec; 2.379 sec/batch)
2018-10-15 14:08:43.190361: step 697, loss = 2.75 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 14:08:45.471560: step 698, loss = 2.54 (28.1 examples/sec; 2.276 sec/batch)
2018-10-15 14:08:47.878912: step 699, loss = 2.92 (26.7 examples/sec; 2.401 sec/batch)
2018-10-15 14:08:50.172712: step 700, loss = 2.91 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 14:08:53.014166: step 701, loss = 2.69 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 14:08:55.367092: step 702, loss = 2.68 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 14:08:57.654692: step 703, loss = 2.56 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 14:08:59.986668: step 704, loss = 2.72 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 14:09:02.389890: step 705, loss = 2.66 (26.7 examples/sec; 2.398 sec/batch)
2018-10-15 14:09:04.774765: step 706, loss = 2.67 (26.9 examples/sec; 2.380 sec/batch)
2018-10-15 14:09:07.042452: step 707, loss = 2.49 (28.3 examples/sec; 2.263 sec/batch)
2018-10-15 14:09:09.429060: step 708, loss = 2.53 (26.9 examples/sec; 2.381 sec/batch)
2018-10-15 14:09:11.912455: step 709, loss = 2.46 (25.8 examples/sec; 2.478 sec/batch)
2018-10-15 14:09:14.218003: step 710, loss = 2.58 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 14:09:16.542130: step 711, loss = 2.57 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 14:09:18.816985: step 712, loss = 2.59 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 14:09:21.108468: step 713, loss = 2.93 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 14:09:23.455452: step 714, loss = 2.68 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 14:09:25.759358: step 715, loss = 2.58 (27.8 examples/sec; 2.299 sec/batch)
2018-10-15 14:09:28.098292: step 716, loss = 2.63 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 14:09:30.405907: step 717, loss = 2.62 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 14:09:32.692832: step 718, loss = 2.58 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 14:09:34.977038: step 719, loss = 2.86 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 14:09:37.380101: step 720, loss = 2.63 (26.7 examples/sec; 2.399 sec/batch)
2018-10-15 14:09:39.663989: step 721, loss = 2.64 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 14:09:42.032537: step 722, loss = 2.83 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 14:09:44.319391: step 723, loss = 2.70 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 14:09:46.677530: step 724, loss = 2.62 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 14:09:49.063332: step 725, loss = 2.81 (26.9 examples/sec; 2.380 sec/batch)
2018-10-15 14:09:51.390382: step 726, loss = 2.57 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:09:53.705090: step 727, loss = 2.63 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 14:09:56.076675: step 728, loss = 2.69 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 14:09:58.436561: step 729, loss = 2.67 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 14:10:00.713277: step 730, loss = 2.71 (28.2 examples/sec; 2.272 sec/batch)
2018-10-15 14:10:03.091915: step 731, loss = 2.66 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:10:05.458436: step 732, loss = 2.88 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 14:10:07.783824: step 733, loss = 2.59 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 14:10:10.135604: step 734, loss = 2.77 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 14:10:12.549193: step 735, loss = 2.95 (26.6 examples/sec; 2.409 sec/batch)
2018-10-15 14:10:14.904167: step 736, loss = 2.56 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 14:10:17.270781: step 737, loss = 2.75 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 14:10:19.611650: step 738, loss = 2.63 (27.4 examples/sec; 2.336 sec/batch)
2018-10-15 14:10:22.023699: step 739, loss = 2.85 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 14:10:24.311674: step 740, loss = 2.78 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 14:10:26.599516: step 741, loss = 2.84 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 14:10:28.973693: step 742, loss = 2.66 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 14:10:31.316373: step 743, loss = 2.81 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 14:10:33.651979: step 744, loss = 2.72 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 14:10:36.051711: step 745, loss = 2.75 (26.7 examples/sec; 2.395 sec/batch)
2018-10-15 14:10:38.401624: step 746, loss = 2.77 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 14:10:40.750190: step 747, loss = 2.96 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 14:10:43.075452: step 748, loss = 2.84 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 14:10:45.402934: step 749, loss = 2.81 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:10:47.758118: step 750, loss = 2.67 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 14:10:50.084512: step 751, loss = 2.74 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 14:10:52.376928: step 752, loss = 2.70 (28.0 examples/sec; 2.288 sec/batch)
2018-10-15 14:10:54.661631: step 753, loss = 2.64 (28.1 examples/sec; 2.281 sec/batch)
2018-10-15 14:10:56.966693: step 754, loss = 2.53 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 14:10:59.215942: step 755, loss = 2.55 (28.5 examples/sec; 2.244 sec/batch)
2018-10-15 14:11:01.449140: step 756, loss = 2.78 (28.7 examples/sec; 2.228 sec/batch)
2018-10-15 14:11:03.785850: step 757, loss = 2.74 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 14:11:06.126176: step 758, loss = 2.68 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 14:11:08.476337: step 759, loss = 2.94 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 14:11:10.844386: step 760, loss = 2.77 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 14:11:13.209431: step 761, loss = 2.66 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 14:11:15.492210: step 762, loss = 2.82 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 14:11:17.810400: step 763, loss = 2.74 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 14:11:20.197596: step 764, loss = 2.78 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 14:11:22.615850: step 765, loss = 2.67 (26.5 examples/sec; 2.414 sec/batch)
2018-10-15 14:11:24.909533: step 766, loss = 2.64 (28.0 examples/sec; 2.290 sec/batch)
2018-10-15 14:11:27.310199: step 767, loss = 2.75 (26.7 examples/sec; 2.396 sec/batch)
2018-10-15 14:11:29.720917: step 768, loss = 2.67 (26.6 examples/sec; 2.406 sec/batch)
2018-10-15 14:11:32.064265: step 769, loss = 2.87 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 14:11:34.446775: step 770, loss = 2.71 (26.9 examples/sec; 2.377 sec/batch)
2018-10-15 14:11:36.806667: step 771, loss = 2.65 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 14:11:39.146063: step 772, loss = 2.69 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 14:11:41.489718: step 773, loss = 2.89 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 14:11:43.870718: step 774, loss = 2.54 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:11:46.211476: step 775, loss = 2.69 (27.4 examples/sec; 2.336 sec/batch)
2018-10-15 14:11:48.454257: step 776, loss = 2.54 (28.6 examples/sec; 2.238 sec/batch)
2018-10-15 14:11:50.807931: step 777, loss = 2.59 (27.2 examples/sec; 2.349 sec/batch)
2018-10-15 14:11:53.171443: step 778, loss = 2.65 (27.1 examples/sec; 2.359 sec/batch)
2018-10-15 14:11:55.477701: step 779, loss = 2.88 (27.8 examples/sec; 2.302 sec/batch)
2018-10-15 14:11:57.772607: step 780, loss = 2.60 (27.9 examples/sec; 2.290 sec/batch)
2018-10-15 14:12:00.052008: step 781, loss = 2.65 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 14:12:02.356725: step 782, loss = 2.59 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 14:12:04.695061: step 783, loss = 2.64 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 14:12:07.013938: step 784, loss = 2.79 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 14:12:09.372437: step 785, loss = 2.75 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 14:12:11.709413: step 786, loss = 2.94 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 14:12:14.084200: step 787, loss = 2.70 (27.0 examples/sec; 2.370 sec/batch)
2018-10-15 14:12:16.431735: step 788, loss = 2.67 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 14:12:18.708402: step 789, loss = 2.83 (28.2 examples/sec; 2.272 sec/batch)
2018-10-15 14:12:21.124662: step 790, loss = 2.77 (26.5 examples/sec; 2.411 sec/batch)
2018-10-15 14:12:23.473064: step 791, loss = 2.55 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 14:12:25.839894: step 792, loss = 2.69 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 14:12:28.266809: step 793, loss = 2.76 (26.4 examples/sec; 2.422 sec/batch)
2018-10-15 14:12:30.603577: step 794, loss = 2.90 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 14:12:32.919006: step 795, loss = 2.70 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 14:12:35.279572: step 796, loss = 2.60 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 14:12:37.653286: step 797, loss = 2.69 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 14:12:40.049713: step 798, loss = 2.84 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 14:12:42.408692: step 799, loss = 2.82 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 14:12:44.816054: step 800, loss = 2.46 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 14:12:47.850372: step 801, loss = 2.76 (26.7 examples/sec; 2.395 sec/batch)
2018-10-15 14:12:50.195300: step 802, loss = 2.58 (27.4 examples/sec; 2.340 sec/batch)
2018-10-15 14:12:52.539767: step 803, loss = 2.56 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 14:12:54.959339: step 804, loss = 2.59 (26.5 examples/sec; 2.414 sec/batch)
2018-10-15 14:12:57.293610: step 805, loss = 2.59 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 14:12:59.638455: step 806, loss = 2.70 (27.3 examples/sec; 2.340 sec/batch)
2018-10-15 14:13:01.979466: step 807, loss = 2.62 (27.4 examples/sec; 2.336 sec/batch)
2018-10-15 14:13:04.351527: step 808, loss = 2.73 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 14:13:06.613325: step 809, loss = 3.02 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 14:13:08.875184: step 810, loss = 2.62 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 14:13:11.189155: step 811, loss = 2.71 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 14:13:13.604429: step 812, loss = 2.73 (26.5 examples/sec; 2.411 sec/batch)
2018-10-15 14:13:15.918832: step 813, loss = 2.66 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 14:13:18.310706: step 814, loss = 2.74 (26.8 examples/sec; 2.387 sec/batch)
2018-10-15 14:13:20.631941: step 815, loss = 2.87 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 14:13:22.958850: step 816, loss = 2.82 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 14:13:25.273348: step 817, loss = 2.69 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 14:13:27.678369: step 818, loss = 2.59 (26.7 examples/sec; 2.400 sec/batch)
2018-10-15 14:13:30.049560: step 819, loss = 2.77 (27.1 examples/sec; 2.366 sec/batch)
2018-10-15 14:13:32.366078: step 820, loss = 2.62 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 14:13:34.747703: step 821, loss = 2.76 (26.9 examples/sec; 2.377 sec/batch)
2018-10-15 14:13:37.106925: step 822, loss = 2.66 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 14:13:39.387299: step 823, loss = 2.82 (28.1 examples/sec; 2.276 sec/batch)
2018-10-15 14:13:41.769209: step 824, loss = 2.97 (26.9 examples/sec; 2.379 sec/batch)
2018-10-15 14:13:44.091927: step 825, loss = 2.67 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 14:13:46.541020: step 826, loss = 2.69 (26.2 examples/sec; 2.444 sec/batch)
2018-10-15 14:13:48.908162: step 827, loss = 2.70 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 14:13:51.276082: step 828, loss = 2.86 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 14:13:53.701283: step 829, loss = 2.80 (26.4 examples/sec; 2.422 sec/batch)
2018-10-15 14:13:56.018325: step 830, loss = 2.73 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 14:13:58.331648: step 831, loss = 2.91 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 14:14:00.723314: step 832, loss = 2.92 (26.8 examples/sec; 2.387 sec/batch)
2018-10-15 14:14:03.059561: step 833, loss = 2.81 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 14:14:05.439853: step 834, loss = 2.74 (26.9 examples/sec; 2.375 sec/batch)
2018-10-15 14:14:07.752668: step 835, loss = 2.87 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 14:14:10.103061: step 836, loss = 2.52 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 14:14:12.411642: step 837, loss = 2.58 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 14:14:14.656666: step 838, loss = 2.66 (28.6 examples/sec; 2.240 sec/batch)
2018-10-15 14:14:17.076003: step 839, loss = 2.70 (26.5 examples/sec; 2.414 sec/batch)
2018-10-15 14:14:19.329002: step 840, loss = 2.59 (28.5 examples/sec; 2.248 sec/batch)
2018-10-15 14:14:21.656596: step 841, loss = 2.64 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:14:24.025770: step 842, loss = 2.70 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 14:14:26.420349: step 843, loss = 2.44 (26.8 examples/sec; 2.389 sec/batch)
2018-10-15 14:14:28.727839: step 844, loss = 2.53 (27.8 examples/sec; 2.302 sec/batch)
2018-10-15 14:14:31.039576: step 845, loss = 2.79 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 14:14:33.343631: step 846, loss = 2.79 (27.8 examples/sec; 2.299 sec/batch)
2018-10-15 14:14:35.772098: step 847, loss = 2.64 (26.4 examples/sec; 2.424 sec/batch)
2018-10-15 14:14:38.141423: step 848, loss = 2.89 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 14:14:40.437627: step 849, loss = 2.67 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 14:14:42.751007: step 850, loss = 2.70 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 14:14:45.109878: step 851, loss = 2.98 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 14:14:47.498455: step 852, loss = 2.60 (26.8 examples/sec; 2.386 sec/batch)
2018-10-15 14:14:49.836152: step 853, loss = 2.75 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 14:14:52.198796: step 854, loss = 2.68 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 14:14:54.574990: step 855, loss = 2.69 (27.0 examples/sec; 2.373 sec/batch)
2018-10-15 14:14:56.892930: step 856, loss = 2.56 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 14:14:59.214945: step 857, loss = 2.57 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 14:15:01.701784: step 858, loss = 2.70 (25.8 examples/sec; 2.482 sec/batch)
2018-10-15 14:15:04.095572: step 859, loss = 2.63 (26.8 examples/sec; 2.390 sec/batch)
2018-10-15 14:15:06.390018: step 860, loss = 2.73 (28.0 examples/sec; 2.290 sec/batch)
2018-10-15 14:15:08.693940: step 861, loss = 2.46 (27.8 examples/sec; 2.299 sec/batch)
2018-10-15 14:15:11.048194: step 862, loss = 2.69 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 14:15:13.415434: step 863, loss = 2.57 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 14:15:15.781768: step 864, loss = 2.42 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 14:15:18.035028: step 865, loss = 2.50 (28.5 examples/sec; 2.248 sec/batch)
2018-10-15 14:15:20.448401: step 866, loss = 2.65 (26.6 examples/sec; 2.409 sec/batch)
2018-10-15 14:15:22.810815: step 867, loss = 2.68 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 14:15:25.136674: step 868, loss = 2.64 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 14:15:27.548088: step 869, loss = 2.62 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 14:15:29.874828: step 870, loss = 2.74 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:15:32.196105: step 871, loss = 2.80 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 14:15:34.484644: step 872, loss = 2.69 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 14:15:36.728963: step 873, loss = 2.91 (28.6 examples/sec; 2.239 sec/batch)
2018-10-15 14:15:38.946691: step 874, loss = 2.58 (28.9 examples/sec; 2.216 sec/batch)
2018-10-15 14:15:41.234149: step 875, loss = 2.65 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 14:15:43.614025: step 876, loss = 2.54 (26.9 examples/sec; 2.377 sec/batch)
2018-10-15 14:15:46.062019: step 877, loss = 2.63 (26.2 examples/sec; 2.443 sec/batch)
2018-10-15 14:15:48.333975: step 878, loss = 2.51 (28.2 examples/sec; 2.267 sec/batch)
2018-10-15 14:15:50.655177: step 879, loss = 2.64 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 14:15:53.018459: step 880, loss = 2.90 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 14:15:55.342726: step 881, loss = 2.79 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 14:15:57.669723: step 882, loss = 2.90 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:15:59.959799: step 883, loss = 2.69 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 14:16:02.372044: step 884, loss = 2.84 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 14:16:04.700713: step 885, loss = 2.83 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 14:16:06.989588: step 886, loss = 2.57 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 14:16:09.335879: step 887, loss = 2.63 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 14:16:11.695097: step 888, loss = 2.92 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 14:16:14.056847: step 889, loss = 2.76 (27.1 examples/sec; 2.359 sec/batch)
2018-10-15 14:16:16.419938: step 890, loss = 2.55 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 14:16:18.743600: step 891, loss = 2.61 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 14:16:21.039241: step 892, loss = 2.71 (27.9 examples/sec; 2.291 sec/batch)
2018-10-15 14:16:23.314144: step 893, loss = 2.72 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 14:16:25.689461: step 894, loss = 2.68 (27.0 examples/sec; 2.370 sec/batch)
2018-10-15 14:16:28.039235: step 895, loss = 2.59 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 14:16:30.333306: step 896, loss = 2.51 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 14:16:32.659967: step 897, loss = 2.57 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 14:16:34.998714: step 898, loss = 2.61 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 14:16:37.429938: step 899, loss = 2.68 (26.4 examples/sec; 2.427 sec/batch)
2018-10-15 14:16:39.719820: step 900, loss = 2.70 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 14:16:42.501785: step 901, loss = 2.77 (28.4 examples/sec; 2.251 sec/batch)
2018-10-15 14:16:44.888442: step 902, loss = 2.78 (26.9 examples/sec; 2.381 sec/batch)
2018-10-15 14:16:47.232737: step 903, loss = 2.83 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 14:16:49.566628: step 904, loss = 2.75 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 14:16:51.839145: step 905, loss = 2.74 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 14:16:54.204476: step 906, loss = 2.63 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 14:16:56.542326: step 907, loss = 2.66 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 14:16:58.896263: step 908, loss = 2.63 (27.2 examples/sec; 2.349 sec/batch)
2018-10-15 14:17:01.270523: step 909, loss = 2.72 (27.0 examples/sec; 2.370 sec/batch)
2018-10-15 14:17:03.606820: step 910, loss = 2.59 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 14:17:05.807376: step 911, loss = 2.41 (29.1 examples/sec; 2.196 sec/batch)
2018-10-15 14:17:08.122947: step 912, loss = 2.55 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 14:17:10.467576: step 913, loss = 2.69 (27.3 examples/sec; 2.340 sec/batch)
2018-10-15 14:17:12.823905: step 914, loss = 2.50 (27.2 examples/sec; 2.351 sec/batch)
2018-10-15 14:17:15.110080: step 915, loss = 2.40 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 14:17:17.464399: step 916, loss = 2.50 (27.2 examples/sec; 2.349 sec/batch)
2018-10-15 14:17:19.882569: step 917, loss = 2.77 (26.5 examples/sec; 2.413 sec/batch)
2018-10-15 14:17:22.183409: step 918, loss = 2.46 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 14:17:24.521208: step 919, loss = 2.36 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 14:17:26.947764: step 920, loss = 2.66 (26.4 examples/sec; 2.421 sec/batch)
2018-10-15 14:17:29.278726: step 921, loss = 2.57 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 14:17:31.615683: step 922, loss = 2.49 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 14:17:33.987943: step 923, loss = 2.58 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 14:17:36.400543: step 924, loss = 2.58 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 14:17:38.751011: step 925, loss = 2.53 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 14:17:41.007211: step 926, loss = 2.49 (28.4 examples/sec; 2.251 sec/batch)
2018-10-15 14:17:43.368546: step 927, loss = 2.51 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 14:17:45.822202: step 928, loss = 2.57 (26.1 examples/sec; 2.449 sec/batch)
2018-10-15 14:17:48.084608: step 929, loss = 2.60 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 14:17:50.413697: step 930, loss = 2.45 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 14:17:52.802452: step 931, loss = 2.49 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 14:17:55.155185: step 932, loss = 2.61 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 14:17:57.534964: step 933, loss = 2.52 (26.9 examples/sec; 2.375 sec/batch)
2018-10-15 14:17:59.771009: step 934, loss = 2.58 (28.7 examples/sec; 2.231 sec/batch)
2018-10-15 14:18:02.117586: step 935, loss = 2.63 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 14:18:04.452407: step 936, loss = 2.66 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 14:18:06.782078: step 937, loss = 2.41 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 14:18:09.142723: step 938, loss = 2.57 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 14:18:11.523772: step 939, loss = 2.60 (26.9 examples/sec; 2.376 sec/batch)
2018-10-15 14:18:13.890274: step 940, loss = 2.50 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 14:18:16.200561: step 941, loss = 2.62 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 14:18:18.588495: step 942, loss = 2.58 (26.9 examples/sec; 2.382 sec/batch)
2018-10-15 14:18:20.918627: step 943, loss = 2.54 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 14:18:23.172060: step 944, loss = 2.61 (28.5 examples/sec; 2.248 sec/batch)
2018-10-15 14:18:25.454556: step 945, loss = 2.77 (28.1 examples/sec; 2.278 sec/batch)
2018-10-15 14:18:27.777868: step 946, loss = 2.63 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 14:18:30.090227: step 947, loss = 2.58 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 14:18:32.362951: step 948, loss = 2.68 (28.2 examples/sec; 2.268 sec/batch)
2018-10-15 14:18:34.628218: step 949, loss = 2.81 (28.3 examples/sec; 2.260 sec/batch)
2018-10-15 14:18:36.984321: step 950, loss = 2.73 (27.2 examples/sec; 2.351 sec/batch)
2018-10-15 14:18:39.429263: step 951, loss = 2.49 (26.2 examples/sec; 2.442 sec/batch)
2018-10-15 14:18:41.695951: step 952, loss = 2.64 (28.3 examples/sec; 2.262 sec/batch)
2018-10-15 14:18:43.958596: step 953, loss = 2.67 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 14:18:46.405080: step 954, loss = 2.69 (26.2 examples/sec; 2.444 sec/batch)
2018-10-15 14:18:48.725033: step 955, loss = 2.61 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 14:18:51.033246: step 956, loss = 2.67 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 14:18:53.318228: step 957, loss = 2.67 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 14:18:55.673638: step 958, loss = 2.71 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 14:18:57.975468: step 959, loss = 2.81 (27.9 examples/sec; 2.297 sec/batch)
2018-10-15 14:19:00.353846: step 960, loss = 2.82 (27.0 examples/sec; 2.373 sec/batch)
2018-10-15 14:19:02.766596: step 961, loss = 2.58 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 14:19:05.117564: step 962, loss = 2.78 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 14:19:07.445150: step 963, loss = 2.53 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:19:09.799496: step 964, loss = 2.60 (27.2 examples/sec; 2.349 sec/batch)
2018-10-15 14:19:12.125346: step 965, loss = 2.73 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 14:19:14.442030: step 966, loss = 2.90 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 14:19:16.752925: step 967, loss = 2.81 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 14:19:19.022949: step 968, loss = 2.58 (28.3 examples/sec; 2.265 sec/batch)
2018-10-15 14:19:21.411528: step 969, loss = 2.67 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 14:19:23.719839: step 970, loss = 2.57 (27.8 examples/sec; 2.304 sec/batch)
2018-10-15 14:19:26.046754: step 971, loss = 2.71 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:19:28.338400: step 972, loss = 2.59 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 14:19:30.717892: step 973, loss = 2.68 (27.0 examples/sec; 2.374 sec/batch)
2018-10-15 14:19:33.042041: step 974, loss = 2.67 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 14:19:35.247042: step 975, loss = 2.61 (29.1 examples/sec; 2.202 sec/batch)
2018-10-15 14:19:37.581045: step 976, loss = 2.59 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 14:19:39.886696: step 977, loss = 2.58 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 14:19:42.234653: step 978, loss = 2.66 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 14:19:44.522108: step 979, loss = 2.65 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 14:19:46.811348: step 980, loss = 2.65 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 14:19:49.097985: step 981, loss = 2.60 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 14:19:51.432625: step 982, loss = 2.48 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 14:19:53.703129: step 983, loss = 2.44 (28.2 examples/sec; 2.266 sec/batch)
2018-10-15 14:19:55.940520: step 984, loss = 2.44 (28.7 examples/sec; 2.233 sec/batch)
2018-10-15 14:19:58.368629: step 985, loss = 2.77 (26.4 examples/sec; 2.423 sec/batch)
2018-10-15 14:20:00.731009: step 986, loss = 2.65 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 14:20:03.031159: step 987, loss = 2.95 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 14:20:05.410639: step 988, loss = 2.72 (26.9 examples/sec; 2.377 sec/batch)
2018-10-15 14:20:07.703726: step 989, loss = 2.71 (28.0 examples/sec; 2.288 sec/batch)
2018-10-15 14:20:10.028981: step 990, loss = 2.43 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 14:20:12.304285: step 991, loss = 2.70 (28.2 examples/sec; 2.271 sec/batch)
2018-10-15 14:20:14.590650: step 992, loss = 2.71 (28.1 examples/sec; 2.281 sec/batch)
2018-10-15 14:20:16.977848: step 993, loss = 2.59 (26.9 examples/sec; 2.382 sec/batch)
2018-10-15 14:20:19.310720: step 994, loss = 2.56 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 14:20:21.629546: step 995, loss = 2.73 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 14:20:23.973894: step 996, loss = 2.59 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 14:20:26.423179: step 997, loss = 2.70 (26.2 examples/sec; 2.444 sec/batch)
2018-10-15 14:20:28.707636: step 998, loss = 2.76 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 14:20:31.033223: step 999, loss = 2.70 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 14:20:33.297505: step 1000, loss = 2.59 (28.3 examples/sec; 2.261 sec/batch)
2018-10-15 14:20:36.089268: step 1001, loss = 2.59 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 14:20:38.431031: step 1002, loss = 2.56 (27.4 examples/sec; 2.336 sec/batch)
2018-10-15 14:20:40.717417: step 1003, loss = 2.61 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 14:20:42.968539: step 1004, loss = 2.59 (28.5 examples/sec; 2.246 sec/batch)
2018-10-15 14:20:45.354090: step 1005, loss = 2.60 (26.9 examples/sec; 2.381 sec/batch)
2018-10-15 14:20:47.680721: step 1006, loss = 2.58 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 14:20:50.025069: step 1007, loss = 2.73 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 14:20:52.330487: step 1008, loss = 2.63 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 14:20:54.721219: step 1009, loss = 2.67 (26.8 examples/sec; 2.385 sec/batch)
2018-10-15 14:20:57.115225: step 1010, loss = 2.63 (26.8 examples/sec; 2.390 sec/batch)
2018-10-15 14:20:59.466620: step 1011, loss = 2.46 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 14:21:01.818752: step 1012, loss = 2.73 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 14:21:04.109170: step 1013, loss = 2.48 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 14:21:06.476183: step 1014, loss = 2.74 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 14:21:08.807565: step 1015, loss = 2.60 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 14:21:11.086920: step 1016, loss = 2.47 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 14:21:13.402410: step 1017, loss = 2.68 (27.7 examples/sec; 2.311 sec/batch)
2018-10-15 14:21:15.776672: step 1018, loss = 2.77 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 14:21:18.109047: step 1019, loss = 2.53 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 14:21:20.438544: step 1020, loss = 2.50 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 14:21:22.811398: step 1021, loss = 2.70 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 14:21:25.066029: step 1022, loss = 2.65 (28.5 examples/sec; 2.249 sec/batch)
2018-10-15 14:21:27.417689: step 1023, loss = 2.67 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 14:21:29.779649: step 1024, loss = 2.66 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 14:21:32.097144: step 1025, loss = 2.56 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 14:21:34.400222: step 1026, loss = 2.72 (27.8 examples/sec; 2.298 sec/batch)
2018-10-15 14:21:36.770728: step 1027, loss = 2.44 (27.1 examples/sec; 2.366 sec/batch)
2018-10-15 14:21:39.103183: step 1028, loss = 2.50 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 14:21:41.410647: step 1029, loss = 2.49 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 14:21:43.706891: step 1030, loss = 2.58 (27.9 examples/sec; 2.291 sec/batch)
2018-10-15 14:21:46.049839: step 1031, loss = 2.41 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 14:21:48.452042: step 1032, loss = 2.69 (26.7 examples/sec; 2.397 sec/batch)
2018-10-15 14:21:50.815031: step 1033, loss = 2.46 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 14:21:53.157458: step 1034, loss = 2.76 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 14:21:55.483209: step 1035, loss = 2.81 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 14:21:57.870561: step 1036, loss = 2.79 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 14:22:00.219313: step 1037, loss = 2.77 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 14:22:02.547215: step 1038, loss = 2.93 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 14:22:04.945056: step 1039, loss = 2.70 (26.7 examples/sec; 2.393 sec/batch)
2018-10-15 14:22:07.379168: step 1040, loss = 2.70 (26.3 examples/sec; 2.429 sec/batch)
2018-10-15 14:22:09.792888: step 1041, loss = 2.69 (26.6 examples/sec; 2.409 sec/batch)
2018-10-15 14:22:12.060648: step 1042, loss = 2.50 (28.3 examples/sec; 2.263 sec/batch)
2018-10-15 14:22:14.420141: step 1043, loss = 2.77 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 14:22:16.817768: step 1044, loss = 2.48 (26.8 examples/sec; 2.392 sec/batch)
2018-10-15 14:22:19.262897: step 1045, loss = 2.61 (26.2 examples/sec; 2.440 sec/batch)
2018-10-15 14:22:21.590105: step 1046, loss = 2.62 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 14:22:23.923453: step 1047, loss = 2.65 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 14:22:26.328355: step 1048, loss = 2.48 (26.7 examples/sec; 2.400 sec/batch)
2018-10-15 14:22:28.610466: step 1049, loss = 2.47 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 14:22:30.902683: step 1050, loss = 2.64 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 14:22:33.263165: step 1051, loss = 2.72 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 14:22:35.555992: step 1052, loss = 2.63 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 14:22:37.888625: step 1053, loss = 2.56 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 14:22:40.216143: step 1054, loss = 2.68 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 14:22:42.625787: step 1055, loss = 2.62 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 14:22:44.869356: step 1056, loss = 2.55 (28.6 examples/sec; 2.238 sec/batch)
2018-10-15 14:22:47.204060: step 1057, loss = 2.62 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 14:22:49.555249: step 1058, loss = 2.71 (27.3 examples/sec; 2.347 sec/batch)
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 267, in <module>
    benchmark_name=args.dataset, batch_num=args.batch_num, batch_size=args.batch_size)
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 184, in train
    _, loss_value, summary = sess.run([train_op, total_loss, summary_op])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_flowers_data/parallel_read/common_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[{{node flowers_data/parallel_read/common_queue_Dequeue}} = QueueDequeueV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:worker/replica:0/task:0/device:CPU:0"](flowers_data/parallel_read/common_queue)]]

Caused by op u'flowers_data/parallel_read/common_queue_Dequeue', defined at:
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 267, in <module>
    benchmark_name=args.dataset, batch_num=args.batch_num, batch_size=args.batch_size)
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 135, in train
    images, labels, num_classes = input_data(batch_size, batch_num)
  File "AlexNet/datasets/__init__.py", line 54, in flowers_data
    num_epochs=num_epochs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py", line 96, in __init__
    scope=scope)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py", line 262, in parallel_read
    reader_kwargs=reader_kwargs).read(filename_queue)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py", line 131, in read
    return self._common_queue.dequeue(name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py", line 433, in dequeue
    self._queue_ref, self._dtypes, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 3735, in queue_dequeue_v2
    timeout_ms=timeout_ms, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3272, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

OutOfRangeError (see above for traceback): RandomShuffleQueue '_1_flowers_data/parallel_read/common_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[{{node flowers_data/parallel_read/common_queue_Dequeue}} = QueueDequeueV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:worker/replica:0/task:0/device:CPU:0"](flowers_data/parallel_read/common_queue)]]
