jayan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode single --dataset flowers --batch_num 10000^C
jayan@node-0:~/alexnet$ vi AlexNet/scripts/train.py
jayan@node-0:~/alexnet$ vi AlexNet/scripts/train.py
jayan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode single --dataset flowers --batch_num 10000
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:worker/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:worker/task:0)
Input batch shape: images: (128, 256, 256, 3) labels: (128,)
num_classes: 5
total_num_examples: 1280000
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 18:24:30.989250: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session be3ede2d9da2368f with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 18:24:39.717211: step 0, loss = 3.77 (20.8 examples/sec; 6.169 sec/batch)
2018-10-15 18:24:45.940680: step 1, loss = 4.36 (22.6 examples/sec; 5.670 sec/batch)
2018-10-15 18:24:50.642052: step 2, loss = 5.12 (27.2 examples/sec; 4.698 sec/batch)
2018-10-15 18:24:55.519097: step 3, loss = 4.37 (26.3 examples/sec; 4.872 sec/batch)
2018-10-15 18:25:00.314977: step 4, loss = 4.38 (26.7 examples/sec; 4.790 sec/batch)
2018-10-15 18:25:05.185886: step 5, loss = 5.59 (26.3 examples/sec; 4.867 sec/batch)
2018-10-15 18:25:11.100094: step 6, loss = 8.10 (21.7 examples/sec; 5.908 sec/batch)
2018-10-15 18:25:15.811952: step 7, loss = 9.01 (27.2 examples/sec; 4.706 sec/batch)
2018-10-15 18:25:20.624931: step 8, loss = 8.72 (26.6 examples/sec; 4.807 sec/batch)
2018-10-15 18:25:25.192438: step 9, loss = 5.75 (28.1 examples/sec; 4.561 sec/batch)
2018-10-15 18:25:29.856182: step 10, loss = 6.13 (27.5 examples/sec; 4.661 sec/batch)
2018-10-15 18:25:34.340851: step 11, loss = 5.91 (28.6 examples/sec; 4.477 sec/batch)
2018-10-15 18:25:38.815557: step 12, loss = 7.00 (28.7 examples/sec; 4.467 sec/batch)
2018-10-15 18:25:43.370433: step 13, loss = 4.85 (28.1 examples/sec; 4.548 sec/batch)
2018-10-15 18:25:48.378914: step 14, loss = 4.65 (25.6 examples/sec; 5.004 sec/batch)
2018-10-15 18:25:53.042105: step 15, loss = 4.74 (27.5 examples/sec; 4.658 sec/batch)
2018-10-15 18:25:57.561927: step 16, loss = 4.65 (28.4 examples/sec; 4.509 sec/batch)
2018-10-15 18:26:02.164915: step 17, loss = 4.03 (27.8 examples/sec; 4.598 sec/batch)
2018-10-15 18:26:06.763001: step 18, loss = 3.60 (27.9 examples/sec; 4.594 sec/batch)
2018-10-15 18:26:11.341099: step 19, loss = 4.85 (28.0 examples/sec; 4.575 sec/batch)
2018-10-15 18:26:15.918263: step 20, loss = 4.55 (28.0 examples/sec; 4.570 sec/batch)
2018-10-15 18:26:20.375365: step 21, loss = 4.56 (28.8 examples/sec; 4.452 sec/batch)
2018-10-15 18:26:24.691672: step 22, loss = 3.68 (29.7 examples/sec; 4.312 sec/batch)
2018-10-15 18:26:29.280302: step 23, loss = 4.08 (27.9 examples/sec; 4.584 sec/batch)
2018-10-15 18:26:33.713247: step 24, loss = 4.29 (28.9 examples/sec; 4.428 sec/batch)
2018-10-15 18:26:38.782729: step 25, loss = 3.81 (25.3 examples/sec; 5.065 sec/batch)
2018-10-15 18:26:43.363997: step 26, loss = 3.69 (28.0 examples/sec; 4.578 sec/batch)
2018-10-15 18:26:47.776404: step 27, loss = 4.80 (29.0 examples/sec; 4.407 sec/batch)
2018-10-15 18:26:52.141901: step 28, loss = 4.07 (29.4 examples/sec; 4.359 sec/batch)
2018-10-15 18:26:56.771641: step 29, loss = 3.83 (27.7 examples/sec; 4.622 sec/batch)
2018-10-15 18:27:01.180172: step 30, loss = 3.75 (29.1 examples/sec; 4.405 sec/batch)
2018-10-15 18:27:04.191265: step 31, loss = 4.08 (42.6 examples/sec; 3.005 sec/batch)
2018-10-15 18:27:07.085486: step 32, loss = 3.76 (44.3 examples/sec; 2.890 sec/batch)
2018-10-15 18:27:10.042618: step 33, loss = 3.67 (43.4 examples/sec; 2.952 sec/batch)
2018-10-15 18:27:12.955162: step 34, loss = 3.64 (44.0 examples/sec; 2.909 sec/batch)
2018-10-15 18:27:15.955486: step 35, loss = 3.58 (42.7 examples/sec; 2.996 sec/batch)
2018-10-15 18:27:18.947126: step 36, loss = 3.73 (42.8 examples/sec; 2.989 sec/batch)
2018-10-15 18:27:22.088329: step 37, loss = 3.69 (40.8 examples/sec; 3.138 sec/batch)
2018-10-15 18:27:25.062576: step 38, loss = 3.62 (43.1 examples/sec; 2.969 sec/batch)
2018-10-15 18:27:28.040133: step 39, loss = 3.57 (43.0 examples/sec; 2.975 sec/batch)
2018-10-15 18:27:31.054299: step 40, loss = 3.59 (42.5 examples/sec; 3.010 sec/batch)
2018-10-15 18:27:34.009170: step 41, loss = 3.68 (43.4 examples/sec; 2.950 sec/batch)
2018-10-15 18:27:36.972911: step 42, loss = 3.56 (43.2 examples/sec; 2.961 sec/batch)
2018-10-15 18:27:40.036821: step 43, loss = 3.51 (41.8 examples/sec; 3.061 sec/batch)
2018-10-15 18:27:43.019429: step 44, loss = 3.47 (43.0 examples/sec; 2.978 sec/batch)
2018-10-15 18:27:46.039266: step 45, loss = 3.56 (42.4 examples/sec; 3.016 sec/batch)
2018-10-15 18:27:49.044332: step 46, loss = 3.47 (42.7 examples/sec; 3.000 sec/batch)
2018-10-15 18:27:51.921938: step 47, loss = 3.42 (44.5 examples/sec; 2.875 sec/batch)
2018-10-15 18:27:54.881714: step 48, loss = 3.54 (43.3 examples/sec; 2.954 sec/batch)
2018-10-15 18:27:57.912048: step 49, loss = 3.50 (42.3 examples/sec; 3.028 sec/batch)
2018-10-15 18:28:00.990900: step 50, loss = 3.43 (41.6 examples/sec; 3.073 sec/batch)
2018-10-15 18:28:04.304720: step 51, loss = 3.52 (38.7 examples/sec; 3.309 sec/batch)
2018-10-15 18:28:07.406077: step 52, loss = 3.61 (41.3 examples/sec; 3.096 sec/batch)
2018-10-15 18:28:10.385126: step 53, loss = 3.53 (43.0 examples/sec; 2.974 sec/batch)
2018-10-15 18:28:13.338385: step 54, loss = 3.55 (43.4 examples/sec; 2.951 sec/batch)
2018-10-15 18:28:16.386583: step 55, loss = 3.57 (42.1 examples/sec; 3.043 sec/batch)
2018-10-15 18:28:19.586550: step 56, loss = 3.55 (40.1 examples/sec; 3.195 sec/batch)
2018-10-15 18:28:22.537182: step 57, loss = 3.52 (43.4 examples/sec; 2.946 sec/batch)
2018-10-15 18:28:25.494509: step 58, loss = 3.51 (43.4 examples/sec; 2.952 sec/batch)
2018-10-15 18:28:28.481963: step 59, loss = 3.49 (42.9 examples/sec; 2.985 sec/batch)
2018-10-15 18:28:31.456851: step 60, loss = 3.58 (43.1 examples/sec; 2.972 sec/batch)
2018-10-15 18:28:34.522830: step 61, loss = 3.48 (41.8 examples/sec; 3.061 sec/batch)
2018-10-15 18:28:37.503857: step 62, loss = 3.61 (43.0 examples/sec; 2.976 sec/batch)
2018-10-15 18:28:40.503802: step 63, loss = 3.47 (42.7 examples/sec; 2.995 sec/batch)
2018-10-15 18:28:43.666916: step 64, loss = 3.53 (40.5 examples/sec; 3.160 sec/batch)
2018-10-15 18:28:46.832300: step 65, loss = 3.56 (40.5 examples/sec; 3.161 sec/batch)
2018-10-15 18:28:49.884072: step 66, loss = 3.46 (42.0 examples/sec; 3.049 sec/batch)
2018-10-15 18:28:52.955159: step 67, loss = 3.44 (41.7 examples/sec; 3.066 sec/batch)
2018-10-15 18:28:55.921230: step 68, loss = 3.43 (43.2 examples/sec; 2.961 sec/batch)
2018-10-15 18:28:58.966211: step 69, loss = 3.45 (42.1 examples/sec; 3.040 sec/batch)
2018-10-15 18:29:02.189125: step 70, loss = 3.48 (39.8 examples/sec; 3.219 sec/batch)
2018-10-15 18:29:05.148779: step 71, loss = 3.50 (43.3 examples/sec; 2.955 sec/batch)
2018-10-15 18:29:08.161459: step 72, loss = 3.44 (42.6 examples/sec; 3.008 sec/batch)
2018-10-15 18:29:11.197130: step 73, loss = 3.46 (42.2 examples/sec; 3.030 sec/batch)
2018-10-15 18:29:14.260840: step 74, loss = 3.48 (41.8 examples/sec; 3.059 sec/batch)
2018-10-15 18:29:17.395674: step 75, loss = 3.52 (40.9 examples/sec; 3.130 sec/batch)
2018-10-15 18:29:20.438801: step 76, loss = 3.50 (42.1 examples/sec; 3.040 sec/batch)
2018-10-15 18:29:23.434591: step 77, loss = 3.41 (42.8 examples/sec; 2.991 sec/batch)
2018-10-15 18:29:26.500066: step 78, loss = 3.40 (41.8 examples/sec; 3.060 sec/batch)
2018-10-15 18:29:29.622876: step 79, loss = 3.45 (41.0 examples/sec; 3.120 sec/batch)
2018-10-15 18:29:32.793833: step 80, loss = 3.54 (40.4 examples/sec; 3.166 sec/batch)
2018-10-15 18:29:35.804874: step 81, loss = 3.40 (42.6 examples/sec; 3.006 sec/batch)
2018-10-15 18:29:38.795877: step 82, loss = 3.36 (42.9 examples/sec; 2.986 sec/batch)
2018-10-15 18:29:41.731161: step 83, loss = 3.50 (43.6 examples/sec; 2.933 sec/batch)
2018-10-15 18:29:44.882417: step 84, loss = 3.48 (40.7 examples/sec; 3.146 sec/batch)
2018-10-15 18:29:47.989891: step 85, loss = 3.40 (41.2 examples/sec; 3.104 sec/batch)
2018-10-15 18:29:50.995132: step 86, loss = 3.42 (42.7 examples/sec; 3.000 sec/batch)
2018-10-15 18:29:53.975389: step 87, loss = 3.46 (43.0 examples/sec; 2.977 sec/batch)
2018-10-15 18:29:57.065274: step 88, loss = 3.42 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:30:00.115211: step 89, loss = 3.37 (42.0 examples/sec; 3.047 sec/batch)
2018-10-15 18:30:03.246359: step 90, loss = 3.39 (40.9 examples/sec; 3.127 sec/batch)
2018-10-15 18:30:06.243767: step 91, loss = 3.47 (42.7 examples/sec; 2.995 sec/batch)
2018-10-15 18:30:09.199250: step 92, loss = 3.32 (43.4 examples/sec; 2.951 sec/batch)
2018-10-15 18:30:12.246698: step 93, loss = 3.33 (42.1 examples/sec; 3.043 sec/batch)
2018-10-15 18:30:15.365654: step 94, loss = 3.24 (41.1 examples/sec; 3.116 sec/batch)
2018-10-15 18:30:18.271148: step 95, loss = 3.23 (44.1 examples/sec; 2.901 sec/batch)
2018-10-15 18:30:21.258016: step 96, loss = 3.30 (42.9 examples/sec; 2.984 sec/batch)
2018-10-15 18:30:24.301787: step 97, loss = 3.28 (42.1 examples/sec; 3.038 sec/batch)
2018-10-15 18:30:27.353046: step 98, loss = 3.36 (42.0 examples/sec; 3.047 sec/batch)
2018-10-15 18:30:30.358689: step 99, loss = 3.23 (42.7 examples/sec; 3.001 sec/batch)
2018-10-15 18:30:33.265785: step 100, loss = 3.36 (44.1 examples/sec; 2.902 sec/batch)
2018-10-15 18:30:36.761622: step 101, loss = 3.21 (42.6 examples/sec; 3.007 sec/batch)
2018-10-15 18:30:39.936121: step 102, loss = 3.32 (42.1 examples/sec; 3.042 sec/batch)
2018-10-15 18:30:43.139666: step 103, loss = 3.33 (40.0 examples/sec; 3.199 sec/batch)
2018-10-15 18:30:46.166572: step 104, loss = 3.34 (42.3 examples/sec; 3.024 sec/batch)
2018-10-15 18:30:49.249342: step 105, loss = 3.27 (41.6 examples/sec; 3.078 sec/batch)
2018-10-15 18:30:52.312268: step 106, loss = 3.26 (41.8 examples/sec; 3.060 sec/batch)
2018-10-15 18:30:55.425312: step 107, loss = 3.17 (41.2 examples/sec; 3.110 sec/batch)
2018-10-15 18:30:58.489743: step 108, loss = 3.35 (41.8 examples/sec; 3.059 sec/batch)
2018-10-15 18:31:01.602262: step 109, loss = 3.27 (41.2 examples/sec; 3.107 sec/batch)
2018-10-15 18:31:04.609495: step 110, loss = 3.13 (42.6 examples/sec; 3.005 sec/batch)
2018-10-15 18:31:07.759582: step 111, loss = 3.27 (40.7 examples/sec; 3.145 sec/batch)
2018-10-15 18:31:10.910092: step 112, loss = 3.33 (40.7 examples/sec; 3.148 sec/batch)
2018-10-15 18:31:13.967064: step 113, loss = 3.28 (41.9 examples/sec; 3.052 sec/batch)
2018-10-15 18:31:16.985157: step 114, loss = 3.38 (42.5 examples/sec; 3.013 sec/batch)
2018-10-15 18:31:20.065434: step 115, loss = 3.41 (41.6 examples/sec; 3.077 sec/batch)
2018-10-15 18:31:23.149052: step 116, loss = 3.12 (41.6 examples/sec; 3.079 sec/batch)
2018-10-15 18:31:26.284980: step 117, loss = 3.08 (40.9 examples/sec; 3.133 sec/batch)
2018-10-15 18:31:29.434768: step 118, loss = 3.01 (40.7 examples/sec; 3.147 sec/batch)
2018-10-15 18:31:32.421047: step 119, loss = 3.27 (42.9 examples/sec; 2.981 sec/batch)
2018-10-15 18:31:35.568235: step 120, loss = 3.28 (40.7 examples/sec; 3.144 sec/batch)
2018-10-15 18:31:38.761352: step 121, loss = 3.31 (40.2 examples/sec; 3.188 sec/batch)
2018-10-15 18:31:41.783880: step 122, loss = 3.12 (42.4 examples/sec; 3.018 sec/batch)
2018-10-15 18:31:44.795371: step 123, loss = 3.10 (42.5 examples/sec; 3.009 sec/batch)
2018-10-15 18:31:47.796434: step 124, loss = 3.14 (42.7 examples/sec; 2.998 sec/batch)
2018-10-15 18:31:50.970960: step 125, loss = 3.28 (40.4 examples/sec; 3.170 sec/batch)
2018-10-15 18:31:54.130245: step 126, loss = 3.14 (40.6 examples/sec; 3.154 sec/batch)
2018-10-15 18:31:57.117715: step 127, loss = 3.20 (42.9 examples/sec; 2.982 sec/batch)
2018-10-15 18:32:00.070774: step 128, loss = 3.33 (43.4 examples/sec; 2.948 sec/batch)
2018-10-15 18:32:03.090866: step 129, loss = 3.23 (42.5 examples/sec; 3.015 sec/batch)
2018-10-15 18:32:06.149202: step 130, loss = 3.40 (41.9 examples/sec; 3.054 sec/batch)
2018-10-15 18:32:09.298008: step 131, loss = 3.34 (40.7 examples/sec; 3.144 sec/batch)
2018-10-15 18:32:12.372411: step 132, loss = 3.20 (41.7 examples/sec; 3.073 sec/batch)
2018-10-15 18:32:15.362333: step 133, loss = 3.08 (42.9 examples/sec; 2.985 sec/batch)
2018-10-15 18:32:18.431841: step 134, loss = 3.17 (41.7 examples/sec; 3.067 sec/batch)
2018-10-15 18:32:21.601178: step 135, loss = 3.22 (40.4 examples/sec; 3.164 sec/batch)
2018-10-15 18:32:24.650053: step 136, loss = 3.22 (42.1 examples/sec; 3.044 sec/batch)
2018-10-15 18:32:27.669851: step 137, loss = 3.07 (42.4 examples/sec; 3.017 sec/batch)
2018-10-15 18:32:30.562787: step 138, loss = 3.14 (44.3 examples/sec; 2.888 sec/batch)
2018-10-15 18:32:33.597796: step 139, loss = 3.12 (42.2 examples/sec; 3.030 sec/batch)
2018-10-15 18:32:36.687171: step 140, loss = 3.20 (41.5 examples/sec; 3.084 sec/batch)
2018-10-15 18:32:39.750338: step 141, loss = 3.20 (41.9 examples/sec; 3.058 sec/batch)
2018-10-15 18:32:42.840498: step 142, loss = 3.11 (41.5 examples/sec; 3.087 sec/batch)
2018-10-15 18:32:45.909859: step 143, loss = 3.21 (41.8 examples/sec; 3.064 sec/batch)
2018-10-15 18:32:49.046100: step 144, loss = 3.11 (40.9 examples/sec; 3.133 sec/batch)
2018-10-15 18:32:52.201125: step 145, loss = 3.13 (40.6 examples/sec; 3.152 sec/batch)
2018-10-15 18:32:55.294891: step 146, loss = 2.98 (41.4 examples/sec; 3.089 sec/batch)
2018-10-15 18:32:58.356242: step 147, loss = 3.09 (41.9 examples/sec; 3.056 sec/batch)
2018-10-15 18:33:01.361483: step 148, loss = 3.10 (42.6 examples/sec; 3.002 sec/batch)
2018-10-15 18:33:04.524402: step 149, loss = 3.06 (40.5 examples/sec; 3.158 sec/batch)
2018-10-15 18:33:07.581926: step 150, loss = 3.04 (41.9 examples/sec; 3.052 sec/batch)
2018-10-15 18:33:10.659151: step 151, loss = 3.06 (41.6 examples/sec; 3.074 sec/batch)
2018-10-15 18:33:13.633808: step 152, loss = 3.04 (43.1 examples/sec; 2.969 sec/batch)
2018-10-15 18:33:16.707651: step 153, loss = 3.09 (41.7 examples/sec; 3.071 sec/batch)
2018-10-15 18:33:19.815791: step 154, loss = 3.15 (41.2 examples/sec; 3.103 sec/batch)
2018-10-15 18:33:22.764384: step 155, loss = 3.09 (43.5 examples/sec; 2.944 sec/batch)
2018-10-15 18:33:25.765068: step 156, loss = 3.03 (42.7 examples/sec; 2.996 sec/batch)
2018-10-15 18:33:28.749189: step 157, loss = 3.05 (42.9 examples/sec; 2.981 sec/batch)
2018-10-15 18:33:31.836717: step 158, loss = 3.21 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:33:35.018237: step 159, loss = 3.16 (40.3 examples/sec; 3.176 sec/batch)
2018-10-15 18:33:38.010658: step 160, loss = 3.00 (42.8 examples/sec; 2.988 sec/batch)
2018-10-15 18:33:41.012453: step 161, loss = 3.08 (42.7 examples/sec; 2.997 sec/batch)
2018-10-15 18:33:44.050098: step 162, loss = 3.20 (42.2 examples/sec; 3.035 sec/batch)
2018-10-15 18:33:47.252710: step 163, loss = 3.25 (40.0 examples/sec; 3.198 sec/batch)
2018-10-15 18:33:50.391784: step 164, loss = 3.11 (40.8 examples/sec; 3.136 sec/batch)
2018-10-15 18:33:53.461963: step 165, loss = 3.13 (41.8 examples/sec; 3.065 sec/batch)
2018-10-15 18:33:56.430542: step 166, loss = 3.18 (43.2 examples/sec; 2.966 sec/batch)
2018-10-15 18:33:59.571267: step 167, loss = 3.15 (40.8 examples/sec; 3.136 sec/batch)
2018-10-15 18:34:02.663194: step 168, loss = 3.17 (41.4 examples/sec; 3.089 sec/batch)
2018-10-15 18:34:05.751392: step 169, loss = 3.29 (41.5 examples/sec; 3.086 sec/batch)
2018-10-15 18:34:08.721336: step 170, loss = 3.13 (43.2 examples/sec; 2.966 sec/batch)
2018-10-15 18:34:11.782499: step 171, loss = 3.19 (41.9 examples/sec; 3.057 sec/batch)
2018-10-15 18:34:14.860437: step 172, loss = 3.22 (41.7 examples/sec; 3.073 sec/batch)
2018-10-15 18:34:18.032955: step 173, loss = 3.21 (40.4 examples/sec; 3.167 sec/batch)
2018-10-15 18:34:21.149294: step 174, loss = 3.17 (41.1 examples/sec; 3.111 sec/batch)
2018-10-15 18:34:24.210385: step 175, loss = 3.19 (41.9 examples/sec; 3.058 sec/batch)
2018-10-15 18:34:27.318649: step 176, loss = 3.27 (41.2 examples/sec; 3.105 sec/batch)
2018-10-15 18:34:30.603717: step 177, loss = 3.07 (39.0 examples/sec; 3.282 sec/batch)
2018-10-15 18:34:33.612078: step 178, loss = 3.13 (42.6 examples/sec; 3.005 sec/batch)
2018-10-15 18:34:36.578146: step 179, loss = 3.05 (43.2 examples/sec; 2.962 sec/batch)
2018-10-15 18:34:39.625709: step 180, loss = 3.15 (42.1 examples/sec; 3.042 sec/batch)
2018-10-15 18:34:42.677451: step 181, loss = 3.16 (42.0 examples/sec; 3.049 sec/batch)
2018-10-15 18:34:45.799600: step 182, loss = 2.92 (41.1 examples/sec; 3.117 sec/batch)
2018-10-15 18:34:48.851808: step 183, loss = 3.11 (42.0 examples/sec; 3.048 sec/batch)
2018-10-15 18:34:51.867820: step 184, loss = 3.01 (42.5 examples/sec; 3.011 sec/batch)
2018-10-15 18:34:55.030843: step 185, loss = 3.00 (40.5 examples/sec; 3.160 sec/batch)
2018-10-15 18:34:58.124024: step 186, loss = 3.17 (41.5 examples/sec; 3.088 sec/batch)
2018-10-15 18:35:01.179671: step 187, loss = 3.10 (42.0 examples/sec; 3.050 sec/batch)
2018-10-15 18:35:04.292106: step 188, loss = 3.16 (41.2 examples/sec; 3.109 sec/batch)
2018-10-15 18:35:07.216177: step 189, loss = 3.03 (43.8 examples/sec; 2.921 sec/batch)
2018-10-15 18:35:10.279383: step 190, loss = 3.04 (41.9 examples/sec; 3.058 sec/batch)
2018-10-15 18:35:13.460941: step 191, loss = 3.17 (40.3 examples/sec; 3.176 sec/batch)
2018-10-15 18:35:16.469069: step 192, loss = 3.18 (42.6 examples/sec; 3.004 sec/batch)
2018-10-15 18:35:19.554960: step 193, loss = 3.02 (41.5 examples/sec; 3.081 sec/batch)
2018-10-15 18:35:22.595413: step 194, loss = 3.01 (42.2 examples/sec; 3.035 sec/batch)
2018-10-15 18:35:25.747107: step 195, loss = 2.98 (40.7 examples/sec; 3.147 sec/batch)
2018-10-15 18:35:28.955975: step 196, loss = 3.03 (39.9 examples/sec; 3.205 sec/batch)
2018-10-15 18:35:31.955369: step 197, loss = 3.08 (42.7 examples/sec; 2.997 sec/batch)
2018-10-15 18:35:35.007181: step 198, loss = 3.06 (42.0 examples/sec; 3.047 sec/batch)
2018-10-15 18:35:38.140008: step 199, loss = 3.02 (40.9 examples/sec; 3.131 sec/batch)
2018-10-15 18:35:41.159790: step 200, loss = 2.98 (42.5 examples/sec; 3.015 sec/batch)
2018-10-15 18:35:44.813491: step 201, loss = 3.19 (40.7 examples/sec; 3.143 sec/batch)
2018-10-15 18:35:47.864352: step 202, loss = 3.00 (42.0 examples/sec; 3.048 sec/batch)
2018-10-15 18:35:50.940117: step 203, loss = 3.16 (41.7 examples/sec; 3.071 sec/batch)
2018-10-15 18:35:53.988949: step 204, loss = 3.03 (42.0 examples/sec; 3.046 sec/batch)
2018-10-15 18:35:57.273060: step 205, loss = 3.12 (39.0 examples/sec; 3.279 sec/batch)
2018-10-15 18:36:00.327410: step 206, loss = 3.17 (42.0 examples/sec; 3.050 sec/batch)
2018-10-15 18:36:03.360510: step 207, loss = 3.19 (42.3 examples/sec; 3.028 sec/batch)
2018-10-15 18:36:06.355656: step 208, loss = 3.13 (42.8 examples/sec; 2.990 sec/batch)
2018-10-15 18:36:09.361267: step 209, loss = 3.05 (42.7 examples/sec; 3.000 sec/batch)
2018-10-15 18:36:12.573249: step 210, loss = 3.06 (39.9 examples/sec; 3.207 sec/batch)
2018-10-15 18:36:15.784535: step 211, loss = 2.97 (39.9 examples/sec; 3.207 sec/batch)
2018-10-15 18:36:18.756318: step 212, loss = 3.21 (43.1 examples/sec; 2.967 sec/batch)
2018-10-15 18:36:21.884151: step 213, loss = 2.96 (41.0 examples/sec; 3.122 sec/batch)
2018-10-15 18:36:24.961989: step 214, loss = 2.97 (41.6 examples/sec; 3.075 sec/batch)
2018-10-15 18:36:28.041118: step 215, loss = 2.90 (41.6 examples/sec; 3.075 sec/batch)
2018-10-15 18:36:31.010188: step 216, loss = 3.18 (43.2 examples/sec; 2.964 sec/batch)
2018-10-15 18:36:33.963079: step 217, loss = 3.26 (43.4 examples/sec; 2.949 sec/batch)
2018-10-15 18:36:37.094085: step 218, loss = 3.10 (40.9 examples/sec; 3.126 sec/batch)
2018-10-15 18:36:40.126594: step 219, loss = 3.08 (42.3 examples/sec; 3.027 sec/batch)
2018-10-15 18:36:43.188774: step 220, loss = 3.17 (41.9 examples/sec; 3.057 sec/batch)
2018-10-15 18:36:46.249925: step 221, loss = 2.98 (41.9 examples/sec; 3.057 sec/batch)
2018-10-15 18:36:49.300808: step 222, loss = 2.99 (42.0 examples/sec; 3.048 sec/batch)
2018-10-15 18:36:52.285360: step 223, loss = 2.96 (43.0 examples/sec; 2.980 sec/batch)
2018-10-15 18:36:55.494242: step 224, loss = 3.16 (39.9 examples/sec; 3.205 sec/batch)
2018-10-15 18:36:58.620688: step 225, loss = 3.11 (41.0 examples/sec; 3.124 sec/batch)
2018-10-15 18:37:01.662661: step 226, loss = 3.00 (42.1 examples/sec; 3.039 sec/batch)
2018-10-15 18:37:04.841578: step 227, loss = 3.07 (40.3 examples/sec; 3.174 sec/batch)
2018-10-15 18:37:08.017209: step 228, loss = 3.15 (40.3 examples/sec; 3.173 sec/batch)
2018-10-15 18:37:11.133531: step 229, loss = 3.20 (41.1 examples/sec; 3.111 sec/batch)
2018-10-15 18:37:14.156241: step 230, loss = 3.05 (42.4 examples/sec; 3.020 sec/batch)
2018-10-15 18:37:17.221946: step 231, loss = 3.00 (41.8 examples/sec; 3.063 sec/batch)
2018-10-15 18:37:20.405138: step 232, loss = 3.11 (40.2 examples/sec; 3.180 sec/batch)
2018-10-15 18:37:23.570296: step 233, loss = 3.15 (40.5 examples/sec; 3.160 sec/batch)
2018-10-15 18:37:26.601048: step 234, loss = 3.36 (42.3 examples/sec; 3.026 sec/batch)
2018-10-15 18:37:29.611184: step 235, loss = 3.27 (42.6 examples/sec; 3.006 sec/batch)
2018-10-15 18:37:32.696372: step 236, loss = 3.24 (41.6 examples/sec; 3.081 sec/batch)
2018-10-15 18:37:35.837585: step 237, loss = 3.04 (40.8 examples/sec; 3.136 sec/batch)
2018-10-15 18:37:38.962279: step 238, loss = 2.93 (41.0 examples/sec; 3.122 sec/batch)
2018-10-15 18:37:42.177684: step 239, loss = 3.19 (39.9 examples/sec; 3.211 sec/batch)
2018-10-15 18:37:45.186204: step 240, loss = 3.07 (42.6 examples/sec; 3.005 sec/batch)
2018-10-15 18:37:48.370440: step 241, loss = 2.95 (40.3 examples/sec; 3.179 sec/batch)
2018-10-15 18:37:51.490910: step 242, loss = 3.06 (41.1 examples/sec; 3.117 sec/batch)
2018-10-15 18:37:54.568476: step 243, loss = 2.99 (41.7 examples/sec; 3.072 sec/batch)
2018-10-15 18:37:57.633389: step 244, loss = 3.22 (41.8 examples/sec; 3.060 sec/batch)
2018-10-15 18:38:00.679378: step 245, loss = 3.00 (42.0 examples/sec; 3.044 sec/batch)
2018-10-15 18:38:03.815655: step 246, loss = 3.12 (40.9 examples/sec; 3.131 sec/batch)
2018-10-15 18:38:06.886428: step 247, loss = 3.01 (41.7 examples/sec; 3.066 sec/batch)
2018-10-15 18:38:09.923351: step 248, loss = 2.97 (42.2 examples/sec; 3.032 sec/batch)
2018-10-15 18:38:12.888420: step 249, loss = 3.02 (43.2 examples/sec; 2.960 sec/batch)
2018-10-15 18:38:15.930326: step 250, loss = 3.01 (42.1 examples/sec; 3.037 sec/batch)
2018-10-15 18:38:19.062449: step 251, loss = 3.00 (40.9 examples/sec; 3.127 sec/batch)
2018-10-15 18:38:22.159706: step 252, loss = 2.96 (41.4 examples/sec; 3.093 sec/batch)
2018-10-15 18:38:25.209472: step 253, loss = 2.92 (42.0 examples/sec; 3.047 sec/batch)
2018-10-15 18:38:28.266865: step 254, loss = 2.86 (41.9 examples/sec; 3.053 sec/batch)
2018-10-15 18:38:31.306586: step 255, loss = 2.88 (42.1 examples/sec; 3.037 sec/batch)
2018-10-15 18:38:34.351906: step 256, loss = 3.06 (42.1 examples/sec; 3.042 sec/batch)
2018-10-15 18:38:37.420578: step 257, loss = 2.89 (41.7 examples/sec; 3.066 sec/batch)
2018-10-15 18:38:40.535161: step 258, loss = 2.93 (41.1 examples/sec; 3.112 sec/batch)
2018-10-15 18:38:43.487356: step 259, loss = 3.03 (43.4 examples/sec; 2.949 sec/batch)
2018-10-15 18:38:46.628250: step 260, loss = 2.95 (40.8 examples/sec; 3.138 sec/batch)
2018-10-15 18:38:49.714822: step 261, loss = 2.92 (41.5 examples/sec; 3.084 sec/batch)
2018-10-15 18:38:52.848824: step 262, loss = 2.91 (40.9 examples/sec; 3.130 sec/batch)
2018-10-15 18:38:55.794048: step 263, loss = 2.94 (43.5 examples/sec; 2.940 sec/batch)
2018-10-15 18:38:58.781391: step 264, loss = 2.89 (42.9 examples/sec; 2.985 sec/batch)
2018-10-15 18:39:01.931325: step 265, loss = 3.04 (40.7 examples/sec; 3.145 sec/batch)
2018-10-15 18:39:05.047544: step 266, loss = 3.10 (41.1 examples/sec; 3.111 sec/batch)
2018-10-15 18:39:08.213921: step 267, loss = 3.04 (40.5 examples/sec; 3.164 sec/batch)
2018-10-15 18:39:11.217044: step 268, loss = 3.09 (42.7 examples/sec; 3.000 sec/batch)
2018-10-15 18:39:14.441001: step 269, loss = 2.98 (39.7 examples/sec; 3.221 sec/batch)
2018-10-15 18:39:17.587401: step 270, loss = 3.07 (40.7 examples/sec; 3.144 sec/batch)
2018-10-15 18:39:20.788113: step 271, loss = 3.12 (40.0 examples/sec; 3.198 sec/batch)
2018-10-15 18:39:23.780977: step 272, loss = 3.11 (42.8 examples/sec; 2.989 sec/batch)
2018-10-15 18:39:26.735032: step 273, loss = 3.14 (43.4 examples/sec; 2.950 sec/batch)
2018-10-15 18:39:29.785667: step 274, loss = 2.94 (42.0 examples/sec; 3.047 sec/batch)
2018-10-15 18:39:32.948028: step 275, loss = 3.09 (40.5 examples/sec; 3.157 sec/batch)
2018-10-15 18:39:36.036206: step 276, loss = 3.05 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:39:39.024566: step 277, loss = 2.96 (42.9 examples/sec; 2.983 sec/batch)
2018-10-15 18:39:42.008427: step 278, loss = 3.12 (42.9 examples/sec; 2.981 sec/batch)
2018-10-15 18:39:45.223434: step 279, loss = 3.13 (39.9 examples/sec; 3.210 sec/batch)
2018-10-15 18:39:48.398063: step 280, loss = 3.12 (40.4 examples/sec; 3.170 sec/batch)
2018-10-15 18:39:51.416252: step 281, loss = 3.04 (42.5 examples/sec; 3.013 sec/batch)
2018-10-15 18:39:54.520461: step 282, loss = 3.11 (41.3 examples/sec; 3.099 sec/batch)
2018-10-15 18:39:57.570560: step 283, loss = 2.99 (42.0 examples/sec; 3.047 sec/batch)
2018-10-15 18:40:00.633423: step 284, loss = 3.02 (41.8 examples/sec; 3.060 sec/batch)
2018-10-15 18:40:03.764175: step 285, loss = 3.03 (41.0 examples/sec; 3.125 sec/batch)
2018-10-15 18:40:06.755292: step 286, loss = 2.90 (42.9 examples/sec; 2.986 sec/batch)
2018-10-15 18:40:09.804092: step 287, loss = 3.06 (42.0 examples/sec; 3.044 sec/batch)
2018-10-15 18:40:12.934117: step 288, loss = 3.04 (41.0 examples/sec; 3.125 sec/batch)
2018-10-15 18:40:15.997358: step 289, loss = 3.05 (41.9 examples/sec; 3.058 sec/batch)
2018-10-15 18:40:19.087897: step 290, loss = 3.17 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:40:22.094583: step 291, loss = 3.05 (42.6 examples/sec; 3.002 sec/batch)
2018-10-15 18:40:25.064417: step 292, loss = 3.03 (43.2 examples/sec; 2.965 sec/batch)
2018-10-15 18:40:28.138978: step 293, loss = 3.18 (41.7 examples/sec; 3.072 sec/batch)
2018-10-15 18:40:31.354517: step 294, loss = 3.04 (39.9 examples/sec; 3.211 sec/batch)
2018-10-15 18:40:34.426556: step 295, loss = 2.94 (41.7 examples/sec; 3.069 sec/batch)
2018-10-15 18:40:37.524557: step 296, loss = 2.94 (41.4 examples/sec; 3.093 sec/batch)
2018-10-15 18:40:40.627280: step 297, loss = 3.07 (41.3 examples/sec; 3.098 sec/batch)
2018-10-15 18:40:43.723269: step 298, loss = 3.11 (41.4 examples/sec; 3.093 sec/batch)
2018-10-15 18:40:46.859267: step 299, loss = 3.13 (40.9 examples/sec; 3.133 sec/batch)
2018-10-15 18:40:49.977978: step 300, loss = 3.24 (41.1 examples/sec; 3.116 sec/batch)
2018-10-15 18:40:53.561100: step 301, loss = 2.93 (41.2 examples/sec; 3.103 sec/batch)
2018-10-15 18:40:56.699781: step 302, loss = 3.11 (40.8 examples/sec; 3.134 sec/batch)
2018-10-15 18:40:59.758528: step 303, loss = 3.10 (41.9 examples/sec; 3.056 sec/batch)
2018-10-15 18:41:02.809091: step 304, loss = 3.13 (42.0 examples/sec; 3.046 sec/batch)
2018-10-15 18:41:05.874811: step 305, loss = 3.10 (41.8 examples/sec; 3.061 sec/batch)
2018-10-15 18:41:08.853811: step 306, loss = 2.91 (43.0 examples/sec; 2.976 sec/batch)
2018-10-15 18:41:12.020257: step 307, loss = 3.02 (40.5 examples/sec; 3.162 sec/batch)
2018-10-15 18:41:15.170087: step 308, loss = 2.92 (40.7 examples/sec; 3.144 sec/batch)
2018-10-15 18:41:18.334871: step 309, loss = 2.95 (40.5 examples/sec; 3.160 sec/batch)
2018-10-15 18:41:21.283957: step 310, loss = 2.94 (43.5 examples/sec; 2.944 sec/batch)
2018-10-15 18:41:24.373942: step 311, loss = 3.10 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:41:27.421682: step 312, loss = 2.94 (42.0 examples/sec; 3.045 sec/batch)
2018-10-15 18:41:30.544349: step 313, loss = 2.95 (41.0 examples/sec; 3.120 sec/batch)
2018-10-15 18:41:33.648505: step 314, loss = 3.04 (41.3 examples/sec; 3.099 sec/batch)
2018-10-15 18:41:36.651581: step 315, loss = 2.94 (42.7 examples/sec; 3.000 sec/batch)
2018-10-15 18:41:39.743387: step 316, loss = 2.92 (41.5 examples/sec; 3.086 sec/batch)
2018-10-15 18:41:42.860385: step 317, loss = 2.95 (41.1 examples/sec; 3.114 sec/batch)
2018-10-15 18:41:45.940326: step 318, loss = 2.89 (41.6 examples/sec; 3.077 sec/batch)
2018-10-15 18:41:49.036896: step 319, loss = 2.96 (41.4 examples/sec; 3.092 sec/batch)
2018-10-15 18:41:52.183521: step 320, loss = 3.00 (40.7 examples/sec; 3.141 sec/batch)
2018-10-15 18:41:55.316619: step 321, loss = 3.10 (40.9 examples/sec; 3.130 sec/batch)
2018-10-15 18:41:58.490054: step 322, loss = 3.12 (40.4 examples/sec; 3.168 sec/batch)
2018-10-15 18:42:01.563720: step 323, loss = 2.86 (41.7 examples/sec; 3.071 sec/batch)
2018-10-15 18:42:04.643182: step 324, loss = 2.96 (41.6 examples/sec; 3.076 sec/batch)
2018-10-15 18:42:07.780061: step 325, loss = 2.92 (40.9 examples/sec; 3.131 sec/batch)
2018-10-15 18:42:10.920710: step 326, loss = 2.86 (40.8 examples/sec; 3.136 sec/batch)
2018-10-15 18:42:13.903773: step 327, loss = 2.97 (43.0 examples/sec; 2.977 sec/batch)
2018-10-15 18:42:16.967823: step 328, loss = 2.94 (41.9 examples/sec; 3.058 sec/batch)
2018-10-15 18:42:19.971082: step 329, loss = 2.92 (42.7 examples/sec; 2.998 sec/batch)
2018-10-15 18:42:23.208533: step 330, loss = 2.90 (39.6 examples/sec; 3.232 sec/batch)
2018-10-15 18:42:26.250098: step 331, loss = 2.96 (42.1 examples/sec; 3.037 sec/batch)
2018-10-15 18:42:29.378135: step 332, loss = 2.94 (41.0 examples/sec; 3.124 sec/batch)
2018-10-15 18:42:32.379803: step 333, loss = 3.04 (42.7 examples/sec; 2.997 sec/batch)
2018-10-15 18:42:35.510884: step 334, loss = 3.05 (40.9 examples/sec; 3.128 sec/batch)
2018-10-15 18:42:38.596759: step 335, loss = 2.97 (41.6 examples/sec; 3.081 sec/batch)
2018-10-15 18:42:41.582743: step 336, loss = 3.03 (42.9 examples/sec; 2.981 sec/batch)
2018-10-15 18:42:44.652907: step 337, loss = 3.00 (41.8 examples/sec; 3.065 sec/batch)
2018-10-15 18:42:47.679180: step 338, loss = 3.02 (42.4 examples/sec; 3.021 sec/batch)
2018-10-15 18:42:50.819520: step 339, loss = 3.02 (40.8 examples/sec; 3.137 sec/batch)
2018-10-15 18:42:53.848370: step 340, loss = 2.95 (42.3 examples/sec; 3.026 sec/batch)
2018-10-15 18:42:56.872600: step 341, loss = 2.89 (42.4 examples/sec; 3.019 sec/batch)
2018-10-15 18:42:59.826037: step 342, loss = 2.94 (43.4 examples/sec; 2.948 sec/batch)
2018-10-15 18:43:02.957320: step 343, loss = 2.91 (40.9 examples/sec; 3.126 sec/batch)
2018-10-15 18:43:06.054786: step 344, loss = 2.82 (41.4 examples/sec; 3.095 sec/batch)
2018-10-15 18:43:09.254902: step 345, loss = 2.98 (40.1 examples/sec; 3.195 sec/batch)
2018-10-15 18:43:12.234980: step 346, loss = 2.99 (43.0 examples/sec; 2.976 sec/batch)
2018-10-15 18:43:15.234499: step 347, loss = 2.96 (42.7 examples/sec; 2.995 sec/batch)
2018-10-15 18:43:18.362388: step 348, loss = 2.90 (41.0 examples/sec; 3.123 sec/batch)
2018-10-15 18:43:21.419319: step 349, loss = 3.10 (41.9 examples/sec; 3.053 sec/batch)
2018-10-15 18:43:24.555375: step 350, loss = 2.85 (40.9 examples/sec; 3.131 sec/batch)
2018-10-15 18:43:27.606439: step 351, loss = 2.99 (42.0 examples/sec; 3.046 sec/batch)
2018-10-15 18:43:30.643735: step 352, loss = 3.02 (42.2 examples/sec; 3.033 sec/batch)
2018-10-15 18:43:33.757349: step 353, loss = 2.96 (41.2 examples/sec; 3.109 sec/batch)
2018-10-15 18:43:36.828191: step 354, loss = 2.77 (41.7 examples/sec; 3.066 sec/batch)
2018-10-15 18:43:39.834573: step 355, loss = 2.73 (42.6 examples/sec; 3.003 sec/batch)
2018-10-15 18:43:42.811571: step 356, loss = 2.92 (43.1 examples/sec; 2.971 sec/batch)
2018-10-15 18:43:45.852475: step 357, loss = 3.03 (42.2 examples/sec; 3.035 sec/batch)
2018-10-15 18:43:48.896102: step 358, loss = 2.76 (42.1 examples/sec; 3.039 sec/batch)
2018-10-15 18:43:52.021382: step 359, loss = 2.78 (41.0 examples/sec; 3.123 sec/batch)
2018-10-15 18:43:55.168191: step 360, loss = 2.85 (40.7 examples/sec; 3.144 sec/batch)
2018-10-15 18:43:58.161422: step 361, loss = 2.90 (42.8 examples/sec; 2.990 sec/batch)
2018-10-15 18:44:01.258624: step 362, loss = 2.97 (41.4 examples/sec; 3.092 sec/batch)
2018-10-15 18:44:04.436214: step 363, loss = 2.91 (40.4 examples/sec; 3.171 sec/batch)
2018-10-15 18:44:07.490664: step 364, loss = 2.86 (41.9 examples/sec; 3.052 sec/batch)
2018-10-15 18:44:10.544837: step 365, loss = 2.78 (42.0 examples/sec; 3.051 sec/batch)
2018-10-15 18:44:13.643474: step 366, loss = 2.80 (41.4 examples/sec; 3.093 sec/batch)
2018-10-15 18:44:16.690630: step 367, loss = 2.85 (42.0 examples/sec; 3.044 sec/batch)
2018-10-15 18:44:19.786843: step 368, loss = 2.85 (41.4 examples/sec; 3.092 sec/batch)
2018-10-15 18:44:22.863352: step 369, loss = 2.92 (41.6 examples/sec; 3.074 sec/batch)
2018-10-15 18:44:25.857301: step 370, loss = 2.96 (42.8 examples/sec; 2.988 sec/batch)
2018-10-15 18:44:28.893165: step 371, loss = 2.93 (42.2 examples/sec; 3.033 sec/batch)
2018-10-15 18:44:32.055937: step 372, loss = 2.89 (40.5 examples/sec; 3.157 sec/batch)
2018-10-15 18:44:35.113567: step 373, loss = 2.96 (41.9 examples/sec; 3.056 sec/batch)
2018-10-15 18:44:38.249735: step 374, loss = 2.97 (40.9 examples/sec; 3.131 sec/batch)
2018-10-15 18:44:41.177399: step 375, loss = 2.91 (43.8 examples/sec; 2.923 sec/batch)
2018-10-15 18:44:44.191501: step 376, loss = 3.04 (42.5 examples/sec; 3.012 sec/batch)
2018-10-15 18:44:47.313175: step 377, loss = 3.02 (41.1 examples/sec; 3.116 sec/batch)
2018-10-15 18:44:50.397416: step 378, loss = 3.07 (41.6 examples/sec; 3.080 sec/batch)
2018-10-15 18:44:53.547302: step 379, loss = 2.78 (40.7 examples/sec; 3.147 sec/batch)
2018-10-15 18:44:56.546817: step 380, loss = 2.98 (42.7 examples/sec; 2.997 sec/batch)
2018-10-15 18:44:59.617464: step 381, loss = 2.99 (41.8 examples/sec; 3.065 sec/batch)
2018-10-15 18:45:02.726079: step 382, loss = 2.98 (41.2 examples/sec; 3.104 sec/batch)
2018-10-15 18:45:05.917795: step 383, loss = 3.04 (40.1 examples/sec; 3.189 sec/batch)
2018-10-15 18:45:08.871905: step 384, loss = 2.89 (43.4 examples/sec; 2.949 sec/batch)
2018-10-15 18:45:11.863328: step 385, loss = 2.88 (42.8 examples/sec; 2.989 sec/batch)
2018-10-15 18:45:14.872093: step 386, loss = 2.97 (42.6 examples/sec; 3.006 sec/batch)
2018-10-15 18:45:18.029751: step 387, loss = 2.88 (40.6 examples/sec; 3.153 sec/batch)
2018-10-15 18:45:21.171215: step 388, loss = 2.98 (40.8 examples/sec; 3.136 sec/batch)
2018-10-15 18:45:24.180274: step 389, loss = 2.85 (42.6 examples/sec; 3.004 sec/batch)
2018-10-15 18:45:27.198390: step 390, loss = 2.76 (42.5 examples/sec; 3.014 sec/batch)
2018-10-15 18:45:30.286127: step 391, loss = 2.85 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:45:33.451326: step 392, loss = 3.19 (40.5 examples/sec; 3.160 sec/batch)
2018-10-15 18:45:36.598140: step 393, loss = 3.12 (40.7 examples/sec; 3.142 sec/batch)
2018-10-15 18:45:39.669378: step 394, loss = 3.02 (41.7 examples/sec; 3.068 sec/batch)
2018-10-15 18:45:42.709300: step 395, loss = 2.94 (42.1 examples/sec; 3.038 sec/batch)
2018-10-15 18:45:45.948992: step 396, loss = 2.98 (39.6 examples/sec; 3.235 sec/batch)
2018-10-15 18:45:49.208013: step 397, loss = 3.09 (39.3 examples/sec; 3.254 sec/batch)
2018-10-15 18:45:52.233298: step 398, loss = 2.98 (42.4 examples/sec; 3.020 sec/batch)
2018-10-15 18:45:55.252355: step 399, loss = 2.84 (42.5 examples/sec; 3.014 sec/batch)
2018-10-15 18:45:58.315805: step 400, loss = 2.99 (41.8 examples/sec; 3.059 sec/batch)
2018-10-15 18:46:01.966717: step 401, loss = 2.80 (41.6 examples/sec; 3.075 sec/batch)
2018-10-15 18:46:05.072494: step 402, loss = 2.86 (41.3 examples/sec; 3.101 sec/batch)
2018-10-15 18:46:08.256396: step 403, loss = 2.90 (40.3 examples/sec; 3.179 sec/batch)
2018-10-15 18:46:11.206888: step 404, loss = 3.01 (43.4 examples/sec; 2.948 sec/batch)
2018-10-15 18:46:14.290097: step 405, loss = 3.10 (41.6 examples/sec; 3.079 sec/batch)
2018-10-15 18:46:17.442959: step 406, loss = 2.98 (40.7 examples/sec; 3.148 sec/batch)
2018-10-15 18:46:20.456454: step 407, loss = 2.94 (42.5 examples/sec; 3.009 sec/batch)
2018-10-15 18:46:23.477264: step 408, loss = 3.04 (42.4 examples/sec; 3.018 sec/batch)
2018-10-15 18:46:26.481538: step 409, loss = 3.05 (42.6 examples/sec; 3.001 sec/batch)
2018-10-15 18:46:29.593849: step 410, loss = 3.02 (41.2 examples/sec; 3.108 sec/batch)
2018-10-15 18:46:32.721054: step 411, loss = 2.90 (41.0 examples/sec; 3.123 sec/batch)
2018-10-15 18:46:35.920815: step 412, loss = 2.90 (40.1 examples/sec; 3.195 sec/batch)
2018-10-15 18:46:38.909362: step 413, loss = 3.09 (42.9 examples/sec; 2.984 sec/batch)
2018-10-15 18:46:41.912993: step 414, loss = 2.94 (42.7 examples/sec; 3.001 sec/batch)
2018-10-15 18:46:45.081886: step 415, loss = 3.03 (40.5 examples/sec; 3.164 sec/batch)
2018-10-15 18:46:48.105980: step 416, loss = 2.89 (42.4 examples/sec; 3.019 sec/batch)
2018-10-15 18:46:51.035627: step 417, loss = 2.95 (43.8 examples/sec; 2.924 sec/batch)
2018-10-15 18:46:54.144363: step 418, loss = 2.85 (41.2 examples/sec; 3.106 sec/batch)
2018-10-15 18:46:57.264107: step 419, loss = 2.87 (41.1 examples/sec; 3.117 sec/batch)
2018-10-15 18:47:00.451330: step 420, loss = 2.89 (40.2 examples/sec; 3.183 sec/batch)
2018-10-15 18:47:03.476079: step 421, loss = 2.85 (42.4 examples/sec; 3.021 sec/batch)
2018-10-15 18:47:06.525975: step 422, loss = 2.70 (42.0 examples/sec; 3.045 sec/batch)
2018-10-15 18:47:09.555872: step 423, loss = 2.93 (42.3 examples/sec; 3.027 sec/batch)
2018-10-15 18:47:12.736612: step 424, loss = 3.02 (40.3 examples/sec; 3.176 sec/batch)
2018-10-15 18:47:15.749590: step 425, loss = 2.92 (42.5 examples/sec; 3.008 sec/batch)
2018-10-15 18:47:18.893066: step 426, loss = 3.02 (40.8 examples/sec; 3.141 sec/batch)
2018-10-15 18:47:21.864673: step 427, loss = 2.97 (43.1 examples/sec; 2.967 sec/batch)
2018-10-15 18:47:25.008338: step 428, loss = 2.75 (40.8 examples/sec; 3.139 sec/batch)
2018-10-15 18:47:28.163257: step 429, loss = 2.88 (40.6 examples/sec; 3.152 sec/batch)
2018-10-15 18:47:31.254154: step 430, loss = 2.83 (41.5 examples/sec; 3.087 sec/batch)
2018-10-15 18:47:34.181971: step 431, loss = 2.76 (43.8 examples/sec; 2.923 sec/batch)
2018-10-15 18:47:37.269123: step 432, loss = 2.79 (41.5 examples/sec; 3.082 sec/batch)
2018-10-15 18:47:40.416499: step 433, loss = 2.87 (40.7 examples/sec; 3.142 sec/batch)
2018-10-15 18:47:43.617544: step 434, loss = 2.82 (40.0 examples/sec; 3.197 sec/batch)
2018-10-15 18:47:46.704437: step 435, loss = 2.88 (41.5 examples/sec; 3.083 sec/batch)
2018-10-15 18:47:49.781408: step 436, loss = 3.02 (41.7 examples/sec; 3.073 sec/batch)
2018-10-15 18:47:52.881457: step 437, loss = 2.99 (41.4 examples/sec; 3.095 sec/batch)
2018-10-15 18:47:55.926332: step 438, loss = 2.81 (42.1 examples/sec; 3.042 sec/batch)
2018-10-15 18:47:59.029484: step 439, loss = 2.86 (41.3 examples/sec; 3.100 sec/batch)
2018-10-15 18:48:02.003763: step 440, loss = 3.05 (43.1 examples/sec; 2.971 sec/batch)
2018-10-15 18:48:05.163167: step 441, loss = 2.93 (40.6 examples/sec; 3.155 sec/batch)
2018-10-15 18:48:08.272122: step 442, loss = 2.96 (41.2 examples/sec; 3.104 sec/batch)
2018-10-15 18:48:11.380326: step 443, loss = 2.96 (41.2 examples/sec; 3.104 sec/batch)
2018-10-15 18:48:14.371039: step 444, loss = 2.82 (42.9 examples/sec; 2.987 sec/batch)
2018-10-15 18:48:17.406284: step 445, loss = 2.79 (42.2 examples/sec; 3.033 sec/batch)
2018-10-15 18:48:20.516064: step 446, loss = 2.91 (41.2 examples/sec; 3.105 sec/batch)
2018-10-15 18:48:23.574301: step 447, loss = 2.97 (41.9 examples/sec; 3.053 sec/batch)
2018-10-15 18:48:26.680370: step 448, loss = 2.84 (41.2 examples/sec; 3.103 sec/batch)
2018-10-15 18:48:29.772507: step 449, loss = 2.93 (41.4 examples/sec; 3.089 sec/batch)
2018-10-15 18:48:32.758245: step 450, loss = 2.78 (42.9 examples/sec; 2.981 sec/batch)
2018-10-15 18:48:35.812703: step 451, loss = 2.97 (42.0 examples/sec; 3.050 sec/batch)
2018-10-15 18:48:38.949401: step 452, loss = 2.89 (40.8 examples/sec; 3.134 sec/batch)
2018-10-15 18:48:42.079831: step 453, loss = 2.81 (40.9 examples/sec; 3.126 sec/batch)
2018-10-15 18:48:45.120698: step 454, loss = 3.04 (42.2 examples/sec; 3.036 sec/batch)
2018-10-15 18:48:48.222123: step 455, loss = 2.87 (41.3 examples/sec; 3.098 sec/batch)
2018-10-15 18:48:51.501155: step 456, loss = 2.81 (39.1 examples/sec; 3.274 sec/batch)
2018-10-15 18:48:54.712127: step 457, loss = 2.78 (39.9 examples/sec; 3.206 sec/batch)
2018-10-15 18:48:57.709616: step 458, loss = 2.74 (42.7 examples/sec; 2.995 sec/batch)
2018-10-15 18:49:00.775684: step 459, loss = 2.66 (41.8 examples/sec; 3.061 sec/batch)
2018-10-15 18:49:03.872300: step 460, loss = 2.84 (41.4 examples/sec; 3.094 sec/batch)
2018-10-15 18:49:07.048311: step 461, loss = 2.77 (40.3 examples/sec; 3.173 sec/batch)
2018-10-15 18:49:10.188559: step 462, loss = 2.79 (40.8 examples/sec; 3.137 sec/batch)
2018-10-15 18:49:13.096116: step 463, loss = 2.69 (44.1 examples/sec; 2.903 sec/batch)
2018-10-15 18:49:16.138628: step 464, loss = 2.84 (42.1 examples/sec; 3.040 sec/batch)
2018-10-15 18:49:19.384570: step 465, loss = 2.69 (39.5 examples/sec; 3.242 sec/batch)
2018-10-15 18:49:22.683702: step 466, loss = 2.71 (38.8 examples/sec; 3.295 sec/batch)
2018-10-15 18:49:25.841263: step 467, loss = 2.77 (40.6 examples/sec; 3.154 sec/batch)
2018-10-15 18:49:28.854354: step 468, loss = 2.75 (42.6 examples/sec; 3.007 sec/batch)
2018-10-15 18:49:31.911571: step 469, loss = 2.76 (41.9 examples/sec; 3.051 sec/batch)
2018-10-15 18:49:35.161860: step 470, loss = 2.71 (39.4 examples/sec; 3.247 sec/batch)
2018-10-15 18:49:38.213449: step 471, loss = 2.72 (42.0 examples/sec; 3.047 sec/batch)
2018-10-15 18:49:41.271423: step 472, loss = 2.96 (41.9 examples/sec; 3.053 sec/batch)
2018-10-15 18:49:44.341622: step 473, loss = 2.95 (41.7 examples/sec; 3.067 sec/batch)
2018-10-15 18:49:47.432271: step 474, loss = 2.69 (41.5 examples/sec; 3.086 sec/batch)
2018-10-15 18:49:50.554003: step 475, loss = 2.90 (41.1 examples/sec; 3.117 sec/batch)
2018-10-15 18:49:53.657533: step 476, loss = 2.79 (41.3 examples/sec; 3.100 sec/batch)
2018-10-15 18:49:56.803049: step 477, loss = 2.89 (40.8 examples/sec; 3.140 sec/batch)
2018-10-15 18:49:59.945951: step 478, loss = 2.90 (40.8 examples/sec; 3.140 sec/batch)
2018-10-15 18:50:03.068481: step 479, loss = 2.77 (41.0 examples/sec; 3.120 sec/batch)
2018-10-15 18:50:06.066675: step 480, loss = 2.83 (42.8 examples/sec; 2.993 sec/batch)
2018-10-15 18:50:09.177410: step 481, loss = 2.82 (41.2 examples/sec; 3.109 sec/batch)
2018-10-15 18:50:12.242009: step 482, loss = 2.94 (41.8 examples/sec; 3.060 sec/batch)
2018-10-15 18:50:15.419173: step 483, loss = 2.99 (40.3 examples/sec; 3.172 sec/batch)
2018-10-15 18:50:18.480757: step 484, loss = 2.84 (41.9 examples/sec; 3.057 sec/batch)
2018-10-15 18:50:21.577698: step 485, loss = 2.74 (41.4 examples/sec; 3.092 sec/batch)
2018-10-15 18:50:24.655441: step 486, loss = 2.99 (41.6 examples/sec; 3.075 sec/batch)
2018-10-15 18:50:27.846890: step 487, loss = 2.83 (40.2 examples/sec; 3.186 sec/batch)
2018-10-15 18:50:30.980869: step 488, loss = 2.81 (40.9 examples/sec; 3.129 sec/batch)
2018-10-15 18:50:34.120924: step 489, loss = 2.77 (40.8 examples/sec; 3.137 sec/batch)
2018-10-15 18:50:37.187318: step 490, loss = 2.84 (41.8 examples/sec; 3.061 sec/batch)
2018-10-15 18:50:40.546795: step 491, loss = 2.80 (38.2 examples/sec; 3.355 sec/batch)
2018-10-15 18:50:43.711092: step 492, loss = 2.84 (40.5 examples/sec; 3.160 sec/batch)
2018-10-15 18:50:46.707925: step 493, loss = 2.82 (42.8 examples/sec; 2.992 sec/batch)
2018-10-15 18:50:49.817335: step 494, loss = 2.74 (41.2 examples/sec; 3.105 sec/batch)
2018-10-15 18:50:53.031579: step 495, loss = 2.73 (39.9 examples/sec; 3.209 sec/batch)
2018-10-15 18:50:56.102180: step 496, loss = 2.82 (41.7 examples/sec; 3.066 sec/batch)
2018-10-15 18:50:59.281433: step 497, loss = 2.81 (40.3 examples/sec; 3.175 sec/batch)
2018-10-15 18:51:02.302840: step 498, loss = 2.92 (42.4 examples/sec; 3.018 sec/batch)
2018-10-15 18:51:05.395254: step 499, loss = 2.89 (41.5 examples/sec; 3.087 sec/batch)
2018-10-15 18:51:08.422096: step 500, loss = 2.92 (42.3 examples/sec; 3.024 sec/batch)
2018-10-15 18:51:12.107263: step 501, loss = 2.90 (40.6 examples/sec; 3.152 sec/batch)
2018-10-15 18:51:15.264198: step 502, loss = 2.76 (40.6 examples/sec; 3.152 sec/batch)
2018-10-15 18:51:18.328186: step 503, loss = 2.80 (41.8 examples/sec; 3.059 sec/batch)
2018-10-15 18:51:21.348515: step 504, loss = 2.78 (42.5 examples/sec; 3.015 sec/batch)
2018-10-15 18:51:24.472392: step 505, loss = 2.83 (41.0 examples/sec; 3.119 sec/batch)
2018-10-15 18:51:27.619997: step 506, loss = 2.67 (40.7 examples/sec; 3.142 sec/batch)
2018-10-15 18:51:30.705302: step 507, loss = 2.74 (41.5 examples/sec; 3.082 sec/batch)
2018-10-15 18:51:33.789096: step 508, loss = 2.87 (41.6 examples/sec; 3.079 sec/batch)
2018-10-15 18:51:36.955427: step 509, loss = 2.80 (40.5 examples/sec; 3.161 sec/batch)
2018-10-15 18:51:40.099140: step 510, loss = 2.74 (40.8 examples/sec; 3.140 sec/batch)
2018-10-15 18:51:43.180684: step 511, loss = 2.85 (41.6 examples/sec; 3.076 sec/batch)
2018-10-15 18:51:46.234203: step 512, loss = 2.80 (42.0 examples/sec; 3.050 sec/batch)
2018-10-15 18:51:49.281719: step 513, loss = 2.87 (42.1 examples/sec; 3.043 sec/batch)
2018-10-15 18:51:52.319638: step 514, loss = 2.96 (42.2 examples/sec; 3.035 sec/batch)
2018-10-15 18:51:55.475729: step 515, loss = 2.95 (40.6 examples/sec; 3.152 sec/batch)
2018-10-15 18:51:58.508522: step 516, loss = 2.96 (42.3 examples/sec; 3.028 sec/batch)
2018-10-15 18:52:01.554449: step 517, loss = 2.82 (42.1 examples/sec; 3.041 sec/batch)
2018-10-15 18:52:04.642761: step 518, loss = 2.81 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:52:07.794673: step 519, loss = 2.84 (40.7 examples/sec; 3.148 sec/batch)
2018-10-15 18:52:11.102757: step 520, loss = 2.80 (38.8 examples/sec; 3.303 sec/batch)
2018-10-15 18:52:14.339642: step 521, loss = 2.81 (39.6 examples/sec; 3.232 sec/batch)
2018-10-15 18:52:17.390298: step 522, loss = 2.86 (42.0 examples/sec; 3.046 sec/batch)
2018-10-15 18:52:20.502362: step 523, loss = 2.69 (41.2 examples/sec; 3.107 sec/batch)
2018-10-15 18:52:23.557954: step 524, loss = 2.74 (42.0 examples/sec; 3.051 sec/batch)
2018-10-15 18:52:26.609740: step 525, loss = 2.80 (42.0 examples/sec; 3.048 sec/batch)
2018-10-15 18:52:29.598688: step 526, loss = 2.81 (42.9 examples/sec; 2.986 sec/batch)
2018-10-15 18:52:32.642415: step 527, loss = 2.85 (42.1 examples/sec; 3.040 sec/batch)
2018-10-15 18:52:35.679959: step 528, loss = 2.73 (42.2 examples/sec; 3.032 sec/batch)
2018-10-15 18:52:38.847202: step 529, loss = 2.79 (40.5 examples/sec; 3.162 sec/batch)
2018-10-15 18:52:42.008321: step 530, loss = 2.86 (40.5 examples/sec; 3.157 sec/batch)
2018-10-15 18:52:45.104829: step 531, loss = 2.77 (41.4 examples/sec; 3.091 sec/batch)
2018-10-15 18:52:48.192418: step 532, loss = 2.79 (41.5 examples/sec; 3.083 sec/batch)
2018-10-15 18:52:51.380239: step 533, loss = 2.79 (40.2 examples/sec; 3.185 sec/batch)
2018-10-15 18:52:54.492963: step 534, loss = 2.74 (41.2 examples/sec; 3.108 sec/batch)
2018-10-15 18:52:57.547109: step 535, loss = 2.88 (42.0 examples/sec; 3.049 sec/batch)
2018-10-15 18:53:00.747672: step 536, loss = 2.72 (40.1 examples/sec; 3.195 sec/batch)
2018-10-15 18:53:03.885902: step 537, loss = 2.70 (40.8 examples/sec; 3.134 sec/batch)
2018-10-15 18:53:06.954547: step 538, loss = 2.73 (41.8 examples/sec; 3.064 sec/batch)
2018-10-15 18:53:10.039864: step 539, loss = 2.73 (41.6 examples/sec; 3.080 sec/batch)
2018-10-15 18:53:13.106218: step 540, loss = 2.79 (41.8 examples/sec; 3.063 sec/batch)
2018-10-15 18:53:16.136632: step 541, loss = 2.77 (42.3 examples/sec; 3.025 sec/batch)
2018-10-15 18:53:19.217406: step 542, loss = 2.76 (41.6 examples/sec; 3.078 sec/batch)
2018-10-15 18:53:22.370648: step 543, loss = 2.95 (40.6 examples/sec; 3.150 sec/batch)
2018-10-15 18:53:25.488029: step 544, loss = 3.00 (41.1 examples/sec; 3.113 sec/batch)
2018-10-15 18:53:28.738440: step 545, loss = 2.83 (39.4 examples/sec; 3.248 sec/batch)
2018-10-15 18:53:31.882738: step 546, loss = 3.01 (40.8 examples/sec; 3.139 sec/batch)
2018-10-15 18:53:34.981345: step 547, loss = 2.83 (41.4 examples/sec; 3.094 sec/batch)
2018-10-15 18:53:38.075905: step 548, loss = 2.76 (41.4 examples/sec; 3.089 sec/batch)
2018-10-15 18:53:41.157841: step 549, loss = 2.77 (41.6 examples/sec; 3.077 sec/batch)
2018-10-15 18:53:44.258505: step 550, loss = 2.78 (41.4 examples/sec; 3.095 sec/batch)
2018-10-15 18:53:47.353601: step 551, loss = 2.69 (41.4 examples/sec; 3.090 sec/batch)
2018-10-15 18:53:50.408737: step 552, loss = 2.75 (42.0 examples/sec; 3.050 sec/batch)
2018-10-15 18:53:53.624025: step 553, loss = 2.79 (39.8 examples/sec; 3.213 sec/batch)
2018-10-15 18:53:56.716961: step 554, loss = 2.84 (41.4 examples/sec; 3.088 sec/batch)
2018-10-15 18:53:59.886333: step 555, loss = 2.72 (40.4 examples/sec; 3.166 sec/batch)
2018-10-15 18:54:03.071055: step 556, loss = 2.76 (40.3 examples/sec; 3.180 sec/batch)
2018-10-15 18:54:06.220273: step 557, loss = 2.66 (40.7 examples/sec; 3.144 sec/batch)
2018-10-15 18:54:09.371761: step 558, loss = 2.87 (40.7 examples/sec; 3.149 sec/batch)
2018-10-15 18:54:12.619826: step 559, loss = 2.84 (39.5 examples/sec; 3.243 sec/batch)
2018-10-15 18:54:15.734830: step 560, loss = 2.62 (41.2 examples/sec; 3.110 sec/batch)
2018-10-15 18:54:18.809433: step 561, loss = 2.79 (41.7 examples/sec; 3.070 sec/batch)
2018-10-15 18:54:21.799244: step 562, loss = 2.54 (42.9 examples/sec; 2.985 sec/batch)
2018-10-15 18:54:24.941589: step 563, loss = 2.81 (40.8 examples/sec; 3.139 sec/batch)
2018-10-15 18:54:27.975215: step 564, loss = 2.66 (42.3 examples/sec; 3.028 sec/batch)
2018-10-15 18:54:31.063003: step 565, loss = 2.75 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:54:34.190441: step 566, loss = 2.66 (41.0 examples/sec; 3.122 sec/batch)
2018-10-15 18:54:37.217596: step 567, loss = 2.66 (42.3 examples/sec; 3.024 sec/batch)
2018-10-15 18:54:40.301220: step 568, loss = 2.66 (41.6 examples/sec; 3.079 sec/batch)
2018-10-15 18:54:43.354084: step 569, loss = 2.66 (42.0 examples/sec; 3.048 sec/batch)
2018-10-15 18:54:46.382560: step 570, loss = 2.69 (42.3 examples/sec; 3.023 sec/batch)
2018-10-15 18:54:49.392944: step 571, loss = 2.56 (42.6 examples/sec; 3.008 sec/batch)
2018-10-15 18:54:52.631384: step 572, loss = 2.75 (39.6 examples/sec; 3.236 sec/batch)
2018-10-15 18:54:55.654214: step 573, loss = 2.65 (42.4 examples/sec; 3.018 sec/batch)
2018-10-15 18:54:58.786237: step 574, loss = 2.61 (40.9 examples/sec; 3.127 sec/batch)
2018-10-15 18:55:01.913908: step 575, loss = 2.79 (41.0 examples/sec; 3.125 sec/batch)
2018-10-15 18:55:05.078965: step 576, loss = 2.67 (40.5 examples/sec; 3.160 sec/batch)
2018-10-15 18:55:08.311018: step 577, loss = 2.69 (39.7 examples/sec; 3.227 sec/batch)
2018-10-15 18:55:11.287693: step 578, loss = 2.65 (43.0 examples/sec; 2.975 sec/batch)
2018-10-15 18:55:14.313682: step 579, loss = 2.69 (42.4 examples/sec; 3.021 sec/batch)
2018-10-15 18:55:17.380888: step 580, loss = 2.74 (41.8 examples/sec; 3.062 sec/batch)
2018-10-15 18:55:20.466606: step 581, loss = 2.76 (41.6 examples/sec; 3.080 sec/batch)
2018-10-15 18:55:23.621308: step 582, loss = 2.89 (40.6 examples/sec; 3.149 sec/batch)
2018-10-15 18:55:26.700155: step 583, loss = 2.72 (41.6 examples/sec; 3.076 sec/batch)
2018-10-15 18:55:29.874780: step 584, loss = 2.63 (40.4 examples/sec; 3.171 sec/batch)
2018-10-15 18:55:33.084869: step 585, loss = 2.81 (39.9 examples/sec; 3.207 sec/batch)
2018-10-15 18:55:36.159242: step 586, loss = 2.84 (41.7 examples/sec; 3.071 sec/batch)
2018-10-15 18:55:39.141064: step 587, loss = 2.83 (43.0 examples/sec; 2.977 sec/batch)
2018-10-15 18:55:42.317959: step 588, loss = 2.83 (40.4 examples/sec; 3.172 sec/batch)
2018-10-15 18:55:45.393066: step 589, loss = 2.87 (41.7 examples/sec; 3.072 sec/batch)
2018-10-15 18:55:48.517737: step 590, loss = 2.71 (41.0 examples/sec; 3.120 sec/batch)
2018-10-15 18:55:51.490090: step 591, loss = 2.71 (43.1 examples/sec; 2.970 sec/batch)
2018-10-15 18:55:54.579451: step 592, loss = 2.80 (41.5 examples/sec; 3.085 sec/batch)
2018-10-15 18:55:57.713016: step 593, loss = 2.73 (40.9 examples/sec; 3.129 sec/batch)
2018-10-15 18:56:00.760710: step 594, loss = 2.65 (42.1 examples/sec; 3.043 sec/batch)
2018-10-15 18:56:03.841057: step 595, loss = 2.70 (41.6 examples/sec; 3.076 sec/batch)
2018-10-15 18:56:06.938536: step 596, loss = 2.70 (41.4 examples/sec; 3.094 sec/batch)
2018-10-15 18:56:09.926269: step 597, loss = 2.72 (42.9 examples/sec; 2.983 sec/batch)
2018-10-15 18:56:13.024862: step 598, loss = 2.72 (41.4 examples/sec; 3.095 sec/batch)
2018-10-15 18:56:16.199655: step 599, loss = 2.84 (40.4 examples/sec; 3.170 sec/batch)
2018-10-15 18:56:19.338520: step 600, loss = 2.81 (40.8 examples/sec; 3.136 sec/batch)
2018-10-15 18:56:22.991750: step 601, loss = 2.78 (41.3 examples/sec; 3.101 sec/batch)
2018-10-15 18:56:26.120736: step 602, loss = 2.68 (41.0 examples/sec; 3.124 sec/batch)
2018-10-15 18:56:29.341967: step 603, loss = 2.77 (39.8 examples/sec; 3.216 sec/batch)
2018-10-15 18:56:32.336341: step 604, loss = 2.94 (42.8 examples/sec; 2.992 sec/batch)
2018-10-15 18:56:35.405444: step 605, loss = 2.75 (41.8 examples/sec; 3.065 sec/batch)
2018-10-15 18:56:38.596138: step 606, loss = 2.73 (40.2 examples/sec; 3.186 sec/batch)
2018-10-15 18:56:41.728143: step 607, loss = 2.64 (40.9 examples/sec; 3.127 sec/batch)
2018-10-15 18:56:44.833042: step 608, loss = 2.77 (41.3 examples/sec; 3.102 sec/batch)
Terminated
