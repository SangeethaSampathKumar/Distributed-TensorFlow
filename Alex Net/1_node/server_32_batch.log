
jayan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode single --dataset flowers --batch_num 10000
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:worker/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:worker/task:0)
Input batch shape: images: (32, 256, 256, 3) labels: (32,)
num_classes: 5
total_num_examples: 320000
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 17:57:43.630450: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session f4afb32522d170fc with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 17:57:49.296861: step 0, loss = 3.91 (10.2 examples/sec; 3.144 sec/batch)
2018-10-15 17:57:52.274277: step 1, loss = 6.70 (13.3 examples/sec; 2.398 sec/batch)
2018-10-15 17:57:54.737282: step 2, loss = 6.87 (13.0 examples/sec; 2.459 sec/batch)
2018-10-15 17:57:57.174683: step 3, loss = 9.61 (13.1 examples/sec; 2.435 sec/batch)
2018-10-15 17:57:59.601398: step 4, loss = 6.65 (13.2 examples/sec; 2.423 sec/batch)
2018-10-15 17:58:01.944081: step 5, loss = 9.53 (13.7 examples/sec; 2.339 sec/batch)
2018-10-15 17:58:04.272656: step 6, loss = 8.34 (13.8 examples/sec; 2.325 sec/batch)
2018-10-15 17:58:06.755972: step 7, loss = 6.97 (12.9 examples/sec; 2.480 sec/batch)
2018-10-15 17:58:09.222578: step 8, loss = 6.25 (13.0 examples/sec; 2.461 sec/batch)
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A2018-10-15 17:58:11.741419: step 9, loss = 7.19 (12.7 examples/sec; 2.517 sec/batch)
2018-10-15 17:58:14.222140: step 10, loss = 5.46 (12.9 examples/sec; 2.477 sec/batch)
2018-10-15 17:58:16.617141: step 11, loss = 4.39 (13.4 examples/sec; 2.393 sec/batch)
2018-10-15 17:58:18.952243: step 12, loss = 5.50 (13.7 examples/sec; 2.332 sec/batch)
2018-10-15 17:58:21.217656: step 13, loss = 7.18 (14.2 examples/sec; 2.261 sec/batch)
2018-10-15 17:58:23.479803: step 14, loss = 5.79 (14.2 examples/sec; 2.258 sec/batch)
2018-10-15 17:58:25.775318: step 15, loss = 6.08 (14.0 examples/sec; 2.291 sec/batch)
2018-10-15 17:58:27.997470: step 16, loss = 4.22 (14.4 examples/sec; 2.218 sec/batch)
2018-10-15 17:58:30.260029: step 17, loss = 3.71 (14.2 examples/sec; 2.258 sec/batch)
2018-10-15 17:58:32.505820: step 18, loss = 4.82 (14.3 examples/sec; 2.241 sec/batch)
2018-10-15 17:58:34.724410: step 19, loss = 4.94 (14.5 examples/sec; 2.214 sec/batch)
2018-10-15 17:58:36.920282: step 20, loss = 4.92 (14.6 examples/sec; 2.191 sec/batch)
2018-10-15 17:58:39.183548: step 21, loss = 4.09 (14.2 examples/sec; 2.259 sec/batch)
2018-10-15 17:58:41.483234: step 22, loss = 4.42 (13.9 examples/sec; 2.295 sec/batch)
2018-10-15 17:58:43.724740: step 23, loss = 4.05 (14.3 examples/sec; 2.237 sec/batch)
2018-10-15 17:58:46.021282: step 24, loss = 4.03 (14.0 examples/sec; 2.293 sec/batch)
2018-10-15 17:58:48.310388: step 25, loss = 3.60 (14.0 examples/sec; 2.284 sec/batch)
2018-10-15 17:58:50.549472: step 26, loss = 4.61 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 17:58:52.729206: step 27, loss = 4.05 (14.7 examples/sec; 2.175 sec/batch)
2018-10-15 17:58:55.107685: step 28, loss = 3.62 (13.5 examples/sec; 2.374 sec/batch)
2018-10-15 17:58:57.281710: step 29, loss = 4.16 (14.7 examples/sec; 2.169 sec/batch)
2018-10-15 17:58:59.518110: step 30, loss = 4.01 (14.3 examples/sec; 2.232 sec/batch)
2018-10-15 17:59:01.886119: step 31, loss = 4.20 (13.5 examples/sec; 2.364 sec/batch)
2018-10-15 17:59:04.040685: step 32, loss = 4.51 (14.9 examples/sec; 2.150 sec/batch)
2018-10-15 17:59:06.241930: step 33, loss = 3.89 (14.6 examples/sec; 2.197 sec/batch)
2018-10-15 17:59:08.528749: step 34, loss = 3.74 (14.0 examples/sec; 2.281 sec/batch)
2018-10-15 17:59:10.694362: step 35, loss = 3.80 (14.8 examples/sec; 2.160 sec/batch)
2018-10-15 17:59:12.894069: step 36, loss = 3.81 (14.6 examples/sec; 2.195 sec/batch)
2018-10-15 17:59:15.187665: step 37, loss = 3.93 (14.0 examples/sec; 2.288 sec/batch)
2018-10-15 17:59:17.444044: step 38, loss = 3.76 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 17:59:19.657998: step 39, loss = 3.79 (14.5 examples/sec; 2.209 sec/batch)
2018-10-15 17:59:22.070411: step 40, loss = 3.73 (13.3 examples/sec; 2.408 sec/batch)
2018-10-15 17:59:24.311665: step 41, loss = 3.70 (14.3 examples/sec; 2.237 sec/batch)
2018-10-15 17:59:26.513589: step 42, loss = 3.66 (14.6 examples/sec; 2.197 sec/batch)
2018-10-15 17:59:28.829826: step 43, loss = 3.56 (13.8 examples/sec; 2.311 sec/batch)
2018-10-15 17:59:31.053099: step 44, loss = 3.64 (14.4 examples/sec; 2.219 sec/batch)
2018-10-15 17:59:33.291226: step 45, loss = 3.46 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 17:59:35.584826: step 46, loss = 3.24 (14.0 examples/sec; 2.288 sec/batch)
2018-10-15 17:59:37.807574: step 47, loss = 4.26 (14.4 examples/sec; 2.217 sec/batch)
2018-10-15 17:59:40.080884: step 48, loss = 4.19 (14.1 examples/sec; 2.269 sec/batch)
2018-10-15 17:59:42.318459: step 49, loss = 3.62 (14.3 examples/sec; 2.233 sec/batch)
2018-10-15 17:59:44.521966: step 50, loss = 3.49 (14.6 examples/sec; 2.199 sec/batch)
2018-10-15 17:59:46.715344: step 51, loss = 3.61 (14.6 examples/sec; 2.189 sec/batch)
2018-10-15 17:59:49.112301: step 52, loss = 3.51 (13.4 examples/sec; 2.392 sec/batch)
2018-10-15 17:59:51.370634: step 53, loss = 3.73 (14.2 examples/sec; 2.256 sec/batch)
2018-10-15 17:59:53.665415: step 54, loss = 3.51 (14.0 examples/sec; 2.290 sec/batch)
2018-10-15 17:59:55.913322: step 55, loss = 3.77 (14.3 examples/sec; 2.243 sec/batch)
2018-10-15 17:59:58.096505: step 56, loss = 3.61 (14.7 examples/sec; 2.178 sec/batch)
2018-10-15 18:00:00.391558: step 57, loss = 3.88 (14.0 examples/sec; 2.290 sec/batch)
^[2018-10-15 18:00:02.637240: step 58, loss = 4.10 (14.3 examples/sec; 2.241 sec/batch)
2018-10-15 18:00:04.846420: step 59, loss = 4.00 (14.5 examples/sec; 2.204 sec/batch)
2018-10-15 18:00:07.112903: step 60, loss = 3.70 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:00:09.320554: step 61, loss = 3.48 (14.5 examples/sec; 2.203 sec/batch)
2018-10-15 18:00:11.538289: step 62, loss = 3.95 (14.5 examples/sec; 2.213 sec/batch)
2018-10-15 18:00:13.901072: step 63, loss = 4.26 (13.6 examples/sec; 2.358 sec/batch)
2018-10-15 18:00:16.201657: step 64, loss = 3.94 (13.9 examples/sec; 2.296 sec/batch)
2018-10-15 18:00:18.445302: step 65, loss = 3.43 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:00:20.785023: step 66, loss = 3.65 (13.7 examples/sec; 2.334 sec/batch)
2018-10-15 18:00:23.031547: step 67, loss = 3.60 (14.3 examples/sec; 2.242 sec/batch)
2018-10-15 18:00:25.289310: step 68, loss = 3.70 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:00:27.601831: step 69, loss = 3.49 (13.9 examples/sec; 2.308 sec/batch)
2018-10-15 18:00:29.768939: step 70, loss = 3.62 (14.8 examples/sec; 2.162 sec/batch)
2018-10-15 18:00:32.054022: step 71, loss = 3.57 (14.0 examples/sec; 2.281 sec/batch)
2018-10-15 18:00:34.366576: step 72, loss = 3.62 (13.9 examples/sec; 2.310 sec/batch)
2018-10-15 18:00:36.583120: step 73, loss = 3.69 (14.5 examples/sec; 2.212 sec/batch)
2018-10-15 18:00:38.838863: step 74, loss = 3.59 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:00:41.100130: step 75, loss = 3.50 (14.2 examples/sec; 2.257 sec/batch)
2018-10-15 18:00:43.343814: step 76, loss = 3.46 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:00:45.675519: step 77, loss = 3.67 (13.8 examples/sec; 2.327 sec/batch)
2018-10-15 18:00:47.958983: step 78, loss = 3.53 (14.0 examples/sec; 2.279 sec/batch)
2018-10-15 18:00:50.212373: step 79, loss = 3.62 (14.2 examples/sec; 2.249 sec/batch)
2018-10-15 18:00:52.597364: step 80, loss = 3.69 (13.4 examples/sec; 2.380 sec/batch)
2018-10-15 18:00:54.912857: step 81, loss = 3.58 (13.8 examples/sec; 2.311 sec/batch)
2018-10-15 18:00:57.206870: step 82, loss = 3.42 (14.0 examples/sec; 2.291 sec/batch)
2018-10-15 18:00:59.422133: step 83, loss = 3.43 (14.5 examples/sec; 2.213 sec/batch)
2018-10-15 18:01:01.683293: step 84, loss = 3.52 (14.2 examples/sec; 2.257 sec/batch)
2018-10-15 18:01:03.995713: step 85, loss = 3.54 (13.9 examples/sec; 2.308 sec/batch)
2018-10-15 18:01:06.197570: step 86, loss = 3.46 (14.6 examples/sec; 2.197 sec/batch)
2018-10-15 18:01:08.434581: step 87, loss = 3.64 (14.3 examples/sec; 2.232 sec/batch)
2018-10-15 18:01:10.818219: step 88, loss = 3.60 (13.4 examples/sec; 2.380 sec/batch)
2018-10-15 18:01:13.061625: step 89, loss = 3.59 (14.3 examples/sec; 2.238 sec/batch)
2018-10-15 18:01:15.283660: step 90, loss = 3.56 (14.4 examples/sec; 2.217 sec/batch)
2018-10-15 18:01:17.609256: step 91, loss = 3.65 (13.8 examples/sec; 2.321 sec/batch)
2018-10-15 18:01:19.800143: step 92, loss = 3.72 (14.6 examples/sec; 2.188 sec/batch)
2018-10-15 18:01:22.080999: step 93, loss = 3.70 (14.1 examples/sec; 2.276 sec/batch)
2018-10-15 18:01:24.375407: step 94, loss = 3.39 (14.0 examples/sec; 2.290 sec/batch)
2018-10-15 18:01:26.568020: step 95, loss = 3.39 (14.6 examples/sec; 2.188 sec/batch)
2018-10-15 18:01:28.852990: step 96, loss = 3.50 (14.0 examples/sec; 2.280 sec/batch)
2018-10-15 18:01:31.101133: step 97, loss = 3.48 (14.3 examples/sec; 2.244 sec/batch)
2018-10-15 18:01:33.345351: step 98, loss = 3.41 (14.3 examples/sec; 2.242 sec/batch)
2018-10-15 18:01:35.588983: step 99, loss = 3.38 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 18:01:37.930279: step 100, loss = 3.54 (13.7 examples/sec; 2.336 sec/batch)
2018-10-15 18:01:40.591452: step 101, loss = 3.64 (14.3 examples/sec; 2.233 sec/batch)
2018-10-15 18:01:42.779168: step 102, loss = 3.54 (14.7 examples/sec; 2.183 sec/batch)
2018-10-15 18:01:45.098307: step 103, loss = 3.59 (13.8 examples/sec; 2.314 sec/batch)
2018-10-15 18:01:47.319254: step 104, loss = 3.52 (14.4 examples/sec; 2.217 sec/batch)
2018-10-15 18:01:49.686804: step 105, loss = 3.45 (13.5 examples/sec; 2.364 sec/batch)
2018-10-15 18:01:51.945227: step 106, loss = 3.50 (14.2 examples/sec; 2.253 sec/batch)
2018-10-15 18:01:54.199283: step 107, loss = 3.53 (14.2 examples/sec; 2.250 sec/batch)
2018-10-15 18:01:56.512439: step 108, loss = 3.60 (13.9 examples/sec; 2.308 sec/batch)
2018-10-15 18:01:58.757663: step 109, loss = 3.51 (14.3 examples/sec; 2.240 sec/batch)
2018-10-15 18:02:00.951598: step 110, loss = 3.54 (14.6 examples/sec; 2.189 sec/batch)
2018-10-15 18:02:03.260708: step 111, loss = 3.57 (13.9 examples/sec; 2.305 sec/batch)
2018-10-15 18:02:05.478534: step 112, loss = 3.50 (14.5 examples/sec; 2.213 sec/batch)
2018-10-15 18:02:07.726263: step 113, loss = 3.51 (14.3 examples/sec; 2.243 sec/batch)
2018-10-15 18:02:09.992740: step 114, loss = 3.56 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:02:12.260401: step 115, loss = 3.56 (14.1 examples/sec; 2.263 sec/batch)
2018-10-15 18:02:14.618120: step 116, loss = 3.49 (13.6 examples/sec; 2.353 sec/batch)
2018-10-15 18:02:16.863852: step 117, loss = 3.48 (14.3 examples/sec; 2.240 sec/batch)
2018-10-15 18:02:19.103774: step 118, loss = 3.45 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 18:02:21.436608: step 119, loss = 3.46 (13.7 examples/sec; 2.328 sec/batch)
2018-10-15 18:02:23.700021: step 120, loss = 3.43 (14.2 examples/sec; 2.259 sec/batch)
2018-10-15 18:02:25.956024: step 121, loss = 3.38 (14.2 examples/sec; 2.251 sec/batch)
2018-10-15 18:02:28.265954: step 122, loss = 3.49 (13.9 examples/sec; 2.305 sec/batch)
2018-10-15 18:02:30.395163: step 123, loss = 3.48 (15.0 examples/sec; 2.126 sec/batch)
2018-10-15 18:02:32.760492: step 124, loss = 3.65 (13.6 examples/sec; 2.360 sec/batch)
2018-10-15 18:02:35.003623: step 125, loss = 3.30 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:02:37.236135: step 126, loss = 3.51 (14.4 examples/sec; 2.228 sec/batch)
2018-10-15 18:02:39.555321: step 127, loss = 3.37 (13.8 examples/sec; 2.314 sec/batch)
2018-10-15 18:02:41.802294: step 128, loss = 3.50 (14.3 examples/sec; 2.242 sec/batch)
2018-10-15 18:02:44.060064: step 129, loss = 3.51 (14.2 examples/sec; 2.253 sec/batch)
2018-10-15 18:02:46.305560: step 130, loss = 3.53 (14.3 examples/sec; 2.241 sec/batch)
2018-10-15 18:02:48.565489: step 131, loss = 3.34 (14.2 examples/sec; 2.256 sec/batch)
2018-10-15 18:02:50.886055: step 132, loss = 3.28 (13.8 examples/sec; 2.316 sec/batch)
2018-10-15 18:02:53.158984: step 133, loss = 3.60 (14.1 examples/sec; 2.271 sec/batch)
2018-10-15 18:02:55.422851: step 134, loss = 3.55 (14.2 examples/sec; 2.259 sec/batch)
2018-10-15 18:02:57.804588: step 135, loss = 3.52 (13.5 examples/sec; 2.377 sec/batch)
2018-10-15 18:03:00.069280: step 136, loss = 3.54 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:03:02.275640: step 137, loss = 3.41 (14.5 examples/sec; 2.201 sec/batch)
2018-10-15 18:03:04.641579: step 138, loss = 3.48 (13.5 examples/sec; 2.362 sec/batch)
2018-10-15 18:03:06.890590: step 139, loss = 3.42 (14.3 examples/sec; 2.244 sec/batch)
2018-10-15 18:03:09.085169: step 140, loss = 3.45 (14.6 examples/sec; 2.190 sec/batch)
2018-10-15 18:03:11.382339: step 141, loss = 3.39 (14.0 examples/sec; 2.293 sec/batch)
2018-10-15 18:03:13.596387: step 142, loss = 3.43 (14.5 examples/sec; 2.211 sec/batch)
2018-10-15 18:03:15.900116: step 143, loss = 3.54 (13.9 examples/sec; 2.299 sec/batch)
2018-10-15 18:03:18.117235: step 144, loss = 3.43 (14.5 examples/sec; 2.212 sec/batch)
2018-10-15 18:03:20.334390: step 145, loss = 3.39 (14.5 examples/sec; 2.213 sec/batch)
2018-10-15 18:03:22.581776: step 146, loss = 3.51 (14.3 examples/sec; 2.245 sec/batch)
2018-10-15 18:03:24.825166: step 147, loss = 3.37 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:03:27.063057: step 148, loss = 3.54 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 18:03:29.366510: step 149, loss = 3.41 (13.9 examples/sec; 2.299 sec/batch)
2018-10-15 18:03:31.631213: step 150, loss = 3.54 (14.2 examples/sec; 2.260 sec/batch)
2018-10-15 18:03:33.859166: step 151, loss = 3.43 (14.4 examples/sec; 2.223 sec/batch)
2018-10-15 18:03:36.118491: step 152, loss = 3.42 (14.2 examples/sec; 2.255 sec/batch)
2018-10-15 18:03:38.454180: step 153, loss = 3.44 (13.7 examples/sec; 2.331 sec/batch)
2018-10-15 18:03:40.699869: step 154, loss = 3.41 (14.3 examples/sec; 2.242 sec/batch)
2018-10-15 18:03:43.089810: step 155, loss = 3.63 (13.4 examples/sec; 2.387 sec/batch)
2018-10-15 18:03:45.340520: step 156, loss = 3.51 (14.2 examples/sec; 2.248 sec/batch)
2018-10-15 18:03:47.559783: step 157, loss = 3.67 (14.4 examples/sec; 2.216 sec/batch)
2018-10-15 18:03:49.899456: step 158, loss = 3.50 (13.7 examples/sec; 2.335 sec/batch)
2018-10-15 18:03:52.162754: step 159, loss = 3.37 (14.2 examples/sec; 2.259 sec/batch)
2018-10-15 18:03:54.453193: step 160, loss = 3.71 (14.0 examples/sec; 2.287 sec/batch)
2018-10-15 18:03:56.725141: step 161, loss = 3.45 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:03:58.934342: step 162, loss = 3.39 (14.5 examples/sec; 2.205 sec/batch)
2018-10-15 18:04:01.116069: step 163, loss = 3.49 (14.7 examples/sec; 2.177 sec/batch)
2018-10-15 18:04:03.388106: step 164, loss = 3.55 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:04:05.703013: step 165, loss = 3.46 (13.9 examples/sec; 2.310 sec/batch)
2018-10-15 18:04:08.007197: step 166, loss = 3.39 (13.9 examples/sec; 2.301 sec/batch)
2018-10-15 18:04:10.274111: step 167, loss = 3.43 (14.1 examples/sec; 2.264 sec/batch)
2018-10-15 18:04:12.478238: step 168, loss = 3.55 (14.5 examples/sec; 2.200 sec/batch)
2018-10-15 18:04:14.777099: step 169, loss = 3.48 (14.0 examples/sec; 2.294 sec/batch)
2018-10-15 18:04:17.057256: step 170, loss = 3.47 (14.1 examples/sec; 2.276 sec/batch)
2018-10-15 18:04:19.357058: step 171, loss = 3.67 (13.9 examples/sec; 2.295 sec/batch)
2018-10-15 18:04:21.774202: step 172, loss = 3.34 (13.3 examples/sec; 2.413 sec/batch)
2018-10-15 18:04:24.055242: step 173, loss = 3.26 (14.1 examples/sec; 2.276 sec/batch)
2018-10-15 18:04:26.247080: step 174, loss = 3.44 (14.6 examples/sec; 2.187 sec/batch)
2018-10-15 18:04:28.579567: step 175, loss = 3.43 (13.7 examples/sec; 2.328 sec/batch)
2018-10-15 18:04:30.827151: step 176, loss = 3.38 (14.3 examples/sec; 2.243 sec/batch)
2018-10-15 18:04:33.114299: step 177, loss = 3.36 (14.0 examples/sec; 2.284 sec/batch)
2018-10-15 18:04:35.381889: step 178, loss = 3.42 (14.1 examples/sec; 2.263 sec/batch)
2018-10-15 18:04:37.601562: step 179, loss = 3.28 (14.4 examples/sec; 2.215 sec/batch)
2018-10-15 18:04:39.842061: step 180, loss = 3.40 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 18:04:42.091613: step 181, loss = 3.20 (14.3 examples/sec; 2.245 sec/batch)
2018-10-15 18:04:44.377795: step 182, loss = 3.50 (14.0 examples/sec; 2.281 sec/batch)
2018-10-15 18:04:46.658641: step 183, loss = 3.24 (14.1 examples/sec; 2.276 sec/batch)
2018-10-15 18:04:48.929933: step 184, loss = 3.58 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:04:51.122908: step 185, loss = 3.45 (14.6 examples/sec; 2.188 sec/batch)
2018-10-15 18:04:53.440871: step 186, loss = 3.48 (13.8 examples/sec; 2.313 sec/batch)
2018-10-15 18:04:55.716834: step 187, loss = 3.44 (14.1 examples/sec; 2.272 sec/batch)
2018-10-15 18:04:57.929870: step 188, loss = 3.46 (14.5 examples/sec; 2.208 sec/batch)
2018-10-15 18:05:00.260799: step 189, loss = 3.18 (13.8 examples/sec; 2.326 sec/batch)
2018-10-15 18:05:02.527606: step 190, loss = 3.34 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:05:04.763380: step 191, loss = 3.45 (14.3 examples/sec; 2.231 sec/batch)
2018-10-15 18:05:06.952331: step 192, loss = 3.51 (14.7 examples/sec; 2.184 sec/batch)
2018-10-15 18:05:09.190316: step 193, loss = 3.44 (14.3 examples/sec; 2.233 sec/batch)
2018-10-15 18:05:11.445216: step 194, loss = 3.37 (14.2 examples/sec; 2.250 sec/batch)
2018-10-15 18:05:13.685501: step 195, loss = 3.37 (14.3 examples/sec; 2.236 sec/batch)
2018-10-15 18:05:15.983976: step 196, loss = 3.33 (13.9 examples/sec; 2.295 sec/batch)
2018-10-15 18:05:18.225034: step 197, loss = 3.40 (14.3 examples/sec; 2.236 sec/batch)
2018-10-15 18:05:20.496652: step 198, loss = 3.19 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:05:22.754135: step 199, loss = 3.33 (14.2 examples/sec; 2.253 sec/batch)
2018-10-15 18:05:24.969332: step 200, loss = 3.21 (14.5 examples/sec; 2.210 sec/batch)
2018-10-15 18:05:27.720574: step 201, loss = 3.61 (13.9 examples/sec; 2.298 sec/batch)
2018-10-15 18:05:29.904004: step 202, loss = 3.57 (14.7 examples/sec; 2.179 sec/batch)
2018-10-15 18:05:32.085946: step 203, loss = 3.53 (14.7 examples/sec; 2.179 sec/batch)
2018-10-15 18:05:34.359914: step 204, loss = 3.46 (14.1 examples/sec; 2.271 sec/batch)
2018-10-15 18:05:36.593563: step 205, loss = 3.25 (14.4 examples/sec; 2.229 sec/batch)
2018-10-15 18:05:38.869013: step 206, loss = 3.33 (14.1 examples/sec; 2.271 sec/batch)
2018-10-15 18:05:41.125950: step 207, loss = 3.34 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:05:43.351980: step 208, loss = 3.42 (14.4 examples/sec; 2.221 sec/batch)
2018-10-15 18:05:45.633358: step 209, loss = 3.42 (14.1 examples/sec; 2.276 sec/batch)
2018-10-15 18:05:47.879262: step 210, loss = 3.31 (14.3 examples/sec; 2.241 sec/batch)
2018-10-15 18:05:50.127285: step 211, loss = 3.30 (14.3 examples/sec; 2.243 sec/batch)
2018-10-15 18:05:52.390841: step 212, loss = 3.38 (14.2 examples/sec; 2.258 sec/batch)
2018-10-15 18:05:54.605403: step 213, loss = 3.31 (14.5 examples/sec; 2.209 sec/batch)
2018-10-15 18:05:56.842062: step 214, loss = 3.55 (14.3 examples/sec; 2.232 sec/batch)
2018-10-15 18:05:59.075392: step 215, loss = 3.40 (14.4 examples/sec; 2.229 sec/batch)
2018-10-15 18:06:01.340227: step 216, loss = 3.39 (14.2 examples/sec; 2.260 sec/batch)
2018-10-15 18:06:03.560538: step 217, loss = 3.33 (14.4 examples/sec; 2.215 sec/batch)
2018-10-15 18:06:05.862787: step 218, loss = 3.34 (13.9 examples/sec; 2.297 sec/batch)
2018-10-15 18:06:08.126960: step 219, loss = 3.46 (14.2 examples/sec; 2.260 sec/batch)
2018-10-15 18:06:10.325187: step 220, loss = 3.58 (14.6 examples/sec; 2.194 sec/batch)
2018-10-15 18:06:12.632514: step 221, loss = 3.51 (13.9 examples/sec; 2.302 sec/batch)
2018-10-15 18:06:14.912968: step 222, loss = 3.34 (14.1 examples/sec; 2.276 sec/batch)
2018-10-15 18:06:17.206439: step 223, loss = 3.43 (14.0 examples/sec; 2.291 sec/batch)
2018-10-15 18:06:19.514101: step 224, loss = 3.31 (13.9 examples/sec; 2.303 sec/batch)
2018-10-15 18:06:21.745502: step 225, loss = 3.21 (14.4 examples/sec; 2.227 sec/batch)
2018-10-15 18:06:24.042122: step 226, loss = 3.29 (14.0 examples/sec; 2.292 sec/batch)
2018-10-15 18:06:26.319901: step 227, loss = 3.24 (14.1 examples/sec; 2.273 sec/batch)
2018-10-15 18:06:28.608143: step 228, loss = 3.60 (14.0 examples/sec; 2.283 sec/batch)
2018-10-15 18:06:30.940625: step 229, loss = 3.27 (13.7 examples/sec; 2.328 sec/batch)
2018-10-15 18:06:33.184876: step 230, loss = 3.37 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:06:35.471078: step 231, loss = 3.32 (14.0 examples/sec; 2.282 sec/batch)
2018-10-15 18:06:37.789457: step 232, loss = 3.14 (13.8 examples/sec; 2.314 sec/batch)
2018-10-15 18:06:40.043843: step 233, loss = 3.21 (14.2 examples/sec; 2.249 sec/batch)
2018-10-15 18:06:42.282643: step 234, loss = 3.45 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 18:06:44.575721: step 235, loss = 3.42 (14.0 examples/sec; 2.290 sec/batch)
2018-10-15 18:06:46.794471: step 236, loss = 3.33 (14.5 examples/sec; 2.214 sec/batch)
2018-10-15 18:06:49.130404: step 237, loss = 3.24 (13.7 examples/sec; 2.331 sec/batch)
2018-10-15 18:06:51.424102: step 238, loss = 3.26 (14.0 examples/sec; 2.289 sec/batch)
2018-10-15 18:06:53.696949: step 239, loss = 3.33 (14.1 examples/sec; 2.268 sec/batch)
2018-10-15 18:06:56.050301: step 240, loss = 3.28 (13.6 examples/sec; 2.349 sec/batch)
2018-10-15 18:06:58.303839: step 241, loss = 3.17 (14.2 examples/sec; 2.249 sec/batch)
2018-10-15 18:07:00.603500: step 242, loss = 3.39 (13.9 examples/sec; 2.294 sec/batch)
2018-10-15 18:07:02.881716: step 243, loss = 3.27 (14.1 examples/sec; 2.273 sec/batch)
2018-10-15 18:07:05.105113: step 244, loss = 3.30 (14.4 examples/sec; 2.219 sec/batch)
2018-10-15 18:07:07.431670: step 245, loss = 3.35 (13.8 examples/sec; 2.324 sec/batch)
2018-10-15 18:07:09.664624: step 246, loss = 3.29 (14.4 examples/sec; 2.227 sec/batch)
2018-10-15 18:07:11.868087: step 247, loss = 3.35 (14.6 examples/sec; 2.198 sec/batch)
2018-10-15 18:07:14.272267: step 248, loss = 3.33 (13.3 examples/sec; 2.399 sec/batch)
2018-10-15 18:07:16.479654: step 249, loss = 3.40 (14.5 examples/sec; 2.205 sec/batch)
2018-10-15 18:07:18.840364: step 250, loss = 3.26 (13.6 examples/sec; 2.356 sec/batch)
2018-10-15 18:07:21.160425: step 251, loss = 3.27 (13.8 examples/sec; 2.315 sec/batch)
2018-10-15 18:07:23.365641: step 252, loss = 3.36 (14.5 examples/sec; 2.201 sec/batch)
2018-10-15 18:07:25.716119: step 253, loss = 3.10 (13.6 examples/sec; 2.346 sec/batch)
2018-10-15 18:07:28.024165: step 254, loss = 3.24 (13.9 examples/sec; 2.305 sec/batch)
2018-10-15 18:07:30.255964: step 255, loss = 3.42 (14.4 examples/sec; 2.229 sec/batch)
2018-10-15 18:07:32.646972: step 256, loss = 3.18 (13.4 examples/sec; 2.386 sec/batch)
2018-10-15 18:07:34.903209: step 257, loss = 3.25 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:07:37.182836: step 258, loss = 3.33 (14.1 examples/sec; 2.275 sec/batch)
2018-10-15 18:07:39.501405: step 259, loss = 3.11 (13.8 examples/sec; 2.313 sec/batch)
2018-10-15 18:07:41.741138: step 260, loss = 3.25 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 18:07:44.012184: step 261, loss = 3.29 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:07:46.343120: step 262, loss = 3.72 (13.8 examples/sec; 2.327 sec/batch)
2018-10-15 18:07:48.580096: step 263, loss = 3.29 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 18:07:50.973325: step 264, loss = 3.34 (13.4 examples/sec; 2.389 sec/batch)
2018-10-15 18:07:53.262031: step 265, loss = 3.09 (14.0 examples/sec; 2.284 sec/batch)
2018-10-15 18:07:55.526173: step 266, loss = 3.40 (14.2 examples/sec; 2.260 sec/batch)
2018-10-15 18:07:57.853029: step 267, loss = 3.26 (13.8 examples/sec; 2.322 sec/batch)
2018-10-15 18:08:00.135331: step 268, loss = 3.26 (14.1 examples/sec; 2.277 sec/batch)
2018-10-15 18:08:02.444201: step 269, loss = 3.45 (13.9 examples/sec; 2.303 sec/batch)
2018-10-15 18:08:04.743342: step 270, loss = 3.26 (13.9 examples/sec; 2.294 sec/batch)
2018-10-15 18:08:06.974253: step 271, loss = 3.19 (14.4 examples/sec; 2.226 sec/batch)
2018-10-15 18:08:09.271970: step 272, loss = 2.88 (14.0 examples/sec; 2.293 sec/batch)
2018-10-15 18:08:11.557395: step 273, loss = 3.19 (14.0 examples/sec; 2.280 sec/batch)
2018-10-15 18:08:13.755972: step 274, loss = 3.19 (14.6 examples/sec; 2.194 sec/batch)
2018-10-15 18:08:15.995999: step 275, loss = 3.07 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 18:08:18.221399: step 276, loss = 3.29 (14.4 examples/sec; 2.220 sec/batch)
2018-10-15 18:08:20.599229: step 277, loss = 3.29 (13.5 examples/sec; 2.373 sec/batch)
2018-10-15 18:08:22.871648: step 278, loss = 3.13 (14.1 examples/sec; 2.268 sec/batch)
2018-10-15 18:08:25.056431: step 279, loss = 3.34 (14.7 examples/sec; 2.180 sec/batch)
2018-10-15 18:08:27.393194: step 280, loss = 3.16 (13.7 examples/sec; 2.332 sec/batch)
2018-10-15 18:08:29.650039: step 281, loss = 3.36 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:08:31.895865: step 282, loss = 3.01 (14.3 examples/sec; 2.241 sec/batch)
2018-10-15 18:08:34.225362: step 283, loss = 3.30 (13.8 examples/sec; 2.324 sec/batch)
2018-10-15 18:08:36.435923: step 284, loss = 3.03 (14.5 examples/sec; 2.206 sec/batch)
2018-10-15 18:08:38.715265: step 285, loss = 3.38 (14.1 examples/sec; 2.275 sec/batch)
2018-10-15 18:08:41.004797: step 286, loss = 2.99 (14.0 examples/sec; 2.285 sec/batch)
2018-10-15 18:08:43.248866: step 287, loss = 3.21 (14.3 examples/sec; 2.240 sec/batch)
2018-10-15 18:08:45.608453: step 288, loss = 3.10 (13.6 examples/sec; 2.355 sec/batch)
2018-10-15 18:08:47.839262: step 289, loss = 2.92 (14.4 examples/sec; 2.226 sec/batch)
2018-10-15 18:08:50.031537: step 290, loss = 2.99 (14.6 examples/sec; 2.188 sec/batch)
2018-10-15 18:08:52.378392: step 291, loss = 3.44 (13.7 examples/sec; 2.342 sec/batch)
2018-10-15 18:08:54.622188: step 292, loss = 3.35 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:08:56.847332: step 293, loss = 3.52 (14.4 examples/sec; 2.221 sec/batch)
2018-10-15 18:08:59.127230: step 294, loss = 3.27 (14.1 examples/sec; 2.277 sec/batch)
2018-10-15 18:09:01.373400: step 295, loss = 3.46 (14.3 examples/sec; 2.242 sec/batch)
2018-10-15 18:09:03.684823: step 296, loss = 3.13 (13.9 examples/sec; 2.308 sec/batch)
2018-10-15 18:09:06.008511: step 297, loss = 3.15 (13.8 examples/sec; 2.321 sec/batch)
2018-10-15 18:09:08.245900: step 298, loss = 3.01 (14.3 examples/sec; 2.233 sec/batch)
2018-10-15 18:09:10.591495: step 299, loss = 3.05 (13.7 examples/sec; 2.342 sec/batch)
2018-10-15 18:09:12.866136: step 300, loss = 3.05 (14.1 examples/sec; 2.269 sec/batch)
2018-10-15 18:09:15.596608: step 301, loss = 3.04 (14.2 examples/sec; 2.259 sec/batch)
2018-10-15 18:09:17.902544: step 302, loss = 3.16 (13.9 examples/sec; 2.301 sec/batch)
2018-10-15 18:09:20.078625: step 303, loss = 3.06 (14.7 examples/sec; 2.171 sec/batch)
2018-10-15 18:09:22.406725: step 304, loss = 3.03 (13.8 examples/sec; 2.323 sec/batch)
2018-10-15 18:09:24.712569: step 305, loss = 3.22 (13.9 examples/sec; 2.301 sec/batch)
2018-10-15 18:09:26.911171: step 306, loss = 2.95 (14.6 examples/sec; 2.194 sec/batch)
2018-10-15 18:09:29.276100: step 307, loss = 3.10 (13.6 examples/sec; 2.359 sec/batch)
2018-10-15 18:09:31.512531: step 308, loss = 3.32 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 18:09:33.747334: step 309, loss = 3.04 (14.3 examples/sec; 2.231 sec/batch)
2018-10-15 18:09:36.003136: step 310, loss = 2.83 (14.2 examples/sec; 2.251 sec/batch)
2018-10-15 18:09:38.256349: step 311, loss = 3.06 (14.2 examples/sec; 2.248 sec/batch)
2018-10-15 18:09:40.586270: step 312, loss = 3.14 (13.8 examples/sec; 2.325 sec/batch)
2018-10-15 18:09:42.810991: step 313, loss = 2.94 (14.4 examples/sec; 2.220 sec/batch)
2018-10-15 18:09:45.074186: step 314, loss = 3.06 (14.2 examples/sec; 2.259 sec/batch)
2018-10-15 18:09:47.404400: step 315, loss = 3.04 (13.8 examples/sec; 2.326 sec/batch)
2018-10-15 18:09:49.692367: step 316, loss = 3.16 (14.0 examples/sec; 2.283 sec/batch)
2018-10-15 18:09:51.902255: step 317, loss = 2.92 (14.5 examples/sec; 2.205 sec/batch)
2018-10-15 18:09:54.167330: step 318, loss = 2.97 (14.2 examples/sec; 2.260 sec/batch)
2018-10-15 18:09:56.413268: step 319, loss = 3.17 (14.3 examples/sec; 2.241 sec/batch)
2018-10-15 18:09:58.712678: step 320, loss = 3.28 (13.9 examples/sec; 2.295 sec/batch)
2018-10-15 18:10:00.978190: step 321, loss = 3.05 (14.2 examples/sec; 2.261 sec/batch)
2018-10-15 18:10:03.246547: step 322, loss = 3.09 (14.1 examples/sec; 2.263 sec/batch)
2018-10-15 18:10:05.611949: step 323, loss = 2.99 (13.6 examples/sec; 2.360 sec/batch)
2018-10-15 18:10:07.909175: step 324, loss = 3.07 (14.0 examples/sec; 2.292 sec/batch)
2018-10-15 18:10:10.176841: step 325, loss = 3.02 (14.1 examples/sec; 2.263 sec/batch)
2018-10-15 18:10:12.478305: step 326, loss = 3.00 (13.9 examples/sec; 2.298 sec/batch)
2018-10-15 18:10:14.735433: step 327, loss = 3.27 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:10:17.105221: step 328, loss = 3.05 (13.5 examples/sec; 2.365 sec/batch)
2018-10-15 18:10:19.380507: step 329, loss = 3.25 (14.1 examples/sec; 2.271 sec/batch)
2018-10-15 18:10:21.653832: step 330, loss = 3.13 (14.1 examples/sec; 2.268 sec/batch)
2018-10-15 18:10:23.939619: step 331, loss = 3.08 (14.0 examples/sec; 2.281 sec/batch)
2018-10-15 18:10:26.166038: step 332, loss = 3.24 (14.4 examples/sec; 2.222 sec/batch)
2018-10-15 18:10:28.571429: step 333, loss = 2.78 (13.3 examples/sec; 2.401 sec/batch)
2018-10-15 18:10:30.858214: step 334, loss = 3.19 (14.0 examples/sec; 2.282 sec/batch)
2018-10-15 18:10:33.116451: step 335, loss = 3.11 (14.2 examples/sec; 2.254 sec/batch)
2018-10-15 18:10:35.456928: step 336, loss = 3.49 (13.7 examples/sec; 2.336 sec/batch)
2018-10-15 18:10:37.697010: step 337, loss = 3.29 (14.3 examples/sec; 2.236 sec/batch)
2018-10-15 18:10:40.045057: step 338, loss = 3.36 (13.7 examples/sec; 2.343 sec/batch)
2018-10-15 18:10:42.360509: step 339, loss = 3.24 (13.8 examples/sec; 2.313 sec/batch)
2018-10-15 18:10:44.570165: step 340, loss = 3.12 (14.5 examples/sec; 2.205 sec/batch)
2018-10-15 18:10:46.875248: step 341, loss = 3.05 (13.9 examples/sec; 2.301 sec/batch)
2018-10-15 18:10:49.130866: step 342, loss = 3.12 (14.2 examples/sec; 2.251 sec/batch)
2018-10-15 18:10:51.387571: step 343, loss = 3.29 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:10:53.666281: step 344, loss = 3.26 (14.1 examples/sec; 2.274 sec/batch)
2018-10-15 18:10:55.959405: step 345, loss = 3.15 (14.0 examples/sec; 2.289 sec/batch)
2018-10-15 18:10:58.275322: step 346, loss = 3.08 (13.8 examples/sec; 2.311 sec/batch)
2018-10-15 18:11:00.574983: step 347, loss = 3.05 (13.9 examples/sec; 2.295 sec/batch)
2018-10-15 18:11:02.754464: step 348, loss = 3.00 (14.7 examples/sec; 2.177 sec/batch)
2018-10-15 18:11:05.070996: step 349, loss = 3.09 (13.8 examples/sec; 2.312 sec/batch)
2018-10-15 18:11:07.327306: step 350, loss = 3.10 (14.2 examples/sec; 2.251 sec/batch)
2018-10-15 18:11:09.596170: step 351, loss = 3.09 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:11:11.861734: step 352, loss = 3.19 (14.1 examples/sec; 2.263 sec/batch)
2018-10-15 18:11:14.078958: step 353, loss = 3.33 (14.5 examples/sec; 2.213 sec/batch)
2018-10-15 18:11:16.346206: step 354, loss = 2.95 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:11:18.612743: step 355, loss = 2.98 (14.2 examples/sec; 2.261 sec/batch)
2018-10-15 18:11:20.874087: step 356, loss = 2.84 (14.2 examples/sec; 2.256 sec/batch)
2018-10-15 18:11:23.168454: step 357, loss = 3.01 (14.0 examples/sec; 2.290 sec/batch)
2018-10-15 18:11:25.362800: step 358, loss = 3.21 (14.6 examples/sec; 2.189 sec/batch)
2018-10-15 18:11:27.662742: step 359, loss = 3.16 (13.9 examples/sec; 2.295 sec/batch)
2018-10-15 18:11:30.001534: step 360, loss = 3.05 (13.7 examples/sec; 2.333 sec/batch)
2018-10-15 18:11:32.241511: step 361, loss = 2.92 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 18:11:34.466740: step 362, loss = 2.86 (14.4 examples/sec; 2.221 sec/batch)
2018-10-15 18:11:36.822468: step 363, loss = 3.34 (13.6 examples/sec; 2.351 sec/batch)
2018-10-15 18:11:39.057898: step 364, loss = 3.11 (14.4 examples/sec; 2.230 sec/batch)
2018-10-15 18:11:41.352217: step 365, loss = 3.09 (14.0 examples/sec; 2.289 sec/batch)
2018-10-15 18:11:43.691393: step 366, loss = 3.06 (13.7 examples/sec; 2.335 sec/batch)
2018-10-15 18:11:45.941728: step 367, loss = 2.99 (14.3 examples/sec; 2.245 sec/batch)
2018-10-15 18:11:48.247830: step 368, loss = 3.13 (13.9 examples/sec; 2.301 sec/batch)
2018-10-15 18:11:50.551015: step 369, loss = 3.00 (13.9 examples/sec; 2.297 sec/batch)
2018-10-15 18:11:52.823858: step 370, loss = 3.12 (14.1 examples/sec; 2.268 sec/batch)
2018-10-15 18:11:55.177214: step 371, loss = 2.91 (13.6 examples/sec; 2.348 sec/batch)
2018-10-15 18:11:57.427824: step 372, loss = 3.23 (14.2 examples/sec; 2.248 sec/batch)
2018-10-15 18:11:59.647981: step 373, loss = 3.19 (14.4 examples/sec; 2.215 sec/batch)
2018-10-15 18:12:01.994793: step 374, loss = 3.18 (13.7 examples/sec; 2.342 sec/batch)
2018-10-15 18:12:04.225890: step 375, loss = 3.14 (14.4 examples/sec; 2.226 sec/batch)
2018-10-15 18:12:06.452379: step 376, loss = 3.09 (14.4 examples/sec; 2.224 sec/batch)
2018-10-15 18:12:08.814124: step 377, loss = 3.20 (13.6 examples/sec; 2.357 sec/batch)
2018-10-15 18:12:11.049762: step 378, loss = 3.14 (14.3 examples/sec; 2.231 sec/batch)
2018-10-15 18:12:13.404959: step 379, loss = 2.97 (13.6 examples/sec; 2.350 sec/batch)
2018-10-15 18:12:15.663534: step 380, loss = 3.03 (14.2 examples/sec; 2.256 sec/batch)
2018-10-15 18:12:17.841420: step 381, loss = 3.09 (14.7 examples/sec; 2.173 sec/batch)
2018-10-15 18:12:20.194637: step 382, loss = 2.74 (13.6 examples/sec; 2.349 sec/batch)
2018-10-15 18:12:22.468024: step 383, loss = 3.11 (14.1 examples/sec; 2.268 sec/batch)
2018-10-15 18:12:24.810870: step 384, loss = 3.11 (13.7 examples/sec; 2.338 sec/batch)
2018-10-15 18:12:27.117681: step 385, loss = 2.85 (13.9 examples/sec; 2.302 sec/batch)
2018-10-15 18:12:29.337599: step 386, loss = 3.07 (14.4 examples/sec; 2.215 sec/batch)
2018-10-15 18:12:31.649739: step 387, loss = 2.98 (13.9 examples/sec; 2.307 sec/batch)
2018-10-15 18:12:33.871326: step 388, loss = 3.11 (14.4 examples/sec; 2.217 sec/batch)
2018-10-15 18:12:36.062883: step 389, loss = 2.87 (14.6 examples/sec; 2.187 sec/batch)
2018-10-15 18:12:38.336802: step 390, loss = 3.10 (14.1 examples/sec; 2.270 sec/batch)
2018-10-15 18:12:40.612829: step 391, loss = 3.14 (14.1 examples/sec; 2.273 sec/batch)
2018-10-15 18:12:42.804523: step 392, loss = 3.61 (14.6 examples/sec; 2.187 sec/batch)
2018-10-15 18:12:45.086671: step 393, loss = 3.07 (14.1 examples/sec; 2.277 sec/batch)
2018-10-15 18:12:47.345694: step 394, loss = 2.95 (14.2 examples/sec; 2.254 sec/batch)
2018-10-15 18:12:49.584619: step 395, loss = 3.30 (14.3 examples/sec; 2.233 sec/batch)
2018-10-15 18:12:51.901195: step 396, loss = 3.13 (13.8 examples/sec; 2.312 sec/batch)
2018-10-15 18:12:54.135096: step 397, loss = 3.15 (14.3 examples/sec; 2.231 sec/batch)
2018-10-15 18:12:56.423122: step 398, loss = 3.00 (14.0 examples/sec; 2.283 sec/batch)
2018-10-15 18:12:58.762229: step 399, loss = 2.95 (13.7 examples/sec; 2.334 sec/batch)
2018-10-15 18:13:01.019090: step 400, loss = 2.92 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:13:04.037547: step 401, loss = 3.25 (13.7 examples/sec; 2.335 sec/batch)
2018-10-15 18:13:06.268249: step 402, loss = 3.15 (14.4 examples/sec; 2.226 sec/batch)
2018-10-15 18:13:08.495648: step 403, loss = 3.02 (14.4 examples/sec; 2.225 sec/batch)
2018-10-15 18:13:10.830771: step 404, loss = 2.96 (13.7 examples/sec; 2.330 sec/batch)
2018-10-15 18:13:13.108981: step 405, loss = 2.90 (14.1 examples/sec; 2.273 sec/batch)
2018-10-15 18:13:15.383954: step 406, loss = 3.14 (14.1 examples/sec; 2.272 sec/batch)
2018-10-15 18:13:17.739065: step 407, loss = 2.90 (13.6 examples/sec; 2.350 sec/batch)
2018-10-15 18:13:20.004086: step 408, loss = 3.21 (14.2 examples/sec; 2.260 sec/batch)
2018-10-15 18:13:22.303809: step 409, loss = 3.62 (13.9 examples/sec; 2.297 sec/batch)
2018-10-15 18:13:24.570473: step 410, loss = 3.13 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:13:26.763464: step 411, loss = 3.12 (14.6 examples/sec; 2.188 sec/batch)
2018-10-15 18:13:29.109363: step 412, loss = 3.04 (13.7 examples/sec; 2.341 sec/batch)
2018-10-15 18:13:31.334300: step 413, loss = 3.11 (14.4 examples/sec; 2.222 sec/batch)
2018-10-15 18:13:33.594338: step 414, loss = 3.05 (14.2 examples/sec; 2.255 sec/batch)
2018-10-15 18:13:35.861113: step 415, loss = 3.10 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:13:38.151999: step 416, loss = 3.08 (14.0 examples/sec; 2.286 sec/batch)
2018-10-15 18:13:40.434695: step 417, loss = 3.23 (14.0 examples/sec; 2.278 sec/batch)
2018-10-15 18:13:42.827225: step 418, loss = 2.90 (13.4 examples/sec; 2.388 sec/batch)
2018-10-15 18:13:45.013080: step 419, loss = 2.97 (14.7 examples/sec; 2.181 sec/batch)
2018-10-15 18:13:47.332845: step 420, loss = 3.12 (13.8 examples/sec; 2.315 sec/batch)
2018-10-15 18:13:49.624460: step 421, loss = 3.06 (14.0 examples/sec; 2.287 sec/batch)
2018-10-15 18:13:51.882310: step 422, loss = 2.86 (14.2 examples/sec; 2.253 sec/batch)
2018-10-15 18:13:54.184631: step 423, loss = 3.26 (13.9 examples/sec; 2.299 sec/batch)
2018-10-15 18:13:56.397039: step 424, loss = 2.95 (14.5 examples/sec; 2.210 sec/batch)
2018-10-15 18:13:58.565144: step 425, loss = 3.01 (14.8 examples/sec; 2.165 sec/batch)
2018-10-15 18:14:00.835537: step 426, loss = 2.92 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:14:03.093404: step 427, loss = 2.92 (14.2 examples/sec; 2.253 sec/batch)
2018-10-15 18:14:05.310618: step 428, loss = 3.13 (14.5 examples/sec; 2.212 sec/batch)
2018-10-15 18:14:07.636307: step 429, loss = 3.18 (13.8 examples/sec; 2.321 sec/batch)
2018-10-15 18:14:09.935707: step 430, loss = 2.99 (13.9 examples/sec; 2.295 sec/batch)
2018-10-15 18:14:12.154222: step 431, loss = 2.79 (14.5 examples/sec; 2.214 sec/batch)
2018-10-15 18:14:14.574264: step 432, loss = 3.49 (13.2 examples/sec; 2.415 sec/batch)
2018-10-15 18:14:16.790424: step 433, loss = 2.98 (14.5 examples/sec; 2.212 sec/batch)
2018-10-15 18:14:19.005815: step 434, loss = 3.14 (14.5 examples/sec; 2.213 sec/batch)
2018-10-15 18:14:21.352753: step 435, loss = 2.87 (13.7 examples/sec; 2.342 sec/batch)
2018-10-15 18:14:23.607378: step 436, loss = 3.03 (14.2 examples/sec; 2.250 sec/batch)
2018-10-15 18:14:25.861117: step 437, loss = 2.72 (14.2 examples/sec; 2.250 sec/batch)
2018-10-15 18:14:28.173841: step 438, loss = 2.91 (13.9 examples/sec; 2.307 sec/batch)
2018-10-15 18:14:30.452057: step 439, loss = 3.03 (14.1 examples/sec; 2.274 sec/batch)
2018-10-15 18:14:32.746426: step 440, loss = 2.79 (14.0 examples/sec; 2.289 sec/batch)
2018-10-15 18:14:34.978301: step 441, loss = 3.01 (14.4 examples/sec; 2.227 sec/batch)
2018-10-15 18:14:37.222919: step 442, loss = 2.77 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:14:39.536854: step 443, loss = 2.79 (13.8 examples/sec; 2.311 sec/batch)
2018-10-15 18:14:41.806023: step 444, loss = 3.08 (14.1 examples/sec; 2.266 sec/batch)
2018-10-15 18:14:44.034570: step 445, loss = 3.28 (14.4 examples/sec; 2.224 sec/batch)
2018-10-15 18:14:46.274549: step 446, loss = 3.09 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 18:14:48.611195: step 447, loss = 3.05 (13.7 examples/sec; 2.334 sec/batch)
2018-10-15 18:14:50.823692: step 448, loss = 2.95 (14.5 examples/sec; 2.208 sec/batch)
2018-10-15 18:14:53.062281: step 449, loss = 2.91 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 18:14:55.378500: step 450, loss = 3.17 (13.8 examples/sec; 2.312 sec/batch)
2018-10-15 18:14:57.577723: step 451, loss = 3.03 (14.6 examples/sec; 2.196 sec/batch)
2018-10-15 18:14:59.941830: step 452, loss = 3.09 (13.6 examples/sec; 2.359 sec/batch)
2018-10-15 18:15:02.227542: step 453, loss = 3.24 (14.0 examples/sec; 2.281 sec/batch)
2018-10-15 18:15:04.448265: step 454, loss = 3.33 (14.4 examples/sec; 2.216 sec/batch)
2018-10-15 18:15:06.758535: step 455, loss = 2.98 (13.9 examples/sec; 2.306 sec/batch)
2018-10-15 18:15:09.068197: step 456, loss = 3.21 (13.9 examples/sec; 2.304 sec/batch)
2018-10-15 18:15:11.328289: step 457, loss = 2.91 (14.2 examples/sec; 2.255 sec/batch)
2018-10-15 18:15:13.688339: step 458, loss = 3.13 (13.6 examples/sec; 2.355 sec/batch)
2018-10-15 18:15:15.953143: step 459, loss = 3.00 (14.2 examples/sec; 2.260 sec/batch)
2018-10-15 18:15:18.185361: step 460, loss = 3.20 (14.4 examples/sec; 2.228 sec/batch)
2018-10-15 18:15:20.471904: step 461, loss = 3.14 (14.0 examples/sec; 2.284 sec/batch)
2018-10-15 18:15:22.701945: step 462, loss = 3.09 (14.4 examples/sec; 2.225 sec/batch)
2018-10-15 18:15:25.019050: step 463, loss = 3.23 (13.8 examples/sec; 2.314 sec/batch)
2018-10-15 18:15:27.306928: step 464, loss = 2.92 (14.0 examples/sec; 2.283 sec/batch)
2018-10-15 18:15:29.564758: step 465, loss = 2.88 (14.2 examples/sec; 2.253 sec/batch)
2018-10-15 18:15:31.912952: step 466, loss = 2.91 (13.7 examples/sec; 2.343 sec/batch)
2018-10-15 18:15:34.150780: step 467, loss = 3.21 (14.3 examples/sec; 2.233 sec/batch)
2018-10-15 18:15:36.357384: step 468, loss = 2.95 (14.5 examples/sec; 2.201 sec/batch)
2018-10-15 18:15:38.684804: step 469, loss = 3.00 (13.8 examples/sec; 2.323 sec/batch)
2018-10-15 18:15:40.928540: step 470, loss = 2.76 (14.3 examples/sec; 2.240 sec/batch)
2018-10-15 18:15:43.106592: step 471, loss = 2.81 (14.7 examples/sec; 2.173 sec/batch)
2018-10-15 18:15:45.432699: step 472, loss = 3.06 (13.8 examples/sec; 2.322 sec/batch)
2018-10-15 18:15:47.666178: step 473, loss = 3.00 (14.4 examples/sec; 2.229 sec/batch)
2018-10-15 18:15:49.984527: step 474, loss = 2.71 (13.8 examples/sec; 2.313 sec/batch)
2018-10-15 18:15:52.344163: step 475, loss = 2.97 (13.6 examples/sec; 2.355 sec/batch)
2018-10-15 18:15:54.581353: step 476, loss = 3.00 (14.3 examples/sec; 2.232 sec/batch)
2018-10-15 18:15:56.868720: step 477, loss = 2.75 (14.0 examples/sec; 2.282 sec/batch)
2018-10-15 18:15:59.110351: step 478, loss = 3.23 (14.3 examples/sec; 2.236 sec/batch)
2018-10-15 18:16:01.355686: step 479, loss = 2.73 (14.3 examples/sec; 2.240 sec/batch)
2018-10-15 18:16:03.646632: step 480, loss = 3.13 (14.0 examples/sec; 2.287 sec/batch)
2018-10-15 18:16:05.898395: step 481, loss = 3.23 (14.2 examples/sec; 2.247 sec/batch)
2018-10-15 18:16:08.137940: step 482, loss = 3.01 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 18:16:10.497607: step 483, loss = 2.88 (13.6 examples/sec; 2.355 sec/batch)
2018-10-15 18:16:12.756716: step 484, loss = 3.00 (14.2 examples/sec; 2.255 sec/batch)
2018-10-15 18:16:15.005916: step 485, loss = 2.89 (14.3 examples/sec; 2.245 sec/batch)
2018-10-15 18:16:17.336480: step 486, loss = 2.96 (13.8 examples/sec; 2.326 sec/batch)
2018-10-15 18:16:19.603226: step 487, loss = 3.22 (14.1 examples/sec; 2.263 sec/batch)
2018-10-15 18:16:21.889476: step 488, loss = 2.95 (14.0 examples/sec; 2.282 sec/batch)
2018-10-15 18:16:24.168758: step 489, loss = 3.03 (14.1 examples/sec; 2.275 sec/batch)
2018-10-15 18:16:26.389871: step 490, loss = 2.92 (14.4 examples/sec; 2.216 sec/batch)
2018-10-15 18:16:28.713801: step 491, loss = 2.95 (13.8 examples/sec; 2.319 sec/batch)
2018-10-15 18:16:30.980713: step 492, loss = 3.14 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:16:33.228265: step 493, loss = 2.95 (14.3 examples/sec; 2.243 sec/batch)
2018-10-15 18:16:35.593470: step 494, loss = 3.17 (13.6 examples/sec; 2.361 sec/batch)
2018-10-15 18:16:37.864420: step 495, loss = 2.75 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:16:40.162741: step 496, loss = 2.99 (14.0 examples/sec; 2.294 sec/batch)
2018-10-15 18:16:42.400489: step 497, loss = 2.98 (14.3 examples/sec; 2.233 sec/batch)
2018-10-15 18:16:44.628951: step 498, loss = 3.01 (14.4 examples/sec; 2.224 sec/batch)
2018-10-15 18:16:46.935501: step 499, loss = 3.02 (13.9 examples/sec; 2.302 sec/batch)
2018-10-15 18:16:49.292774: step 500, loss = 3.20 (13.6 examples/sec; 2.353 sec/batch)
2018-10-15 18:16:52.006057: step 501, loss = 3.28 (14.5 examples/sec; 2.213 sec/batch)
2018-10-15 18:16:54.329491: step 502, loss = 3.03 (13.8 examples/sec; 2.319 sec/batch)
2018-10-15 18:16:56.568344: step 503, loss = 3.08 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 18:16:58.880604: step 504, loss = 3.32 (13.9 examples/sec; 2.308 sec/batch)
2018-10-15 18:17:01.201639: step 505, loss = 3.09 (13.8 examples/sec; 2.317 sec/batch)
2018-10-15 18:17:03.499180: step 506, loss = 3.08 (14.0 examples/sec; 2.294 sec/batch)
2018-10-15 18:17:05.773242: step 507, loss = 3.14 (14.1 examples/sec; 2.271 sec/batch)
2018-10-15 18:17:08.008306: step 508, loss = 2.99 (14.3 examples/sec; 2.230 sec/batch)
2018-10-15 18:17:10.307185: step 509, loss = 3.30 (13.9 examples/sec; 2.294 sec/batch)
2018-10-15 18:17:12.607858: step 510, loss = 2.92 (13.9 examples/sec; 2.295 sec/batch)
2018-10-15 18:17:14.779243: step 511, loss = 2.85 (14.8 examples/sec; 2.166 sec/batch)
2018-10-15 18:17:17.107052: step 512, loss = 2.96 (13.8 examples/sec; 2.323 sec/batch)
2018-10-15 18:17:19.353225: step 513, loss = 2.99 (14.3 examples/sec; 2.241 sec/batch)
2018-10-15 18:17:21.665328: step 514, loss = 3.20 (13.9 examples/sec; 2.307 sec/batch)
2018-10-15 18:17:23.935757: step 515, loss = 3.27 (14.1 examples/sec; 2.267 sec/batch)
2018-10-15 18:17:26.187043: step 516, loss = 3.19 (14.2 examples/sec; 2.246 sec/batch)
2018-10-15 18:17:28.508961: step 517, loss = 3.49 (13.8 examples/sec; 2.319 sec/batch)
2018-10-15 18:17:30.723869: step 518, loss = 3.04 (14.5 examples/sec; 2.212 sec/batch)
2018-10-15 18:17:32.998750: step 519, loss = 2.97 (14.1 examples/sec; 2.270 sec/batch)
2018-10-15 18:17:35.308957: step 520, loss = 3.04 (13.9 examples/sec; 2.306 sec/batch)
2018-10-15 18:17:37.599660: step 521, loss = 3.13 (14.0 examples/sec; 2.285 sec/batch)
2018-10-15 18:17:39.819821: step 522, loss = 3.13 (14.5 examples/sec; 2.214 sec/batch)
2018-10-15 18:17:42.197231: step 523, loss = 3.03 (13.5 examples/sec; 2.373 sec/batch)
2018-10-15 18:17:44.456124: step 524, loss = 3.00 (14.2 examples/sec; 2.254 sec/batch)
2018-10-15 18:17:46.729257: step 525, loss = 3.30 (14.1 examples/sec; 2.268 sec/batch)
2018-10-15 18:17:49.076284: step 526, loss = 3.10 (13.7 examples/sec; 2.342 sec/batch)
2018-10-15 18:17:51.280918: step 527, loss = 3.06 (14.5 examples/sec; 2.200 sec/batch)
2018-10-15 18:17:53.547948: step 528, loss = 3.16 (14.1 examples/sec; 2.263 sec/batch)
2018-10-15 18:17:55.849908: step 529, loss = 3.29 (13.9 examples/sec; 2.297 sec/batch)
2018-10-15 18:17:58.093338: step 530, loss = 3.01 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:18:00.390764: step 531, loss = 3.12 (14.0 examples/sec; 2.293 sec/batch)
2018-10-15 18:18:02.603766: step 532, loss = 3.06 (14.5 examples/sec; 2.208 sec/batch)
2018-10-15 18:18:04.805601: step 533, loss = 2.69 (14.6 examples/sec; 2.197 sec/batch)
2018-10-15 18:18:07.162809: step 534, loss = 3.08 (13.6 examples/sec; 2.352 sec/batch)
2018-10-15 18:18:09.406778: step 535, loss = 2.87 (14.3 examples/sec; 2.239 sec/batch)
2018-10-15 18:18:11.696919: step 536, loss = 3.17 (14.0 examples/sec; 2.285 sec/batch)
2018-10-15 18:18:13.934286: step 537, loss = 2.90 (14.3 examples/sec; 2.233 sec/batch)
2018-10-15 18:18:16.134733: step 538, loss = 3.02 (14.6 examples/sec; 2.196 sec/batch)
2018-10-15 18:18:18.516899: step 539, loss = 3.16 (13.5 examples/sec; 2.377 sec/batch)
2018-10-15 18:18:20.809442: step 540, loss = 3.02 (14.0 examples/sec; 2.288 sec/batch)
2018-10-15 18:18:23.052712: step 541, loss = 3.11 (14.3 examples/sec; 2.238 sec/batch)
2018-10-15 18:18:25.412052: step 542, loss = 3.06 (13.6 examples/sec; 2.355 sec/batch)
2018-10-15 18:18:27.648164: step 543, loss = 2.87 (14.3 examples/sec; 2.231 sec/batch)
2018-10-15 18:18:29.967477: step 544, loss = 2.97 (13.8 examples/sec; 2.315 sec/batch)
2018-10-15 18:18:32.251366: step 545, loss = 3.37 (14.0 examples/sec; 2.279 sec/batch)
2018-10-15 18:18:34.431083: step 546, loss = 2.89 (14.7 examples/sec; 2.175 sec/batch)
2018-10-15 18:18:36.709757: step 547, loss = 2.82 (14.1 examples/sec; 2.274 sec/batch)
2018-10-15 18:18:39.008048: step 548, loss = 2.85 (14.0 examples/sec; 2.294 sec/batch)
2018-10-15 18:18:41.262574: step 549, loss = 3.28 (14.2 examples/sec; 2.250 sec/batch)
2018-10-15 18:18:43.581739: step 550, loss = 2.97 (13.8 examples/sec; 2.315 sec/batch)
2018-10-15 18:18:45.793735: step 551, loss = 2.91 (14.5 examples/sec; 2.207 sec/batch)
2018-10-15 18:18:48.018936: step 552, loss = 2.89 (14.4 examples/sec; 2.220 sec/batch)
2018-10-15 18:18:50.382264: step 553, loss = 2.83 (13.6 examples/sec; 2.358 sec/batch)
2018-10-15 18:18:52.690052: step 554, loss = 3.01 (13.9 examples/sec; 2.303 sec/batch)
2018-10-15 18:18:54.946304: step 555, loss = 2.94 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:18:57.183226: step 556, loss = 3.07 (14.3 examples/sec; 2.232 sec/batch)
2018-10-15 18:18:59.425504: step 557, loss = 3.03 (14.3 examples/sec; 2.238 sec/batch)
2018-10-15 18:19:01.782085: step 558, loss = 3.01 (13.6 examples/sec; 2.352 sec/batch)
2018-10-15 18:19:04.024141: step 559, loss = 3.05 (14.3 examples/sec; 2.237 sec/batch)
2018-10-15 18:19:06.196946: step 560, loss = 2.89 (14.8 examples/sec; 2.168 sec/batch)
2018-10-15 18:19:08.520169: step 561, loss = 2.77 (13.8 examples/sec; 2.320 sec/batch)
2018-10-15 18:19:10.735841: step 562, loss = 2.99 (14.5 examples/sec; 2.210 sec/batch)
2018-10-15 18:19:13.037050: step 563, loss = 2.74 (13.9 examples/sec; 2.299 sec/batch)
2018-10-15 18:19:15.275887: step 564, loss = 2.93 (14.3 examples/sec; 2.234 sec/batch)
2018-10-15 18:19:17.444232: step 565, loss = 3.15 (14.8 examples/sec; 2.164 sec/batch)
2018-10-15 18:19:19.798130: step 566, loss = 3.17 (13.6 examples/sec; 2.349 sec/batch)
2018-10-15 18:19:22.056129: step 567, loss = 2.95 (14.2 examples/sec; 2.255 sec/batch)
2018-10-15 18:19:24.382820: step 568, loss = 3.12 (13.8 examples/sec; 2.322 sec/batch)
2018-10-15 18:19:26.698311: step 569, loss = 2.84 (13.8 examples/sec; 2.311 sec/batch)
2018-10-15 18:19:28.968911: step 570, loss = 2.91 (14.1 examples/sec; 2.268 sec/batch)
2018-10-15 18:19:31.218767: step 571, loss = 3.00 (14.3 examples/sec; 2.245 sec/batch)
2018-10-15 18:19:33.598206: step 572, loss = 3.03 (13.5 examples/sec; 2.375 sec/batch)
2018-10-15 18:19:35.783169: step 573, loss = 2.94 (14.7 examples/sec; 2.180 sec/batch)
2018-10-15 18:19:38.066079: step 574, loss = 3.10 (14.0 examples/sec; 2.278 sec/batch)
2018-10-15 18:19:40.393867: step 575, loss = 2.74 (13.8 examples/sec; 2.323 sec/batch)
2018-10-15 18:19:42.695531: step 576, loss = 3.11 (13.9 examples/sec; 2.297 sec/batch)
2018-10-15 18:19:45.009824: step 577, loss = 3.02 (13.9 examples/sec; 2.309 sec/batch)
2018-10-15 18:19:47.284919: step 578, loss = 3.00 (14.1 examples/sec; 2.270 sec/batch)
2018-10-15 18:19:49.517407: step 579, loss = 3.01 (14.4 examples/sec; 2.227 sec/batch)
2018-10-15 18:19:51.830110: step 580, loss = 2.83 (13.9 examples/sec; 2.307 sec/batch)
2018-10-15 18:19:54.131161: step 581, loss = 2.94 (13.9 examples/sec; 2.296 sec/batch)
2018-10-15 18:19:56.382424: step 582, loss = 2.95 (14.2 examples/sec; 2.246 sec/batch)
2018-10-15 18:19:58.720281: step 583, loss = 2.99 (13.7 examples/sec; 2.333 sec/batch)
2018-10-15 18:20:00.966448: step 584, loss = 2.88 (14.3 examples/sec; 2.242 sec/batch)
2018-10-15 18:20:03.125591: step 585, loss = 2.70 (14.9 examples/sec; 2.154 sec/batch)
2018-10-15 18:20:05.390383: step 586, loss = 2.96 (14.2 examples/sec; 2.259 sec/batch)
2018-10-15 18:20:07.670613: step 587, loss = 2.82 (14.1 examples/sec; 2.275 sec/batch)
2018-10-15 18:20:09.924787: step 588, loss = 3.07 (14.2 examples/sec; 2.249 sec/batch)
2018-10-15 18:20:12.283291: step 589, loss = 2.88 (13.6 examples/sec; 2.354 sec/batch)
2018-10-15 18:20:14.589858: step 590, loss = 2.63 (13.9 examples/sec; 2.301 sec/batch)
2018-10-15 18:20:16.941897: step 591, loss = 3.09 (13.6 examples/sec; 2.347 sec/batch)
2018-10-15 18:20:19.267389: step 592, loss = 2.81 (13.8 examples/sec; 2.321 sec/batch)
2018-10-15 18:20:21.507261: step 593, loss = 3.09 (14.3 examples/sec; 2.235 sec/batch)
2018-10-15 18:20:23.756953: step 594, loss = 3.13 (14.3 examples/sec; 2.244 sec/batch)
2018-10-15 18:20:26.087226: step 595, loss = 2.90 (13.8 examples/sec; 2.325 sec/batch)
2018-10-15 18:20:28.299837: step 596, loss = 2.66 (14.5 examples/sec; 2.208 sec/batch)
2018-10-15 18:20:30.560004: step 597, loss = 2.70 (14.2 examples/sec; 2.255 sec/batch)
2018-10-15 18:20:32.880029: step 598, loss = 3.43 (13.8 examples/sec; 2.315 sec/batch)
2018-10-15 18:20:35.116399: step 599, loss = 2.87 (14.3 examples/sec; 2.232 sec/batch)
2018-10-15 18:20:37.360837: step 600, loss = 3.06 (14.3 examples/sec; 2.242 sec/batch)
2018-10-15 18:20:40.087918: step 601, loss = 2.97 (14.2 examples/sec; 2.252 sec/batch)
2018-10-15 18:20:42.298324: step 602, loss = 2.88 (14.5 examples/sec; 2.206 sec/batch)
2018-10-15 18:20:44.648683: step 603, loss = 2.55 (13.6 examples/sec; 2.346 sec/batch)
2018-10-15 18:20:46.915786: step 604, loss = 3.17 (14.1 examples/sec; 2.262 sec/batch)
2018-10-15 18:20:49.125207: step 605, loss = 2.53 (14.5 examples/sec; 2.204 sec/batch)
2018-10-15 18:20:51.515491: step 606, loss = 2.78 (13.4 examples/sec; 2.386 sec/batch)
2018-10-15 18:20:53.734616: step 607, loss = 3.01 (14.5 examples/sec; 2.214 sec/batch)
2018-10-15 18:20:56.012097: step 608, loss = 2.72 (14.1 examples/sec; 2.273 sec/batch)
2018-10-15 18:20:58.377500: step 609, loss = 3.34 (13.6 examples/sec; 2.360 sec/batch)
2018-10-15 18:21:00.663822: step 610, loss = 2.92 (14.0 examples/sec; 2.283 sec/batch)
2018-10-15 18:21:02.952382: step 611, loss = 2.86 (14.0 examples/sec; 2.286 sec/batch)
2018-10-15 18:21:05.352018: step 612, loss = 2.88 (13.4 examples/sec; 2.395 sec/batch)
2018-10-15 18:21:07.592572: step 613, loss = 2.98 (14.3 examples/sec; 2.236 sec/batch)
2018-10-15 18:21:09.884433: step 614, loss = 2.97 (14.0 examples/sec; 2.287 sec/batch)
2018-10-15 18:21:12.178439: step 615, loss = 3.01 (14.0 examples/sec; 2.289 sec/batch)
2018-10-15 18:21:14.399964: step 616, loss = 3.00 (14.4 examples/sec; 2.216 sec/batch)
2018-10-15 18:21:16.678709: step 617, loss = 2.80 (14.1 examples/sec; 2.274 sec/batch)
Terminated
