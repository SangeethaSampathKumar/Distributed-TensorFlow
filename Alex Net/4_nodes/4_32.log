jayan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode cluster2 --dataset flowers --batch_num 10000
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:ps/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:ps/task:0)
Input batch shape: images: (128, 256, 256, 3) labels: (128,)
num_classes: 5
total_num_examples: 1280000
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 19:05:54.080872: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session d777eb8705442847 with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 19:06:03.099944: step 0, loss = 3.79 (19.7 examples/sec; 6.483 sec/batch)
^C2018-10-15 19:06:08.688698: W tensorflow/core/kernels/queue_base.cc:277] _0_flowers_data/parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.690639: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.692905: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.692944: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.692962: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.692977: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.693373: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.693409: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.693423: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
2018-10-15 19:06:08.693434: W tensorflow/core/kernels/queue_base.cc:277] _1_flowers_data/parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 267, in <module>
    benchmark_name=args.dataset, batch_num=args.batch_num, batch_size=args.batch_size)
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 184, in train
    _, loss_value, summary = sess.run([train_op, total_loss, summary_op])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1292, in _do_call
    return fn(*args)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1367, in _call_tf_sessionrun
    run_metadata)
KeyboardInterrupt
jayan@node-0:~/alexnet$ vi AlexNet/scripts/train.py
[1]-  Terminated              python startserver.py -deploy_mode cluster2 -job_name ps --task_index 0
[2]+  Terminated              python startserver.py -deploy_mode cluster2 -job_name worker --task_index 0
jayan@node-0:~/alexnet$ python startserver.py -deploy_mode cluster2 -job_name ps --task_index 0 & python startserver.py -deploy_mode cluster2 -job_name worker --task_index 0 &
[1] 79164
[2] 79165
jayan@node-0:~/alexnet$
jayan@node-0:~/alexnet$ 2018-10-15 19:06:29.367354: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2018-10-15 19:06:29.371120: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2223}
2018-10-15 19:06:29.371140: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> node-0:2222, 1 -> node-1:2222, 2 -> node-2:2222, 3 -> node-3:2222}
2018-10-15 19:06:29.373214: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2223
2018-10-15 19:06:29.374598: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2018-10-15 19:06:29.380316: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> node-0:2223}
2018-10-15 19:06:29.380350: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> node-1:2222, 2 -> node-2:2222, 3 -> node-3:2222}
2018-10-15 19:06:29.383905: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2222

jayan@node-0:~/alexnet$
jayan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode cluster2 --dataset flowers --batch_num 10000
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:ps/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:ps/task:0)
Input batch shape: images: (128, 256, 256, 3) labels: (128,)
num_classes: 5
total_num_examples: 1280000
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 19:07:12.922165: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 94a9947f6041ae52 with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 19:07:22.350810: step 0, loss = 3.98 (18.6 examples/sec; 6.882 sec/batch)
2018-10-15 19:07:28.690712: step 1, loss = 4.96 (25.9 examples/sec; 4.946 sec/batch)
2018-10-15 19:07:33.866633: step 2, loss = 6.66 (24.8 examples/sec; 5.170 sec/batch)
2018-10-15 19:07:38.998776: step 3, loss = 5.81 (25.0 examples/sec; 5.127 sec/batch)
2018-10-15 19:07:43.938661: step 4, loss = 8.83 (26.0 examples/sec; 4.931 sec/batch)
2018-10-15 19:07:48.782765: step 5, loss = 5.85 (26.4 examples/sec; 4.839 sec/batch)
2018-10-15 19:07:53.721508: step 6, loss = 6.06 (26.0 examples/sec; 4.931 sec/batch)
2018-10-15 19:07:58.747979: step 7, loss = 6.20 (25.5 examples/sec; 5.021 sec/batch)
2018-10-15 19:08:03.735950: step 8, loss = 7.43 (25.7 examples/sec; 4.985 sec/batch)
2018-10-15 19:08:08.974401: step 9, loss = 8.24 (24.5 examples/sec; 5.234 sec/batch)
2018-10-15 19:08:14.277668: step 10, loss = 4.68 (24.2 examples/sec; 5.297 sec/batch)
2018-10-15 19:08:19.392655: step 11, loss = 7.15 (25.0 examples/sec; 5.112 sec/batch)
2018-10-15 19:08:24.133259: step 12, loss = 6.25 (27.0 examples/sec; 4.736 sec/batch)
2018-10-15 19:08:29.236752: step 13, loss = 4.26 (25.1 examples/sec; 5.096 sec/batch)
2018-10-15 19:08:34.434452: step 14, loss = 5.23 (24.7 examples/sec; 5.192 sec/batch)
2018-10-15 19:08:39.343135: step 15, loss = 4.97 (26.1 examples/sec; 4.903 sec/batch)
2018-10-15 19:08:44.526409: step 16, loss = 4.65 (24.7 examples/sec; 5.180 sec/batch)
2018-10-15 19:08:49.701284: step 17, loss = 5.07 (24.7 examples/sec; 5.173 sec/batch)
2018-10-15 19:08:54.715798: step 18, loss = 4.19 (25.6 examples/sec; 5.009 sec/batch)
2018-10-15 19:08:59.745420: step 19, loss = 3.93 (25.5 examples/sec; 5.022 sec/batch)
2018-10-15 19:09:04.832027: step 20, loss = 4.98 (25.2 examples/sec; 5.082 sec/batch)
2018-10-15 19:09:10.348995: step 21, loss = 4.37 (23.2 examples/sec; 5.515 sec/batch)
2018-10-15 19:09:15.808604: step 22, loss = 3.62 (23.5 examples/sec; 5.457 sec/batch)
2018-10-15 19:09:21.106211: step 23, loss = 4.06 (24.2 examples/sec; 5.296 sec/batch)
2018-10-15 19:09:25.806041: step 24, loss = 4.04 (27.3 examples/sec; 4.690 sec/batch)
2018-10-15 19:09:30.326229: step 25, loss = 3.86 (28.4 examples/sec; 4.512 sec/batch)
2018-10-15 19:09:33.876356: step 26, loss = 4.51 (36.1 examples/sec; 3.545 sec/batch)
2018-10-15 19:09:37.123306: step 27, loss = 4.69 (39.5 examples/sec; 3.242 sec/batch)
2018-10-15 19:09:40.499266: step 28, loss = 3.75 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:09:43.801254: step 29, loss = 3.81 (38.8 examples/sec; 3.297 sec/batch)
2018-10-15 19:09:47.260474: step 30, loss = 4.08 (37.1 examples/sec; 3.455 sec/batch)
2018-10-15 19:09:50.562951: step 31, loss = 4.05 (38.8 examples/sec; 3.298 sec/batch)
2018-10-15 19:09:54.008869: step 32, loss = 3.99 (37.2 examples/sec; 3.441 sec/batch)
2018-10-15 19:09:57.332915: step 33, loss = 4.00 (38.6 examples/sec; 3.319 sec/batch)
2018-10-15 19:10:00.636740: step 34, loss = 3.77 (38.8 examples/sec; 3.299 sec/batch)
2018-10-15 19:10:04.040905: step 35, loss = 3.79 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:10:07.381990: step 36, loss = 3.75 (38.4 examples/sec; 3.337 sec/batch)
2018-10-15 19:10:10.760298: step 37, loss = 3.76 (37.9 examples/sec; 3.373 sec/batch)
2018-10-15 19:10:14.166148: step 38, loss = 3.69 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:10:17.508209: step 39, loss = 3.58 (38.4 examples/sec; 3.337 sec/batch)
2018-10-15 19:10:20.956388: step 40, loss = 3.65 (37.2 examples/sec; 3.443 sec/batch)
2018-10-15 19:10:24.328103: step 41, loss = 3.54 (38.0 examples/sec; 3.366 sec/batch)
2018-10-15 19:10:27.809683: step 42, loss = 3.67 (36.8 examples/sec; 3.476 sec/batch)
2018-10-15 19:10:31.132476: step 43, loss = 3.58 (38.6 examples/sec; 3.318 sec/batch)
2018-10-15 19:10:34.471447: step 44, loss = 3.56 (38.4 examples/sec; 3.334 sec/batch)
2018-10-15 19:10:37.894450: step 45, loss = 3.60 (37.5 examples/sec; 3.418 sec/batch)
2018-10-15 19:10:41.295254: step 46, loss = 3.56 (37.7 examples/sec; 3.398 sec/batch)
2018-10-15 19:10:44.723800: step 47, loss = 3.52 (37.4 examples/sec; 3.426 sec/batch)
2018-10-15 19:10:48.195431: step 48, loss = 3.73 (36.9 examples/sec; 3.469 sec/batch)
2018-10-15 19:10:51.578030: step 49, loss = 3.64 (37.9 examples/sec; 3.378 sec/batch)
2018-10-15 19:10:55.143229: step 50, loss = 3.57 (36.0 examples/sec; 3.560 sec/batch)
2018-10-15 19:10:58.427634: step 51, loss = 3.46 (39.0 examples/sec; 3.280 sec/batch)
2018-10-15 19:11:01.948473: step 52, loss = 3.52 (36.4 examples/sec; 3.516 sec/batch)
2018-10-15 19:11:05.343001: step 53, loss = 3.62 (37.8 examples/sec; 3.389 sec/batch)
2018-10-15 19:11:08.709996: step 54, loss = 3.77 (38.1 examples/sec; 3.362 sec/batch)
2018-10-15 19:11:12.072359: step 55, loss = 3.61 (38.1 examples/sec; 3.357 sec/batch)
2018-10-15 19:11:15.452270: step 56, loss = 3.48 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:11:18.750345: step 57, loss = 3.45 (38.8 examples/sec; 3.295 sec/batch)
2018-10-15 19:11:22.240863: step 58, loss = 3.57 (36.7 examples/sec; 3.488 sec/batch)
2018-10-15 19:11:25.621314: step 59, loss = 3.53 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:11:29.139772: step 60, loss = 3.60 (36.4 examples/sec; 3.513 sec/batch)
2018-10-15 19:11:32.569072: step 61, loss = 3.51 (37.4 examples/sec; 3.424 sec/batch)
2018-10-15 19:11:35.901269: step 62, loss = 3.50 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:11:39.206127: step 63, loss = 3.54 (38.8 examples/sec; 3.300 sec/batch)
2018-10-15 19:11:42.764075: step 64, loss = 3.57 (36.0 examples/sec; 3.555 sec/batch)
2018-10-15 19:11:46.184886: step 65, loss = 3.49 (37.5 examples/sec; 3.416 sec/batch)
2018-10-15 19:11:49.442523: step 66, loss = 3.49 (39.4 examples/sec; 3.253 sec/batch)
2018-10-15 19:11:52.774352: step 67, loss = 3.48 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:11:56.151564: step 68, loss = 3.47 (37.9 examples/sec; 3.374 sec/batch)
2018-10-15 19:11:59.481836: step 69, loss = 3.56 (38.5 examples/sec; 3.325 sec/batch)
2018-10-15 19:12:02.856290: step 70, loss = 3.46 (38.0 examples/sec; 3.369 sec/batch)
2018-10-15 19:12:06.346609: step 71, loss = 3.49 (36.7 examples/sec; 3.485 sec/batch)
2018-10-15 19:12:09.870656: step 72, loss = 3.51 (36.4 examples/sec; 3.519 sec/batch)
2018-10-15 19:12:13.369373: step 73, loss = 3.46 (36.6 examples/sec; 3.494 sec/batch)
2018-10-15 19:12:16.692561: step 74, loss = 3.48 (38.6 examples/sec; 3.319 sec/batch)
2018-10-15 19:12:20.024166: step 75, loss = 3.50 (38.5 examples/sec; 3.326 sec/batch)
2018-10-15 19:12:23.388563: step 76, loss = 3.51 (38.1 examples/sec; 3.359 sec/batch)
2018-10-15 19:12:26.834454: step 77, loss = 3.49 (37.2 examples/sec; 3.441 sec/batch)
2018-10-15 19:12:30.229778: step 78, loss = 3.43 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:12:33.684186: step 79, loss = 3.44 (37.1 examples/sec; 3.452 sec/batch)
2018-10-15 19:12:36.952930: step 80, loss = 3.43 (39.2 examples/sec; 3.264 sec/batch)
2018-10-15 19:12:40.496867: step 81, loss = 3.50 (36.2 examples/sec; 3.539 sec/batch)
2018-10-15 19:12:43.825372: step 82, loss = 3.57 (38.5 examples/sec; 3.326 sec/batch)
2018-10-15 19:12:47.104373: step 83, loss = 3.44 (39.1 examples/sec; 3.276 sec/batch)
2018-10-15 19:12:50.566138: step 84, loss = 3.51 (37.0 examples/sec; 3.457 sec/batch)
2018-10-15 19:12:54.071386: step 85, loss = 3.51 (36.6 examples/sec; 3.501 sec/batch)
2018-10-15 19:12:57.498791: step 86, loss = 3.50 (37.4 examples/sec; 3.423 sec/batch)
2018-10-15 19:13:00.922414: step 87, loss = 3.46 (37.4 examples/sec; 3.419 sec/batch)
2018-10-15 19:13:04.310736: step 88, loss = 3.46 (37.8 examples/sec; 3.383 sec/batch)
2018-10-15 19:13:07.612988: step 89, loss = 3.51 (38.8 examples/sec; 3.299 sec/batch)
2018-10-15 19:13:11.050236: step 90, loss = 3.54 (37.3 examples/sec; 3.433 sec/batch)
2018-10-15 19:13:14.499222: step 91, loss = 3.50 (37.2 examples/sec; 3.444 sec/batch)
2018-10-15 19:13:17.931969: step 92, loss = 3.49 (37.3 examples/sec; 3.427 sec/batch)
2018-10-15 19:13:21.302958: step 93, loss = 3.47 (38.0 examples/sec; 3.368 sec/batch)
2018-10-15 19:13:24.669909: step 94, loss = 3.47 (38.1 examples/sec; 3.362 sec/batch)
2018-10-15 19:13:28.215013: step 95, loss = 3.57 (36.2 examples/sec; 3.540 sec/batch)
2018-10-15 19:13:31.602302: step 96, loss = 3.55 (37.8 examples/sec; 3.382 sec/batch)
2018-10-15 19:13:34.898681: step 97, loss = 3.48 (38.9 examples/sec; 3.291 sec/batch)
2018-10-15 19:13:38.355270: step 98, loss = 3.44 (37.1 examples/sec; 3.452 sec/batch)
2018-10-15 19:13:41.653248: step 99, loss = 3.43 (38.9 examples/sec; 3.293 sec/batch)
2018-10-15 19:13:45.059163: step 100, loss = 3.52 (37.6 examples/sec; 3.403 sec/batch)
2018-10-15 19:13:48.938824: step 101, loss = 3.50 (37.4 examples/sec; 3.420 sec/batch)
2018-10-15 19:13:52.426049: step 102, loss = 3.59 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:13:55.857068: step 103, loss = 3.42 (37.4 examples/sec; 3.426 sec/batch)
2018-10-15 19:13:59.397988: step 104, loss = 3.47 (36.2 examples/sec; 3.536 sec/batch)
2018-10-15 19:14:02.642192: step 105, loss = 3.45 (39.5 examples/sec; 3.239 sec/batch)
2018-10-15 19:14:06.047228: step 106, loss = 3.44 (37.6 examples/sec; 3.400 sec/batch)
2018-10-15 19:14:09.620980: step 107, loss = 3.45 (35.9 examples/sec; 3.568 sec/batch)
2018-10-15 19:14:13.111302: step 108, loss = 3.53 (36.7 examples/sec; 3.485 sec/batch)
2018-10-15 19:14:16.775244: step 109, loss = 3.46 (35.0 examples/sec; 3.661 sec/batch)
2018-10-15 19:14:20.229974: step 110, loss = 3.43 (37.1 examples/sec; 3.450 sec/batch)
2018-10-15 19:14:23.547581: step 111, loss = 3.45 (38.6 examples/sec; 3.315 sec/batch)
2018-10-15 19:14:27.000029: step 112, loss = 3.40 (37.1 examples/sec; 3.450 sec/batch)
2018-10-15 19:14:30.420098: step 113, loss = 3.49 (37.5 examples/sec; 3.415 sec/batch)
2018-10-15 19:14:33.959097: step 114, loss = 3.57 (36.2 examples/sec; 3.534 sec/batch)
2018-10-15 19:14:37.335253: step 115, loss = 3.53 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:14:40.764458: step 116, loss = 3.42 (37.4 examples/sec; 3.424 sec/batch)
2018-10-15 19:14:44.068356: step 117, loss = 3.31 (38.8 examples/sec; 3.300 sec/batch)
2018-10-15 19:14:47.418746: step 118, loss = 3.44 (38.3 examples/sec; 3.346 sec/batch)
2018-10-15 19:14:50.898545: step 119, loss = 3.49 (36.8 examples/sec; 3.475 sec/batch)
2018-10-15 19:14:54.284672: step 120, loss = 3.42 (37.9 examples/sec; 3.381 sec/batch)
2018-10-15 19:14:57.605239: step 121, loss = 3.46 (38.6 examples/sec; 3.315 sec/batch)
2018-10-15 19:15:01.067163: step 122, loss = 3.40 (37.0 examples/sec; 3.456 sec/batch)
2018-10-15 19:15:04.468219: step 123, loss = 3.42 (37.7 examples/sec; 3.396 sec/batch)
2018-10-15 19:15:07.757255: step 124, loss = 3.46 (39.0 examples/sec; 3.284 sec/batch)
2018-10-15 19:15:11.039413: step 125, loss = 3.40 (39.1 examples/sec; 3.277 sec/batch)
2018-10-15 19:15:14.286357: step 126, loss = 3.35 (39.5 examples/sec; 3.242 sec/batch)
2018-10-15 19:15:17.654285: step 127, loss = 3.32 (38.1 examples/sec; 3.363 sec/batch)
2018-10-15 19:15:21.004887: step 128, loss = 3.35 (38.3 examples/sec; 3.346 sec/batch)
2018-10-15 19:15:24.301625: step 129, loss = 3.30 (38.9 examples/sec; 3.292 sec/batch)
2018-10-15 19:15:27.617988: step 130, loss = 3.36 (38.7 examples/sec; 3.311 sec/batch)
2018-10-15 19:15:31.007536: step 131, loss = 3.34 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:15:34.352887: step 132, loss = 3.41 (38.3 examples/sec; 3.341 sec/batch)
2018-10-15 19:15:37.815646: step 133, loss = 3.31 (37.0 examples/sec; 3.459 sec/batch)
2018-10-15 19:15:41.269412: step 134, loss = 3.25 (37.1 examples/sec; 3.448 sec/batch)
2018-10-15 19:15:44.641644: step 135, loss = 3.22 (38.0 examples/sec; 3.367 sec/batch)
2018-10-15 19:15:48.002170: step 136, loss = 3.24 (38.1 examples/sec; 3.356 sec/batch)
2018-10-15 19:15:51.416721: step 137, loss = 3.27 (37.5 examples/sec; 3.409 sec/batch)
2018-10-15 19:15:54.875066: step 138, loss = 3.21 (37.1 examples/sec; 3.453 sec/batch)
2018-10-15 19:15:58.128678: step 139, loss = 3.24 (39.4 examples/sec; 3.248 sec/batch)
2018-10-15 19:16:01.554450: step 140, loss = 3.21 (37.4 examples/sec; 3.421 sec/batch)
2018-10-15 19:16:05.072214: step 141, loss = 3.34 (36.4 examples/sec; 3.513 sec/batch)
2018-10-15 19:16:08.411628: step 142, loss = 3.20 (38.4 examples/sec; 3.334 sec/batch)
2018-10-15 19:16:11.928781: step 143, loss = 3.16 (36.4 examples/sec; 3.514 sec/batch)
2018-10-15 19:16:15.298034: step 144, loss = 3.23 (38.1 examples/sec; 3.361 sec/batch)
2018-10-15 19:16:18.629600: step 145, loss = 3.20 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:16:21.978369: step 146, loss = 3.24 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:16:25.421702: step 147, loss = 3.16 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:16:28.881150: step 148, loss = 3.16 (37.1 examples/sec; 3.454 sec/batch)
2018-10-15 19:16:32.471081: step 149, loss = 3.03 (35.7 examples/sec; 3.584 sec/batch)
2018-10-15 19:16:35.915316: step 150, loss = 3.13 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:16:39.455783: step 151, loss = 3.04 (36.2 examples/sec; 3.536 sec/batch)
2018-10-15 19:16:42.877743: step 152, loss = 3.14 (37.5 examples/sec; 3.417 sec/batch)
2018-10-15 19:16:46.241609: step 153, loss = 3.27 (38.1 examples/sec; 3.360 sec/batch)
2018-10-15 19:16:49.647859: step 154, loss = 3.24 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:16:53.075464: step 155, loss = 3.17 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:16:56.385069: step 156, loss = 3.12 (38.7 examples/sec; 3.304 sec/batch)
2018-10-15 19:16:59.756716: step 157, loss = 3.06 (38.0 examples/sec; 3.369 sec/batch)
2018-10-15 19:17:03.098652: step 158, loss = 3.13 (38.4 examples/sec; 3.337 sec/batch)
2018-10-15 19:17:06.576614: step 159, loss = 3.30 (36.9 examples/sec; 3.473 sec/batch)
2018-10-15 19:17:09.956382: step 160, loss = 3.10 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:17:13.503314: step 161, loss = 3.03 (36.1 examples/sec; 3.541 sec/batch)
2018-10-15 19:17:16.944558: step 162, loss = 3.10 (37.3 examples/sec; 3.436 sec/batch)
2018-10-15 19:17:20.294032: step 163, loss = 3.26 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:17:23.902392: step 164, loss = 3.27 (35.5 examples/sec; 3.605 sec/batch)
2018-10-15 19:17:27.248726: step 165, loss = 3.17 (38.3 examples/sec; 3.342 sec/batch)
2018-10-15 19:17:30.587710: step 166, loss = 3.26 (38.4 examples/sec; 3.333 sec/batch)
2018-10-15 19:17:33.932597: step 167, loss = 3.11 (38.3 examples/sec; 3.340 sec/batch)
2018-10-15 19:17:37.327180: step 168, loss = 3.15 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:17:40.645285: step 169, loss = 3.27 (38.6 examples/sec; 3.313 sec/batch)
2018-10-15 19:17:43.981160: step 170, loss = 3.16 (38.4 examples/sec; 3.331 sec/batch)
2018-10-15 19:17:47.312357: step 171, loss = 3.13 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:17:50.642162: step 172, loss = 3.26 (38.5 examples/sec; 3.325 sec/batch)
2018-10-15 19:17:53.986826: step 173, loss = 3.20 (38.3 examples/sec; 3.340 sec/batch)
2018-10-15 19:17:57.534508: step 174, loss = 3.13 (36.1 examples/sec; 3.543 sec/batch)
2018-10-15 19:18:00.933105: step 175, loss = 3.12 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:18:04.290570: step 176, loss = 3.28 (38.2 examples/sec; 3.352 sec/batch)
2018-10-15 19:18:07.687536: step 177, loss = 3.22 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:18:11.124048: step 178, loss = 3.15 (37.3 examples/sec; 3.431 sec/batch)
2018-10-15 19:18:14.498842: step 179, loss = 3.08 (38.0 examples/sec; 3.370 sec/batch)
2018-10-15 19:18:17.963962: step 180, loss = 3.18 (37.0 examples/sec; 3.460 sec/batch)
2018-10-15 19:18:21.675152: step 181, loss = 3.25 (34.5 examples/sec; 3.706 sec/batch)
2018-10-15 19:18:25.033424: step 182, loss = 3.05 (38.2 examples/sec; 3.354 sec/batch)
2018-10-15 19:18:28.503631: step 183, loss = 3.10 (36.9 examples/sec; 3.466 sec/batch)
2018-10-15 19:18:31.940309: step 184, loss = 3.04 (37.3 examples/sec; 3.432 sec/batch)
2018-10-15 19:18:35.467933: step 185, loss = 3.11 (36.3 examples/sec; 3.523 sec/batch)
2018-10-15 19:18:39.038449: step 186, loss = 3.21 (35.9 examples/sec; 3.566 sec/batch)
2018-10-15 19:18:42.698939: step 187, loss = 3.39 (35.0 examples/sec; 3.656 sec/batch)
2018-10-15 19:18:46.054032: step 188, loss = 3.14 (38.2 examples/sec; 3.350 sec/batch)
2018-10-15 19:18:49.592514: step 189, loss = 3.15 (36.2 examples/sec; 3.534 sec/batch)
2018-10-15 19:18:53.000075: step 190, loss = 3.24 (37.6 examples/sec; 3.403 sec/batch)
2018-10-15 19:18:56.454163: step 191, loss = 3.11 (37.1 examples/sec; 3.449 sec/batch)
2018-10-15 19:18:59.803718: step 192, loss = 3.07 (38.3 examples/sec; 3.345 sec/batch)
2018-10-15 19:19:03.138893: step 193, loss = 3.11 (38.4 examples/sec; 3.332 sec/batch)
2018-10-15 19:19:06.514355: step 194, loss = 3.14 (38.0 examples/sec; 3.370 sec/batch)
2018-10-15 19:19:09.982046: step 195, loss = 3.17 (37.0 examples/sec; 3.463 sec/batch)
2018-10-15 19:19:13.320816: step 196, loss = 3.26 (38.4 examples/sec; 3.334 sec/batch)
2018-10-15 19:19:16.641970: step 197, loss = 3.24 (38.6 examples/sec; 3.318 sec/batch)
2018-10-15 19:19:20.081264: step 198, loss = 3.25 (37.3 examples/sec; 3.434 sec/batch)
2018-10-15 19:19:23.479293: step 199, loss = 3.02 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:19:26.769269: step 200, loss = 3.08 (38.9 examples/sec; 3.287 sec/batch)
2018-10-15 19:19:30.627812: step 201, loss = 3.32 (37.7 examples/sec; 3.394 sec/batch)
2018-10-15 19:19:34.032748: step 202, loss = 3.19 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:19:37.428433: step 203, loss = 2.96 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:19:40.806857: step 204, loss = 3.02 (37.9 examples/sec; 3.376 sec/batch)
2018-10-15 19:19:44.204927: step 205, loss = 3.05 (37.7 examples/sec; 3.394 sec/batch)
2018-10-15 19:19:47.669731: step 206, loss = 3.21 (37.0 examples/sec; 3.460 sec/batch)
2018-10-15 19:19:51.148099: step 207, loss = 3.19 (36.8 examples/sec; 3.474 sec/batch)
2018-10-15 19:19:54.562890: step 208, loss = 3.26 (37.5 examples/sec; 3.410 sec/batch)
2018-10-15 19:19:58.044063: step 209, loss = 3.19 (36.8 examples/sec; 3.476 sec/batch)
2018-10-15 19:20:01.489174: step 210, loss = 3.00 (37.2 examples/sec; 3.442 sec/batch)
2018-10-15 19:20:04.898028: step 211, loss = 3.08 (37.6 examples/sec; 3.404 sec/batch)
2018-10-15 19:20:08.407792: step 212, loss = 3.04 (36.5 examples/sec; 3.505 sec/batch)
2018-10-15 19:20:11.741638: step 213, loss = 3.03 (38.4 examples/sec; 3.329 sec/batch)
2018-10-15 19:20:15.092828: step 214, loss = 2.96 (38.2 examples/sec; 3.346 sec/batch)
2018-10-15 19:20:18.550844: step 215, loss = 3.04 (37.1 examples/sec; 3.453 sec/batch)
2018-10-15 19:20:21.916456: step 216, loss = 3.10 (38.1 examples/sec; 3.363 sec/batch)
2018-10-15 19:20:25.262971: step 217, loss = 3.23 (38.3 examples/sec; 3.342 sec/batch)
2018-10-15 19:20:28.610968: step 218, loss = 3.16 (38.3 examples/sec; 3.343 sec/batch)
2018-10-15 19:20:32.055097: step 219, loss = 3.12 (37.2 examples/sec; 3.440 sec/batch)
2018-10-15 19:20:35.417857: step 220, loss = 2.98 (38.1 examples/sec; 3.358 sec/batch)
2018-10-15 19:20:38.808321: step 221, loss = 2.97 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:20:42.169128: step 222, loss = 2.95 (38.1 examples/sec; 3.358 sec/batch)
2018-10-15 19:20:45.498567: step 223, loss = 3.15 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:20:48.875822: step 224, loss = 3.19 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:20:52.272196: step 225, loss = 3.07 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:20:55.620839: step 226, loss = 3.03 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:20:59.107880: step 227, loss = 3.08 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:21:02.677017: step 228, loss = 3.19 (35.9 examples/sec; 3.564 sec/batch)
2018-10-15 19:21:06.060544: step 229, loss = 3.17 (37.9 examples/sec; 3.378 sec/batch)
2018-10-15 19:21:09.409370: step 230, loss = 3.18 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:21:12.931128: step 231, loss = 3.10 (36.4 examples/sec; 3.517 sec/batch)
2018-10-15 19:21:16.437923: step 232, loss = 3.12 (36.6 examples/sec; 3.502 sec/batch)
2018-10-15 19:21:19.834629: step 233, loss = 3.20 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:21:23.342380: step 234, loss = 3.15 (36.5 examples/sec; 3.503 sec/batch)
2018-10-15 19:21:26.864459: step 235, loss = 3.18 (36.4 examples/sec; 3.517 sec/batch)
2018-10-15 19:21:30.388228: step 236, loss = 3.32 (36.4 examples/sec; 3.518 sec/batch)
2018-10-15 19:21:33.749862: step 237, loss = 3.01 (38.1 examples/sec; 3.359 sec/batch)
2018-10-15 19:21:37.332086: step 238, loss = 2.98 (35.8 examples/sec; 3.577 sec/batch)
2018-10-15 19:21:40.861604: step 239, loss = 3.15 (36.3 examples/sec; 3.525 sec/batch)
2018-10-15 19:21:44.407994: step 240, loss = 3.10 (36.1 examples/sec; 3.542 sec/batch)
2018-10-15 19:21:47.755089: step 241, loss = 3.17 (38.3 examples/sec; 3.342 sec/batch)
2018-10-15 19:21:51.120606: step 242, loss = 3.07 (38.1 examples/sec; 3.361 sec/batch)
2018-10-15 19:21:54.416727: step 243, loss = 3.11 (38.9 examples/sec; 3.290 sec/batch)
2018-10-15 19:21:57.840695: step 244, loss = 2.94 (37.4 examples/sec; 3.419 sec/batch)
2018-10-15 19:22:01.323331: step 245, loss = 3.17 (36.8 examples/sec; 3.478 sec/batch)
2018-10-15 19:22:04.658464: step 246, loss = 3.04 (38.4 examples/sec; 3.331 sec/batch)
2018-10-15 19:22:07.997985: step 247, loss = 3.09 (38.4 examples/sec; 3.335 sec/batch)
2018-10-15 19:22:11.307186: step 248, loss = 3.20 (38.7 examples/sec; 3.305 sec/batch)
2018-10-15 19:22:14.810342: step 249, loss = 3.15 (36.6 examples/sec; 3.498 sec/batch)
2018-10-15 19:22:18.216132: step 250, loss = 2.94 (37.6 examples/sec; 3.400 sec/batch)
2018-10-15 19:22:21.804676: step 251, loss = 2.97 (35.7 examples/sec; 3.583 sec/batch)
2018-10-15 19:22:25.132040: step 252, loss = 3.05 (38.5 examples/sec; 3.325 sec/batch)
2018-10-15 19:22:28.603215: step 253, loss = 2.94 (36.9 examples/sec; 3.466 sec/batch)
2018-10-15 19:22:32.028505: step 254, loss = 3.05 (37.4 examples/sec; 3.420 sec/batch)
2018-10-15 19:22:35.473644: step 255, loss = 3.09 (37.2 examples/sec; 3.440 sec/batch)
2018-10-15 19:22:38.879702: step 256, loss = 2.97 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:22:42.496770: step 257, loss = 2.92 (35.4 examples/sec; 3.613 sec/batch)
2018-10-15 19:22:45.848568: step 258, loss = 3.05 (38.2 examples/sec; 3.347 sec/batch)
2018-10-15 19:22:49.456294: step 259, loss = 3.01 (35.5 examples/sec; 3.603 sec/batch)
2018-10-15 19:22:52.900129: step 260, loss = 3.00 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:22:56.326750: step 261, loss = 2.96 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:22:59.808285: step 262, loss = 3.03 (36.8 examples/sec; 3.477 sec/batch)
2018-10-15 19:23:03.225325: step 263, loss = 3.02 (37.5 examples/sec; 3.414 sec/batch)
2018-10-15 19:23:06.685728: step 264, loss = 3.05 (37.0 examples/sec; 3.456 sec/batch)
2018-10-15 19:23:10.091894: step 265, loss = 2.99 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:23:13.419552: step 266, loss = 3.05 (38.5 examples/sec; 3.323 sec/batch)
2018-10-15 19:23:16.950055: step 267, loss = 3.06 (36.3 examples/sec; 3.525 sec/batch)
2018-10-15 19:23:20.433348: step 268, loss = 3.03 (36.8 examples/sec; 3.478 sec/batch)
2018-10-15 19:23:23.871314: step 269, loss = 2.99 (37.3 examples/sec; 3.434 sec/batch)
2018-10-15 19:23:27.303468: step 270, loss = 3.03 (37.3 examples/sec; 3.427 sec/batch)
2018-10-15 19:23:30.807387: step 271, loss = 3.02 (36.6 examples/sec; 3.497 sec/batch)
2018-10-15 19:23:34.135309: step 272, loss = 3.18 (38.5 examples/sec; 3.323 sec/batch)
2018-10-15 19:23:37.549118: step 273, loss = 3.12 (37.5 examples/sec; 3.411 sec/batch)
2018-10-15 19:23:41.116369: step 274, loss = 3.17 (35.9 examples/sec; 3.561 sec/batch)
2018-10-15 19:23:44.629165: step 275, loss = 3.03 (36.5 examples/sec; 3.507 sec/batch)
2018-10-15 19:23:47.947020: step 276, loss = 3.17 (38.6 examples/sec; 3.313 sec/batch)
2018-10-15 19:23:51.311457: step 277, loss = 3.12 (38.1 examples/sec; 3.361 sec/batch)
2018-10-15 19:23:54.846203: step 278, loss = 3.09 (36.3 examples/sec; 3.530 sec/batch)
2018-10-15 19:23:58.225981: step 279, loss = 2.99 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:24:01.602217: step 280, loss = 3.11 (38.0 examples/sec; 3.371 sec/batch)
2018-10-15 19:24:04.962278: step 281, loss = 3.07 (38.2 examples/sec; 3.355 sec/batch)
2018-10-15 19:24:08.355520: step 282, loss = 3.10 (37.8 examples/sec; 3.388 sec/batch)
2018-10-15 19:24:11.782257: step 283, loss = 3.01 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:24:15.157595: step 284, loss = 3.09 (38.0 examples/sec; 3.371 sec/batch)
2018-10-15 19:24:18.675329: step 285, loss = 3.06 (36.4 examples/sec; 3.513 sec/batch)
2018-10-15 19:24:22.063676: step 286, loss = 2.90 (37.8 examples/sec; 3.383 sec/batch)
2018-10-15 19:24:25.429425: step 287, loss = 2.93 (38.1 examples/sec; 3.363 sec/batch)
2018-10-15 19:24:28.806767: step 288, loss = 3.06 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:24:32.183558: step 289, loss = 3.14 (37.9 examples/sec; 3.374 sec/batch)
2018-10-15 19:24:35.691756: step 290, loss = 3.17 (36.5 examples/sec; 3.503 sec/batch)
2018-10-15 19:24:39.009792: step 291, loss = 3.13 (38.6 examples/sec; 3.313 sec/batch)
2018-10-15 19:24:42.407347: step 292, loss = 3.03 (37.7 examples/sec; 3.395 sec/batch)
2018-10-15 19:24:45.819686: step 293, loss = 3.01 (37.6 examples/sec; 3.408 sec/batch)
2018-10-15 19:24:49.299594: step 294, loss = 2.99 (36.8 examples/sec; 3.475 sec/batch)
2018-10-15 19:24:52.683503: step 295, loss = 3.01 (37.9 examples/sec; 3.379 sec/batch)
2018-10-15 19:24:56.262525: step 296, loss = 3.15 (35.8 examples/sec; 3.574 sec/batch)
2018-10-15 19:24:59.625882: step 297, loss = 2.91 (38.1 examples/sec; 3.359 sec/batch)
2018-10-15 19:25:03.047715: step 298, loss = 2.90 (37.5 examples/sec; 3.418 sec/batch)
2018-10-15 19:25:06.663743: step 299, loss = 3.00 (35.4 examples/sec; 3.611 sec/batch)
2018-10-15 19:25:10.088841: step 300, loss = 3.01 (37.4 examples/sec; 3.420 sec/batch)
2018-10-15 19:25:13.986812: step 301, loss = 3.04 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:25:17.382248: step 302, loss = 3.02 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:25:20.837713: step 303, loss = 3.02 (37.1 examples/sec; 3.451 sec/batch)
2018-10-15 19:25:24.368141: step 304, loss = 3.10 (36.3 examples/sec; 3.526 sec/batch)
2018-10-15 19:25:27.818913: step 305, loss = 3.09 (37.1 examples/sec; 3.446 sec/batch)
2018-10-15 19:25:31.240967: step 306, loss = 2.98 (37.5 examples/sec; 3.417 sec/batch)
2018-10-15 19:25:34.648075: step 307, loss = 3.00 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:25:38.094188: step 308, loss = 3.14 (37.2 examples/sec; 3.443 sec/batch)
2018-10-15 19:25:41.582379: step 309, loss = 3.03 (36.8 examples/sec; 3.483 sec/batch)
2018-10-15 19:25:45.148724: step 310, loss = 3.05 (35.9 examples/sec; 3.562 sec/batch)
2018-10-15 19:25:48.592315: step 311, loss = 3.07 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:25:51.880030: step 312, loss = 3.01 (39.0 examples/sec; 3.283 sec/batch)
2018-10-15 19:25:55.507118: step 313, loss = 3.06 (35.3 examples/sec; 3.624 sec/batch)
2018-10-15 19:25:59.031312: step 314, loss = 3.08 (36.4 examples/sec; 3.519 sec/batch)
2018-10-15 19:26:02.593792: step 315, loss = 3.01 (36.0 examples/sec; 3.557 sec/batch)
2018-10-15 19:26:06.008047: step 316, loss = 3.00 (37.5 examples/sec; 3.410 sec/batch)
2018-10-15 19:26:09.434068: step 317, loss = 2.92 (37.4 examples/sec; 3.423 sec/batch)
2018-10-15 19:26:12.747297: step 318, loss = 2.94 (38.7 examples/sec; 3.310 sec/batch)
2018-10-15 19:26:16.185834: step 319, loss = 2.99 (37.3 examples/sec; 3.434 sec/batch)
2018-10-15 19:26:19.674788: step 320, loss = 2.99 (36.7 examples/sec; 3.484 sec/batch)
2018-10-15 19:26:23.080546: step 321, loss = 3.09 (37.6 examples/sec; 3.400 sec/batch)
2018-10-15 19:26:26.658339: step 322, loss = 3.24 (35.8 examples/sec; 3.573 sec/batch)
2018-10-15 19:26:29.965614: step 323, loss = 2.93 (38.7 examples/sec; 3.304 sec/batch)
2018-10-15 19:26:33.443196: step 324, loss = 2.99 (36.9 examples/sec; 3.472 sec/batch)
2018-10-15 19:26:36.811218: step 325, loss = 2.96 (38.1 examples/sec; 3.363 sec/batch)
2018-10-15 19:26:40.293576: step 326, loss = 3.04 (36.8 examples/sec; 3.479 sec/batch)
2018-10-15 19:26:43.680797: step 327, loss = 3.01 (37.8 examples/sec; 3.384 sec/batch)
2018-10-15 19:26:47.077493: step 328, loss = 2.98 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:26:50.581266: step 329, loss = 2.97 (36.6 examples/sec; 3.499 sec/batch)
2018-10-15 19:26:53.993342: step 330, loss = 2.99 (37.6 examples/sec; 3.407 sec/batch)
2018-10-15 19:26:57.544538: step 331, loss = 2.96 (36.1 examples/sec; 3.546 sec/batch)
2018-10-15 19:27:00.974030: step 332, loss = 3.08 (37.4 examples/sec; 3.424 sec/batch)
2018-10-15 19:27:04.438559: step 333, loss = 2.99 (37.0 examples/sec; 3.460 sec/batch)
2018-10-15 19:27:07.828740: step 334, loss = 3.16 (37.8 examples/sec; 3.387 sec/batch)
2018-10-15 19:27:11.265642: step 335, loss = 2.99 (37.3 examples/sec; 3.435 sec/batch)
2018-10-15 19:27:14.610624: step 336, loss = 3.14 (38.3 examples/sec; 3.340 sec/batch)
2018-10-15 19:27:18.115822: step 337, loss = 3.19 (36.6 examples/sec; 3.500 sec/batch)
2018-10-15 19:27:21.511609: step 338, loss = 3.21 (37.8 examples/sec; 3.391 sec/batch)
2018-10-15 19:27:24.953739: step 339, loss = 3.19 (37.2 examples/sec; 3.437 sec/batch)
2018-10-15 19:27:28.385431: step 340, loss = 3.14 (37.4 examples/sec; 3.426 sec/batch)
2018-10-15 19:27:31.841809: step 341, loss = 2.95 (37.1 examples/sec; 3.452 sec/batch)
2018-10-15 19:27:35.215240: step 342, loss = 2.90 (38.0 examples/sec; 3.368 sec/batch)
2018-10-15 19:27:38.763268: step 343, loss = 3.07 (36.1 examples/sec; 3.543 sec/batch)
2018-10-15 19:27:42.233239: step 344, loss = 3.02 (36.9 examples/sec; 3.465 sec/batch)
2018-10-15 19:27:45.670597: step 345, loss = 2.91 (37.3 examples/sec; 3.432 sec/batch)
2018-10-15 19:27:49.148143: step 346, loss = 2.90 (36.8 examples/sec; 3.475 sec/batch)
2018-10-15 19:27:52.523182: step 347, loss = 2.99 (38.0 examples/sec; 3.370 sec/batch)
2018-10-15 19:27:55.872192: step 348, loss = 3.12 (38.3 examples/sec; 3.346 sec/batch)
2018-10-15 19:27:59.198481: step 349, loss = 3.13 (38.5 examples/sec; 3.322 sec/batch)
2018-10-15 19:28:02.811529: step 350, loss = 2.86 (35.5 examples/sec; 3.608 sec/batch)
2018-10-15 19:28:06.210177: step 351, loss = 3.06 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:28:09.653451: step 352, loss = 3.01 (37.2 examples/sec; 3.437 sec/batch)
2018-10-15 19:28:13.162797: step 353, loss = 2.99 (36.5 examples/sec; 3.504 sec/batch)
2018-10-15 19:28:16.709720: step 354, loss = 2.97 (36.1 examples/sec; 3.542 sec/batch)
2018-10-15 19:28:20.055418: step 355, loss = 2.86 (38.3 examples/sec; 3.343 sec/batch)
2018-10-15 19:28:23.437012: step 356, loss = 2.86 (37.9 examples/sec; 3.376 sec/batch)
2018-10-15 19:28:26.821897: step 357, loss = 2.92 (37.9 examples/sec; 3.380 sec/batch)
2018-10-15 19:28:30.326589: step 358, loss = 2.94 (36.6 examples/sec; 3.499 sec/batch)
2018-10-15 19:28:33.721584: step 359, loss = 3.04 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:28:37.189447: step 360, loss = 2.84 (37.0 examples/sec; 3.463 sec/batch)
2018-10-15 19:28:40.604725: step 361, loss = 2.99 (37.5 examples/sec; 3.410 sec/batch)
2018-10-15 19:28:44.025011: step 362, loss = 3.01 (37.5 examples/sec; 3.415 sec/batch)
2018-10-15 19:28:47.456753: step 363, loss = 2.90 (37.4 examples/sec; 3.427 sec/batch)
2018-10-15 19:28:50.955800: step 364, loss = 2.97 (36.6 examples/sec; 3.493 sec/batch)
2018-10-15 19:28:54.363979: step 365, loss = 2.85 (37.6 examples/sec; 3.405 sec/batch)
2018-10-15 19:28:57.731811: step 366, loss = 2.85 (38.1 examples/sec; 3.362 sec/batch)
2018-10-15 19:29:01.187974: step 367, loss = 2.91 (37.1 examples/sec; 3.451 sec/batch)
2018-10-15 19:29:04.778063: step 368, loss = 2.94 (35.7 examples/sec; 3.586 sec/batch)
2018-10-15 19:29:08.167434: step 369, loss = 2.92 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:29:11.590161: step 370, loss = 3.00 (37.4 examples/sec; 3.420 sec/batch)
2018-10-15 19:29:14.978725: step 371, loss = 2.94 (37.8 examples/sec; 3.386 sec/batch)
2018-10-15 19:29:18.513530: step 372, loss = 2.99 (36.3 examples/sec; 3.530 sec/batch)
2018-10-15 19:29:21.993953: step 373, loss = 3.00 (36.8 examples/sec; 3.476 sec/batch)
2018-10-15 19:29:25.397528: step 374, loss = 2.96 (37.7 examples/sec; 3.399 sec/batch)
2018-10-15 19:29:28.906300: step 375, loss = 2.99 (36.5 examples/sec; 3.504 sec/batch)
2018-10-15 19:29:32.537155: step 376, loss = 3.01 (35.3 examples/sec; 3.627 sec/batch)
2018-10-15 19:29:35.899931: step 377, loss = 3.15 (38.1 examples/sec; 3.360 sec/batch)
2018-10-15 19:29:39.258616: step 378, loss = 2.99 (38.1 examples/sec; 3.356 sec/batch)
2018-10-15 19:29:42.847116: step 379, loss = 2.97 (35.7 examples/sec; 3.584 sec/batch)
2018-10-15 19:29:46.222597: step 380, loss = 3.03 (38.0 examples/sec; 3.371 sec/batch)
2018-10-15 19:29:49.684257: step 381, loss = 3.08 (37.0 examples/sec; 3.456 sec/batch)
2018-10-15 19:29:53.050237: step 382, loss = 3.18 (38.1 examples/sec; 3.361 sec/batch)
2018-10-15 19:29:56.673554: step 383, loss = 3.03 (35.4 examples/sec; 3.618 sec/batch)
2018-10-15 19:30:00.056417: step 384, loss = 2.97 (37.9 examples/sec; 3.378 sec/batch)
2018-10-15 19:30:03.506539: step 385, loss = 2.92 (37.1 examples/sec; 3.446 sec/batch)
2018-10-15 19:30:06.854289: step 386, loss = 2.96 (38.3 examples/sec; 3.343 sec/batch)
2018-10-15 19:30:10.242144: step 387, loss = 2.94 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:30:13.783512: step 388, loss = 2.93 (36.2 examples/sec; 3.538 sec/batch)
2018-10-15 19:30:17.280812: step 389, loss = 3.06 (36.6 examples/sec; 3.493 sec/batch)
2018-10-15 19:30:20.934924: step 390, loss = 2.93 (35.1 examples/sec; 3.649 sec/batch)
2018-10-15 19:30:24.292508: step 391, loss = 2.92 (38.2 examples/sec; 3.355 sec/batch)
2018-10-15 19:30:27.691956: step 392, loss = 2.89 (37.7 examples/sec; 3.397 sec/batch)
2018-10-15 19:30:31.038110: step 393, loss = 2.92 (38.3 examples/sec; 3.343 sec/batch)
2018-10-15 19:30:34.586189: step 394, loss = 2.85 (36.1 examples/sec; 3.545 sec/batch)
2018-10-15 19:30:38.027990: step 395, loss = 3.07 (37.2 examples/sec; 3.437 sec/batch)
2018-10-15 19:30:41.565751: step 396, loss = 3.04 (36.2 examples/sec; 3.533 sec/batch)
2018-10-15 19:30:44.987654: step 397, loss = 3.09 (37.5 examples/sec; 3.417 sec/batch)
2018-10-15 19:30:48.409028: step 398, loss = 2.97 (37.5 examples/sec; 3.416 sec/batch)
2018-10-15 19:30:51.750184: step 399, loss = 2.99 (38.3 examples/sec; 3.338 sec/batch)
2018-10-15 19:30:55.241109: step 400, loss = 3.01 (36.7 examples/sec; 3.486 sec/batch)
2018-10-15 19:30:59.194110: step 401, loss = 2.80 (37.9 examples/sec; 3.382 sec/batch)
2018-10-15 19:31:02.516487: step 402, loss = 2.97 (38.6 examples/sec; 3.320 sec/batch)
2018-10-15 19:31:05.928871: step 403, loss = 2.85 (37.6 examples/sec; 3.408 sec/batch)
2018-10-15 19:31:09.352705: step 404, loss = 2.96 (37.4 examples/sec; 3.418 sec/batch)
2018-10-15 19:31:12.880527: step 405, loss = 3.07 (36.3 examples/sec; 3.522 sec/batch)
2018-10-15 19:31:16.293798: step 406, loss = 2.86 (37.6 examples/sec; 3.408 sec/batch)
2018-10-15 19:31:19.648769: step 407, loss = 2.84 (38.2 examples/sec; 3.350 sec/batch)
2018-10-15 19:31:23.032115: step 408, loss = 2.92 (37.9 examples/sec; 3.381 sec/batch)
2018-10-15 19:31:26.617110: step 409, loss = 3.02 (35.8 examples/sec; 3.580 sec/batch)
2018-10-15 19:31:30.072980: step 410, loss = 2.99 (37.1 examples/sec; 3.453 sec/batch)
2018-10-15 19:31:33.472146: step 411, loss = 2.95 (37.7 examples/sec; 3.394 sec/batch)
2018-10-15 19:31:36.872772: step 412, loss = 3.11 (37.7 examples/sec; 3.396 sec/batch)
2018-10-15 19:31:40.270441: step 413, loss = 2.88 (37.7 examples/sec; 3.395 sec/batch)
2018-10-15 19:31:43.803436: step 414, loss = 2.96 (36.3 examples/sec; 3.528 sec/batch)
2018-10-15 19:31:47.277135: step 415, loss = 2.99 (36.9 examples/sec; 3.469 sec/batch)
2018-10-15 19:31:50.849959: step 416, loss = 2.86 (35.9 examples/sec; 3.568 sec/batch)
2018-10-15 19:31:54.320118: step 417, loss = 2.91 (36.9 examples/sec; 3.466 sec/batch)
2018-10-15 19:31:57.749261: step 418, loss = 2.93 (37.4 examples/sec; 3.424 sec/batch)
2018-10-15 19:32:01.176657: step 419, loss = 2.93 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:32:04.615450: step 420, loss = 2.88 (37.3 examples/sec; 3.434 sec/batch)
2018-10-15 19:32:07.901701: step 421, loss = 2.80 (39.0 examples/sec; 3.281 sec/batch)
2018-10-15 19:32:11.301261: step 422, loss = 2.84 (37.7 examples/sec; 3.394 sec/batch)
2018-10-15 19:32:14.651741: step 423, loss = 2.99 (38.3 examples/sec; 3.345 sec/batch)
2018-10-15 19:32:18.093728: step 424, loss = 3.05 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:32:21.510869: step 425, loss = 3.00 (37.5 examples/sec; 3.414 sec/batch)
2018-10-15 19:32:25.025218: step 426, loss = 3.01 (36.5 examples/sec; 3.509 sec/batch)
2018-10-15 19:32:28.460471: step 427, loss = 2.87 (37.3 examples/sec; 3.430 sec/batch)
2018-10-15 19:32:32.001251: step 428, loss = 2.93 (36.2 examples/sec; 3.538 sec/batch)
2018-10-15 19:32:35.452669: step 429, loss = 2.83 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:32:39.077174: step 430, loss = 2.90 (35.4 examples/sec; 3.620 sec/batch)
2018-10-15 19:32:42.526906: step 431, loss = 2.78 (37.2 examples/sec; 3.445 sec/batch)
2018-10-15 19:32:46.027108: step 432, loss = 2.87 (36.6 examples/sec; 3.497 sec/batch)
2018-10-15 19:32:49.417670: step 433, loss = 3.06 (37.8 examples/sec; 3.386 sec/batch)
2018-10-15 19:32:52.833720: step 434, loss = 2.91 (37.5 examples/sec; 3.411 sec/batch)
2018-10-15 19:32:56.189013: step 435, loss = 2.97 (38.2 examples/sec; 3.352 sec/batch)
2018-10-15 19:32:59.634692: step 436, loss = 2.90 (37.2 examples/sec; 3.441 sec/batch)
2018-10-15 19:33:03.186367: step 437, loss = 2.88 (36.1 examples/sec; 3.549 sec/batch)
2018-10-15 19:33:06.553343: step 438, loss = 3.03 (38.1 examples/sec; 3.362 sec/batch)
2018-10-15 19:33:10.012215: step 439, loss = 2.93 (37.1 examples/sec; 3.454 sec/batch)
2018-10-15 19:33:13.336069: step 440, loss = 2.93 (38.6 examples/sec; 3.319 sec/batch)
2018-10-15 19:33:16.847592: step 441, loss = 2.98 (36.5 examples/sec; 3.507 sec/batch)
2018-10-15 19:33:20.299707: step 442, loss = 3.07 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:33:23.798629: step 443, loss = 3.08 (36.6 examples/sec; 3.494 sec/batch)
2018-10-15 19:33:27.195700: step 444, loss = 2.95 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:33:30.680968: step 445, loss = 2.85 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:33:34.127439: step 446, loss = 2.89 (37.2 examples/sec; 3.444 sec/batch)
2018-10-15 19:33:37.541249: step 447, loss = 2.90 (37.5 examples/sec; 3.409 sec/batch)
2018-10-15 19:33:40.994476: step 448, loss = 2.86 (37.1 examples/sec; 3.450 sec/batch)
2018-10-15 19:33:44.428498: step 449, loss = 2.83 (37.3 examples/sec; 3.429 sec/batch)
2018-10-15 19:33:47.820075: step 450, loss = 2.95 (37.8 examples/sec; 3.387 sec/batch)
2018-10-15 19:33:51.358654: step 451, loss = 2.93 (36.2 examples/sec; 3.534 sec/batch)
2018-10-15 19:33:54.800083: step 452, loss = 2.87 (37.2 examples/sec; 3.437 sec/batch)
2018-10-15 19:33:58.284533: step 453, loss = 2.96 (36.8 examples/sec; 3.479 sec/batch)
2018-10-15 19:34:01.606089: step 454, loss = 2.93 (38.6 examples/sec; 3.318 sec/batch)
2018-10-15 19:34:05.012468: step 455, loss = 2.87 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:34:08.448622: step 456, loss = 3.04 (37.3 examples/sec; 3.431 sec/batch)
2018-10-15 19:34:11.926204: step 457, loss = 2.78 (36.9 examples/sec; 3.473 sec/batch)
2018-10-15 19:34:15.364346: step 458, loss = 2.81 (37.3 examples/sec; 3.433 sec/batch)
2018-10-15 19:34:18.775977: step 459, loss = 2.76 (37.6 examples/sec; 3.407 sec/batch)
2018-10-15 19:34:22.241044: step 460, loss = 2.74 (37.0 examples/sec; 3.460 sec/batch)
2018-10-15 19:34:25.796772: step 461, loss = 2.91 (36.0 examples/sec; 3.552 sec/batch)
2018-10-15 19:34:29.213370: step 462, loss = 2.86 (37.5 examples/sec; 3.411 sec/batch)
2018-10-15 19:34:32.717755: step 463, loss = 2.92 (36.6 examples/sec; 3.502 sec/batch)
2018-10-15 19:34:36.188892: step 464, loss = 2.66 (36.9 examples/sec; 3.468 sec/batch)
2018-10-15 19:34:39.774823: step 465, loss = 2.90 (35.7 examples/sec; 3.582 sec/batch)
2018-10-15 19:34:43.173205: step 466, loss = 2.75 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:34:46.770508: step 467, loss = 2.90 (35.6 examples/sec; 3.592 sec/batch)
2018-10-15 19:34:50.223607: step 468, loss = 2.75 (37.1 examples/sec; 3.448 sec/batch)
2018-10-15 19:34:53.582415: step 469, loss = 2.79 (38.2 examples/sec; 3.355 sec/batch)
2018-10-15 19:34:57.043610: step 470, loss = 2.82 (37.0 examples/sec; 3.458 sec/batch)
2018-10-15 19:35:00.439256: step 471, loss = 2.81 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:35:03.956320: step 472, loss = 2.89 (36.4 examples/sec; 3.515 sec/batch)
2018-10-15 19:35:07.442912: step 473, loss = 2.86 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:35:11.098595: step 474, loss = 2.81 (35.1 examples/sec; 3.651 sec/batch)
2018-10-15 19:35:14.497802: step 475, loss = 2.81 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:35:17.930551: step 476, loss = 2.95 (37.3 examples/sec; 3.427 sec/batch)
2018-10-15 19:35:21.331085: step 477, loss = 2.86 (37.7 examples/sec; 3.396 sec/batch)
2018-10-15 19:35:24.914545: step 478, loss = 2.91 (35.8 examples/sec; 3.578 sec/batch)
2018-10-15 19:35:28.316999: step 479, loss = 2.97 (37.7 examples/sec; 3.397 sec/batch)
2018-10-15 19:35:31.941542: step 480, loss = 2.94 (35.4 examples/sec; 3.620 sec/batch)
2018-10-15 19:35:35.360589: step 481, loss = 2.90 (37.5 examples/sec; 3.416 sec/batch)
2018-10-15 19:35:38.939035: step 482, loss = 2.83 (35.8 examples/sec; 3.576 sec/batch)
2018-10-15 19:35:42.337366: step 483, loss = 2.89 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:35:45.665286: step 484, loss = 2.93 (38.5 examples/sec; 3.323 sec/batch)
2018-10-15 19:35:49.209944: step 485, loss = 3.04 (36.2 examples/sec; 3.540 sec/batch)
2018-10-15 19:35:52.663987: step 486, loss = 2.93 (37.1 examples/sec; 3.449 sec/batch)
2018-10-15 19:35:56.042000: step 487, loss = 3.04 (37.9 examples/sec; 3.374 sec/batch)
2018-10-15 19:35:59.516992: step 488, loss = 3.02 (36.9 examples/sec; 3.472 sec/batch)
2018-10-15 19:36:02.870847: step 489, loss = 2.86 (38.2 examples/sec; 3.349 sec/batch)
2018-10-15 19:36:06.351877: step 490, loss = 2.79 (36.8 examples/sec; 3.476 sec/batch)
2018-10-15 19:36:09.800891: step 491, loss = 2.87 (37.1 examples/sec; 3.446 sec/batch)
2018-10-15 19:36:13.311315: step 492, loss = 2.82 (36.5 examples/sec; 3.508 sec/batch)
2018-10-15 19:36:16.716303: step 493, loss = 2.93 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:36:20.154412: step 494, loss = 2.82 (37.3 examples/sec; 3.435 sec/batch)
2018-10-15 19:36:23.603788: step 495, loss = 2.84 (37.2 examples/sec; 3.445 sec/batch)
2018-10-15 19:36:27.055824: step 496, loss = 2.84 (37.1 examples/sec; 3.446 sec/batch)
2018-10-15 19:36:30.450502: step 497, loss = 2.88 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:36:33.799114: step 498, loss = 2.91 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:36:37.175478: step 499, loss = 2.93 (38.0 examples/sec; 3.371 sec/batch)
2018-10-15 19:36:40.593338: step 500, loss = 3.00 (37.5 examples/sec; 3.413 sec/batch)
2018-10-15 19:36:44.519679: step 501, loss = 2.87 (37.2 examples/sec; 3.444 sec/batch)
2018-10-15 19:36:47.998623: step 502, loss = 3.00 (36.9 examples/sec; 3.473 sec/batch)
2018-10-15 19:36:51.650782: step 503, loss = 3.06 (35.1 examples/sec; 3.647 sec/batch)
2018-10-15 19:36:55.047658: step 504, loss = 2.84 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:36:58.495081: step 505, loss = 2.77 (37.2 examples/sec; 3.443 sec/batch)
2018-10-15 19:37:01.947056: step 506, loss = 2.91 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:37:05.432174: step 507, loss = 2.90 (36.8 examples/sec; 3.480 sec/batch)
2018-10-15 19:37:08.833884: step 508, loss = 2.75 (37.7 examples/sec; 3.398 sec/batch)
2018-10-15 19:37:12.204952: step 509, loss = 2.77 (38.0 examples/sec; 3.368 sec/batch)
2018-10-15 19:37:15.687941: step 510, loss = 2.94 (36.8 examples/sec; 3.478 sec/batch)
2018-10-15 19:37:19.069287: step 511, loss = 2.95 (37.9 examples/sec; 3.377 sec/batch)
2018-10-15 19:37:22.505388: step 512, loss = 2.93 (37.3 examples/sec; 3.431 sec/batch)
2018-10-15 19:37:25.926381: step 513, loss = 2.90 (37.4 examples/sec; 3.418 sec/batch)
2018-10-15 19:37:29.356542: step 514, loss = 2.98 (37.4 examples/sec; 3.425 sec/batch)
2018-10-15 19:37:32.767607: step 515, loss = 2.80 (37.6 examples/sec; 3.406 sec/batch)
2018-10-15 19:37:36.200183: step 516, loss = 3.28 (37.3 examples/sec; 3.428 sec/batch)
2018-10-15 19:37:39.616088: step 517, loss = 2.96 (37.5 examples/sec; 3.411 sec/batch)
2018-10-15 19:37:43.042850: step 518, loss = 2.87 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:37:46.454112: step 519, loss = 2.94 (37.6 examples/sec; 3.406 sec/batch)
2018-10-15 19:37:49.942755: step 520, loss = 2.85 (36.7 examples/sec; 3.484 sec/batch)
2018-10-15 19:37:53.361405: step 521, loss = 2.95 (37.5 examples/sec; 3.416 sec/batch)
2018-10-15 19:37:56.733970: step 522, loss = 2.92 (38.0 examples/sec; 3.367 sec/batch)
2018-10-15 19:38:00.245559: step 523, loss = 2.77 (36.5 examples/sec; 3.506 sec/batch)
2018-10-15 19:38:03.684195: step 524, loss = 2.89 (37.3 examples/sec; 3.436 sec/batch)
2018-10-15 19:38:07.150767: step 525, loss = 2.85 (37.0 examples/sec; 3.462 sec/batch)
2018-10-15 19:38:10.509991: step 526, loss = 2.90 (38.2 examples/sec; 3.354 sec/batch)
2018-10-15 19:38:14.001913: step 527, loss = 2.85 (36.7 examples/sec; 3.487 sec/batch)
2018-10-15 19:38:17.523200: step 528, loss = 3.02 (36.4 examples/sec; 3.516 sec/batch)
2018-10-15 19:38:21.022981: step 529, loss = 2.94 (36.6 examples/sec; 3.495 sec/batch)
2018-10-15 19:38:24.414726: step 530, loss = 2.84 (37.8 examples/sec; 3.387 sec/batch)
2018-10-15 19:38:27.804977: step 531, loss = 3.02 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:38:31.327762: step 532, loss = 2.81 (36.4 examples/sec; 3.518 sec/batch)
2018-10-15 19:38:34.703889: step 533, loss = 2.81 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:38:38.183199: step 534, loss = 2.81 (36.8 examples/sec; 3.475 sec/batch)
2018-10-15 19:38:41.678136: step 535, loss = 2.80 (36.7 examples/sec; 3.490 sec/batch)
2018-10-15 19:38:45.094445: step 536, loss = 2.88 (37.5 examples/sec; 3.413 sec/batch)
2018-10-15 19:38:48.644497: step 537, loss = 2.89 (36.1 examples/sec; 3.547 sec/batch)
2018-10-15 19:38:52.034992: step 538, loss = 2.89 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:38:55.505193: step 539, loss = 2.94 (36.9 examples/sec; 3.467 sec/batch)
2018-10-15 19:38:58.988265: step 540, loss = 2.92 (36.8 examples/sec; 3.478 sec/batch)
2018-10-15 19:39:02.499193: step 541, loss = 2.97 (36.5 examples/sec; 3.505 sec/batch)
2018-10-15 19:39:05.876134: step 542, loss = 2.94 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:39:09.327938: step 543, loss = 3.01 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:39:12.818450: step 544, loss = 2.94 (36.7 examples/sec; 3.487 sec/batch)
2018-10-15 19:39:16.288249: step 545, loss = 2.98 (36.9 examples/sec; 3.465 sec/batch)
2018-10-15 19:39:19.775263: step 546, loss = 2.97 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:39:23.154663: step 547, loss = 2.87 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:39:26.519328: step 548, loss = 2.82 (38.1 examples/sec; 3.360 sec/batch)
2018-10-15 19:39:30.102692: step 549, loss = 2.78 (35.7 examples/sec; 3.581 sec/batch)
2018-10-15 19:39:33.515103: step 550, loss = 2.99 (37.6 examples/sec; 3.407 sec/batch)
2018-10-15 19:39:37.071386: step 551, loss = 2.88 (36.0 examples/sec; 3.552 sec/batch)
2018-10-15 19:39:40.485132: step 552, loss = 2.90 (37.5 examples/sec; 3.409 sec/batch)
2018-10-15 19:39:43.911293: step 553, loss = 2.79 (37.4 examples/sec; 3.421 sec/batch)
2018-10-15 19:39:47.459079: step 554, loss = 2.83 (36.1 examples/sec; 3.543 sec/batch)
2018-10-15 19:39:50.862729: step 555, loss = 2.86 (37.7 examples/sec; 3.398 sec/batch)
2018-10-15 19:39:54.206715: step 556, loss = 2.92 (38.3 examples/sec; 3.339 sec/batch)
2018-10-15 19:39:57.763372: step 557, loss = 2.93 (36.0 examples/sec; 3.552 sec/batch)
2018-10-15 19:40:01.219078: step 558, loss = 2.89 (37.1 examples/sec; 3.453 sec/batch)
2018-10-15 19:40:04.711484: step 559, loss = 2.63 (36.7 examples/sec; 3.487 sec/batch)
2018-10-15 19:40:08.128776: step 560, loss = 2.86 (37.5 examples/sec; 3.412 sec/batch)
2018-10-15 19:40:11.481044: step 561, loss = 2.87 (38.2 examples/sec; 3.347 sec/batch)
2018-10-15 19:40:14.876097: step 562, loss = 2.75 (37.8 examples/sec; 3.391 sec/batch)
2018-10-15 19:40:18.362076: step 563, loss = 2.95 (36.8 examples/sec; 3.483 sec/batch)
2018-10-15 19:40:21.814184: step 564, loss = 2.76 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:40:25.215634: step 565, loss = 2.71 (37.7 examples/sec; 3.397 sec/batch)
2018-10-15 19:40:28.622672: step 566, loss = 2.70 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:40:32.071033: step 567, loss = 2.77 (37.2 examples/sec; 3.444 sec/batch)
2018-10-15 19:40:35.564326: step 568, loss = 2.67 (36.7 examples/sec; 3.488 sec/batch)
2018-10-15 19:40:39.032938: step 569, loss = 2.77 (37.0 examples/sec; 3.463 sec/batch)
2018-10-15 19:40:42.426169: step 570, loss = 2.72 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:40:45.898553: step 571, loss = 2.66 (36.9 examples/sec; 3.467 sec/batch)
2018-10-15 19:40:49.403182: step 572, loss = 2.75 (36.6 examples/sec; 3.499 sec/batch)
2018-10-15 19:40:52.773963: step 573, loss = 2.73 (38.0 examples/sec; 3.366 sec/batch)
2018-10-15 19:40:56.211059: step 574, loss = 2.80 (37.3 examples/sec; 3.433 sec/batch)
2018-10-15 19:40:59.607351: step 575, loss = 2.78 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:41:03.128651: step 576, loss = 2.66 (36.4 examples/sec; 3.516 sec/batch)
2018-10-15 19:41:06.556749: step 577, loss = 2.86 (37.4 examples/sec; 3.423 sec/batch)
2018-10-15 19:41:09.982297: step 578, loss = 2.90 (37.4 examples/sec; 3.421 sec/batch)
2018-10-15 19:41:13.448427: step 579, loss = 2.85 (37.0 examples/sec; 3.461 sec/batch)
2018-10-15 19:41:16.794721: step 580, loss = 2.77 (38.3 examples/sec; 3.342 sec/batch)
2018-10-15 19:41:20.128396: step 581, loss = 2.95 (38.5 examples/sec; 3.329 sec/batch)
2018-10-15 19:41:23.566550: step 582, loss = 2.68 (37.3 examples/sec; 3.433 sec/batch)
2018-10-15 19:41:27.066593: step 583, loss = 2.86 (36.6 examples/sec; 3.495 sec/batch)
2018-10-15 19:41:30.471459: step 584, loss = 2.78 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:41:34.039120: step 585, loss = 2.76 (35.9 examples/sec; 3.563 sec/batch)
2018-10-15 19:41:37.517558: step 586, loss = 2.81 (36.9 examples/sec; 3.474 sec/batch)
2018-10-15 19:41:41.019279: step 587, loss = 2.84 (36.6 examples/sec; 3.497 sec/batch)
2018-10-15 19:41:44.443085: step 588, loss = 2.78 (37.4 examples/sec; 3.419 sec/batch)
2018-10-15 19:41:47.878211: step 589, loss = 3.03 (37.3 examples/sec; 3.430 sec/batch)
2018-10-15 19:41:51.378990: step 590, loss = 2.79 (36.6 examples/sec; 3.498 sec/batch)
2018-10-15 19:41:54.917349: step 591, loss = 3.02 (36.2 examples/sec; 3.533 sec/batch)
2018-10-15 19:41:58.313676: step 592, loss = 2.71 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:42:01.630020: step 593, loss = 2.86 (38.7 examples/sec; 3.311 sec/batch)
2018-10-15 19:42:05.017142: step 594, loss = 2.84 (37.8 examples/sec; 3.382 sec/batch)
2018-10-15 19:42:08.404584: step 595, loss = 2.79 (37.8 examples/sec; 3.383 sec/batch)
2018-10-15 19:42:11.876447: step 596, loss = 2.84 (36.9 examples/sec; 3.469 sec/batch)
2018-10-15 19:42:15.261147: step 597, loss = 2.77 (37.9 examples/sec; 3.379 sec/batch)
2018-10-15 19:42:18.625297: step 598, loss = 2.92 (38.1 examples/sec; 3.360 sec/batch)
2018-10-15 19:42:22.286709: step 599, loss = 2.72 (35.0 examples/sec; 3.659 sec/batch)
2018-10-15 19:42:25.681828: step 600, loss = 2.80 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:42:29.725181: step 601, loss = 2.83 (36.2 examples/sec; 3.539 sec/batch)
2018-10-15 19:42:33.096096: step 602, loss = 2.74 (38.0 examples/sec; 3.368 sec/batch)
2018-10-15 19:42:36.415099: step 603, loss = 2.77 (38.6 examples/sec; 3.314 sec/batch)
2018-10-15 19:42:39.809710: step 604, loss = 2.78 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:42:43.255704: step 605, loss = 3.03 (37.2 examples/sec; 3.441 sec/batch)
2018-10-15 19:42:46.755388: step 606, loss = 2.79 (36.6 examples/sec; 3.495 sec/batch)
2018-10-15 19:42:50.326843: step 607, loss = 2.92 (35.9 examples/sec; 3.567 sec/batch)
2018-10-15 19:42:53.750321: step 608, loss = 2.65 (37.4 examples/sec; 3.419 sec/batch)
Terminated
