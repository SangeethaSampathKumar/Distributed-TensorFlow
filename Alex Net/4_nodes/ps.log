yan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode cluster --dataset flowers --batch_num 2048
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:ps/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:ps/task:0)
Input batch shape: images: (128, 256, 256, 3) labels: (128,)
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 10:26:58.868665: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 4887009db457261b with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 10:27:06.044773: step 0, loss = 3.88 (28.1 examples/sec; 4.551 sec/batch)
2018-10-15 10:27:09.777223: step 1, loss = 5.56 (48.3 examples/sec; 2.650 sec/batch)
2018-10-15 10:27:12.384027: step 2, loss = 6.56 (49.3 examples/sec; 2.599 sec/batch)
2018-10-15 10:27:14.957472: step 3, loss = 7.16 (49.9 examples/sec; 2.563 sec/batch)
2018-10-15 10:27:17.505996: step 4, loss = 5.61 (50.4 examples/sec; 2.542 sec/batch)
2018-10-15 10:27:20.101649: step 5, loss = 5.78 (49.6 examples/sec; 2.581 sec/batch)
2018-10-15 10:27:22.617013: step 6, loss = 5.64 (51.0 examples/sec; 2.511 sec/batch)
2018-10-15 10:27:25.196318: step 7, loss = 5.56 (49.8 examples/sec; 2.571 sec/batch)
2018-10-15 10:27:28.464102: step 8, loss = 5.62 (39.3 examples/sec; 3.259 sec/batch)
2018-10-15 10:27:31.045133: step 9, loss = 6.46 (49.8 examples/sec; 2.573 sec/batch)
2018-10-15 10:27:33.560678: step 10, loss = 7.15 (51.0 examples/sec; 2.508 sec/batch)
2018-10-15 10:27:36.128789: step 11, loss = 6.74 (49.9 examples/sec; 2.563 sec/batch)
2018-10-15 10:27:38.698903: step 12, loss = 3.78 (49.9 examples/sec; 2.565 sec/batch)
2018-10-15 10:27:41.242501: step 13, loss = 5.05 (50.5 examples/sec; 2.534 sec/batch)
2018-10-15 10:27:43.636523: step 14, loss = 4.11 (53.7 examples/sec; 2.386 sec/batch)
2018-10-15 10:27:46.217783: step 15, loss = 5.36 (49.8 examples/sec; 2.572 sec/batch)
2018-10-15 10:27:49.461955: step 16, loss = 5.58 (39.6 examples/sec; 3.236 sec/batch)
2018-10-15 10:27:52.053734: step 17, loss = 4.56 (49.5 examples/sec; 2.584 sec/batch)
2018-10-15 10:27:54.691447: step 18, loss = 3.66 (48.7 examples/sec; 2.629 sec/batch)
2018-10-15 10:27:57.262556: step 19, loss = 4.19 (49.9 examples/sec; 2.564 sec/batch)
2018-10-15 10:27:59.819916: step 20, loss = 4.22 (50.2 examples/sec; 2.549 sec/batch)
2018-10-15 10:28:02.377328: step 21, loss = 3.79 (50.2 examples/sec; 2.550 sec/batch)
2018-10-15 10:28:04.921706: step 22, loss = 3.86 (50.4 examples/sec; 2.540 sec/batch)
2018-10-15 10:28:07.452343: step 23, loss = 3.92 (50.7 examples/sec; 2.526 sec/batch)
2018-10-15 10:28:09.991396: step 24, loss = 4.17 (50.6 examples/sec; 2.530 sec/batch)
2018-10-15 10:28:12.580597: step 25, loss = 4.27 (49.6 examples/sec; 2.582 sec/batch)
2018-10-15 10:28:15.195323: step 26, loss = 4.28 (49.1 examples/sec; 2.607 sec/batch)
2018-10-15 10:28:17.808476: step 27, loss = 4.55 (49.1 examples/sec; 2.606 sec/batch)
2018-10-15 10:28:20.369634: step 28, loss = 3.81 (50.1 examples/sec; 2.556 sec/batch)
2018-10-15 10:28:22.939350: step 29, loss = 3.98 (50.0 examples/sec; 2.561 sec/batch)
2018-10-15 10:28:25.487174: step 30, loss = 4.12 (50.3 examples/sec; 2.543 sec/batch)
2018-10-15 10:28:28.079867: step 31, loss = 3.70 (49.5 examples/sec; 2.586 sec/batch)
2018-10-15 10:28:30.587671: step 32, loss = 3.69 (51.2 examples/sec; 2.498 sec/batch)
2018-10-15 10:28:33.135502: step 33, loss = 3.81 (50.4 examples/sec; 2.541 sec/batch)
2018-10-15 10:28:35.779697: step 34, loss = 3.80 (48.5 examples/sec; 2.637 sec/batch)
2018-10-15 10:28:38.298681: step 35, loss = 3.64 (51.0 examples/sec; 2.510 sec/batch)
2018-10-15 10:28:40.833913: step 36, loss = 3.57 (50.7 examples/sec; 2.526 sec/batch)
2018-10-15 10:28:43.308139: step 37, loss = 3.69 (52.0 examples/sec; 2.463 sec/batch)
2018-10-15 10:28:45.857653: step 38, loss = 3.75 (50.3 examples/sec; 2.544 sec/batch)
2018-10-15 10:28:48.469998: step 39, loss = 3.64 (49.2 examples/sec; 2.602 sec/batch)
2018-10-15 10:28:51.112678: step 40, loss = 3.60 (48.6 examples/sec; 2.633 sec/batch)
2018-10-15 10:28:53.689162: step 41, loss = 3.50 (49.8 examples/sec; 2.570 sec/batch)
2018-10-15 10:28:56.163635: step 42, loss = 4.01 (51.8 examples/sec; 2.471 sec/batch)
2018-10-15 10:28:58.499754: step 43, loss = 3.53 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 10:29:00.756930: step 44, loss = 3.61 (56.9 examples/sec; 2.249 sec/batch)
2018-10-15 10:29:03.000668: step 45, loss = 3.64 (57.3 examples/sec; 2.236 sec/batch)
2018-10-15 10:29:05.266832: step 46, loss = 3.63 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 10:29:07.551617: step 47, loss = 3.72 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 10:29:09.785141: step 48, loss = 4.08 (57.5 examples/sec; 2.225 sec/batch)
2018-10-15 10:29:12.033114: step 49, loss = 3.85 (57.1 examples/sec; 2.243 sec/batch)
2018-10-15 10:29:14.276668: step 50, loss = 3.52 (57.2 examples/sec; 2.239 sec/batch)
2018-10-15 10:29:16.524288: step 51, loss = 3.49 (57.1 examples/sec; 2.240 sec/batch)
2018-10-15 10:29:18.763341: step 52, loss = 3.71 (57.4 examples/sec; 2.231 sec/batch)
2018-10-15 10:29:20.971349: step 53, loss = 3.86 (58.2 examples/sec; 2.200 sec/batch)
2018-10-15 10:29:23.195567: step 54, loss = 3.79 (57.8 examples/sec; 2.216 sec/batch)
2018-10-15 10:29:25.496289: step 55, loss = 3.52 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 10:29:27.803592: step 56, loss = 3.80 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:29:30.052565: step 57, loss = 3.52 (57.0 examples/sec; 2.244 sec/batch)
2018-10-15 10:29:32.287885: step 58, loss = 3.51 (57.4 examples/sec; 2.231 sec/batch)
2018-10-15 10:29:34.539322: step 59, loss = 3.65 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 10:29:36.773385: step 60, loss = 3.43 (57.5 examples/sec; 2.226 sec/batch)
2018-10-15 10:29:39.038599: step 61, loss = 3.61 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 10:29:41.248491: step 62, loss = 3.90 (58.1 examples/sec; 2.201 sec/batch)
2018-10-15 10:29:43.471036: step 63, loss = 3.76 (57.7 examples/sec; 2.218 sec/batch)
2018-10-15 10:29:45.753662: step 64, loss = 3.63 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 10:29:48.008378: step 65, loss = 3.54 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 10:29:50.217102: step 66, loss = 3.56 (58.1 examples/sec; 2.202 sec/batch)
2018-10-15 10:29:52.508390: step 67, loss = 3.65 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:29:54.769292: step 68, loss = 3.57 (56.8 examples/sec; 2.253 sec/batch)
2018-10-15 10:29:56.996048: step 69, loss = 3.54 (57.7 examples/sec; 2.219 sec/batch)
2018-10-15 10:29:59.304761: step 70, loss = 3.55 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:30:01.542748: step 71, loss = 3.69 (57.4 examples/sec; 2.230 sec/batch)
2018-10-15 10:30:03.867353: step 72, loss = 3.38 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 10:30:06.143794: step 73, loss = 3.52 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 10:30:08.405526: step 74, loss = 3.43 (56.8 examples/sec; 2.255 sec/batch)
2018-10-15 10:30:10.674877: step 75, loss = 3.40 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 10:30:12.930559: step 76, loss = 3.47 (57.0 examples/sec; 2.247 sec/batch)
2018-10-15 10:30:15.177475: step 77, loss = 3.40 (57.2 examples/sec; 2.239 sec/batch)
2018-10-15 10:30:17.448876: step 78, loss = 3.41 (56.5 examples/sec; 2.266 sec/batch)
2018-10-15 10:30:19.663437: step 79, loss = 3.49 (58.0 examples/sec; 2.206 sec/batch)
2018-10-15 10:30:21.899739: step 80, loss = 3.39 (57.5 examples/sec; 2.228 sec/batch)
2018-10-15 10:30:24.201880: step 81, loss = 3.35 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 10:30:26.475918: step 82, loss = 3.36 (56.5 examples/sec; 2.266 sec/batch)
2018-10-15 10:30:28.757447: step 83, loss = 3.87 (56.3 examples/sec; 2.276 sec/batch)
2018-10-15 10:30:31.063418: step 84, loss = 3.56 (55.7 examples/sec; 2.300 sec/batch)
2018-10-15 10:30:33.416013: step 85, loss = 3.48 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 10:30:35.735993: step 86, loss = 3.59 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 10:30:38.027306: step 87, loss = 3.44 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:30:40.332677: step 88, loss = 3.40 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:30:42.629097: step 89, loss = 3.40 (55.9 examples/sec; 2.288 sec/batch)
2018-10-15 10:30:44.903518: step 90, loss = 3.49 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 10:30:47.205538: step 91, loss = 3.28 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:30:49.494359: step 92, loss = 3.41 (56.0 examples/sec; 2.284 sec/batch)
2018-10-15 10:30:51.762020: step 93, loss = 3.41 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 10:30:54.133679: step 94, loss = 3.37 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 10:30:56.373820: step 95, loss = 3.51 (57.4 examples/sec; 2.232 sec/batch)
2018-10-15 10:30:58.649112: step 96, loss = 3.29 (56.5 examples/sec; 2.267 sec/batch)
2018-10-15 10:31:00.875405: step 97, loss = 3.42 (57.7 examples/sec; 2.218 sec/batch)
2018-10-15 10:31:03.179208: step 98, loss = 3.37 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 10:31:05.474535: step 99, loss = 3.36 (55.9 examples/sec; 2.290 sec/batch)
2018-10-15 10:31:07.754635: step 100, loss = 3.57 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 10:31:10.617558: step 101, loss = 3.62 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 10:31:12.922963: step 102, loss = 3.51 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 10:31:15.257914: step 103, loss = 3.18 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 10:31:17.557238: step 104, loss = 3.44 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 10:31:19.873268: step 105, loss = 3.42 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 10:31:22.176755: step 106, loss = 3.38 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 10:31:24.482977: step 107, loss = 3.35 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:31:26.877756: step 108, loss = 3.45 (53.6 examples/sec; 2.388 sec/batch)
2018-10-15 10:31:29.281263: step 109, loss = 3.37 (53.4 examples/sec; 2.395 sec/batch)
2018-10-15 10:31:31.486219: step 110, loss = 3.23 (58.3 examples/sec; 2.197 sec/batch)
2018-10-15 10:31:33.830772: step 111, loss = 3.30 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 10:31:36.204746: step 112, loss = 3.55 (54.1 examples/sec; 2.366 sec/batch)
2018-10-15 10:31:38.512580: step 113, loss = 3.28 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:31:40.857968: step 114, loss = 3.33 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 10:31:43.223753: step 115, loss = 3.36 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 10:31:45.563547: step 116, loss = 3.35 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 10:31:47.884737: step 117, loss = 3.21 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 10:31:50.136281: step 118, loss = 3.37 (57.1 examples/sec; 2.244 sec/batch)
2018-10-15 10:31:52.452812: step 119, loss = 3.44 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 10:31:54.828865: step 120, loss = 3.60 (54.1 examples/sec; 2.368 sec/batch)
2018-10-15 10:31:57.101530: step 121, loss = 3.24 (56.5 examples/sec; 2.266 sec/batch)
2018-10-15 10:31:59.449841: step 122, loss = 3.28 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 10:32:01.733845: step 123, loss = 3.23 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 10:32:04.089299: step 124, loss = 3.32 (54.5 examples/sec; 2.347 sec/batch)
2018-10-15 10:32:06.357971: step 125, loss = 3.18 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 10:32:08.748106: step 126, loss = 3.50 (53.8 examples/sec; 2.381 sec/batch)
2018-10-15 10:32:11.011809: step 127, loss = 3.56 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 10:32:13.270525: step 128, loss = 3.33 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 10:32:15.544272: step 129, loss = 3.36 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 10:32:17.925057: step 130, loss = 3.46 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 10:32:20.262198: step 131, loss = 3.54 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 10:32:22.599730: step 132, loss = 3.24 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 10:32:24.905285: step 133, loss = 3.10 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 10:32:27.239538: step 134, loss = 3.21 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 10:32:29.624192: step 135, loss = 3.05 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 10:32:31.967102: step 136, loss = 3.13 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 10:32:34.399893: step 137, loss = 3.19 (52.8 examples/sec; 2.425 sec/batch)
2018-10-15 10:32:36.737423: step 138, loss = 3.20 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 10:32:39.044977: step 139, loss = 3.25 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:32:41.289024: step 140, loss = 3.29 (57.3 examples/sec; 2.235 sec/batch)
2018-10-15 10:32:43.693056: step 141, loss = 3.29 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 10:32:46.097388: step 142, loss = 3.34 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 10:32:48.453977: step 143, loss = 3.19 (54.4 examples/sec; 2.351 sec/batch)
2018-10-15 10:32:50.786464: step 144, loss = 3.11 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 10:32:53.141647: step 145, loss = 3.44 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 10:32:55.498469: step 146, loss = 3.36 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 10:32:57.833078: step 147, loss = 3.11 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 10:33:00.241238: step 148, loss = 3.09 (53.3 examples/sec; 2.401 sec/batch)
2018-10-15 10:33:02.530477: step 149, loss = 3.19 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:33:04.880498: step 150, loss = 2.96 (54.7 examples/sec; 2.342 sec/batch)
2018-10-15 10:33:07.357764: step 151, loss = 3.24 (51.8 examples/sec; 2.469 sec/batch)
2018-10-15 10:33:09.689956: step 152, loss = 3.13 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 10:33:12.019253: step 153, loss = 3.09 (55.1 examples/sec; 2.321 sec/batch)
2018-10-15 10:33:14.345423: step 154, loss = 3.19 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 10:33:16.720220: step 155, loss = 3.03 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 10:33:19.043895: step 156, loss = 3.09 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 10:33:21.413903: step 157, loss = 3.32 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 10:33:23.736541: step 158, loss = 3.00 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:33:26.078395: step 159, loss = 3.30 (54.8 examples/sec; 2.334 sec/batch)
2018-10-15 10:33:28.396282: step 160, loss = 3.18 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:33:30.730295: step 161, loss = 3.14 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 10:33:33.195499: step 162, loss = 3.27 (52.1 examples/sec; 2.457 sec/batch)
2018-10-15 10:33:35.512929: step 163, loss = 3.04 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 10:33:37.807271: step 164, loss = 3.44 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 10:33:40.115123: step 165, loss = 3.13 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 10:33:42.609595: step 166, loss = 3.26 (51.4 examples/sec; 2.488 sec/batch)
2018-10-15 10:33:44.977774: step 167, loss = 3.25 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:33:47.321361: step 168, loss = 3.20 (54.7 examples/sec; 2.338 sec/batch)
2018-10-15 10:33:49.635508: step 169, loss = 3.16 (55.5 examples/sec; 2.304 sec/batch)
2018-10-15 10:33:51.951793: step 170, loss = 3.28 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 10:33:54.354881: step 171, loss = 3.24 (53.4 examples/sec; 2.395 sec/batch)
2018-10-15 10:33:56.672315: step 172, loss = 3.20 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 10:33:59.074423: step 173, loss = 3.05 (53.4 examples/sec; 2.397 sec/batch)
2018-10-15 10:34:01.385146: step 174, loss = 2.95 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 10:34:03.744748: step 175, loss = 3.00 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 10:34:06.090238: step 176, loss = 3.31 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 10:34:08.378013: step 177, loss = 3.25 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 10:34:10.683869: step 178, loss = 3.04 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:34:12.987707: step 179, loss = 3.08 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:34:15.232224: step 180, loss = 3.20 (57.2 examples/sec; 2.239 sec/batch)
2018-10-15 10:34:17.541958: step 181, loss = 3.17 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 10:34:19.788215: step 182, loss = 3.00 (57.1 examples/sec; 2.241 sec/batch)
2018-10-15 10:34:22.112210: step 183, loss = 3.12 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 10:34:24.378101: step 184, loss = 2.96 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 10:34:26.694979: step 185, loss = 3.07 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 10:34:29.060721: step 186, loss = 3.15 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 10:34:31.365449: step 187, loss = 3.24 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:34:33.657129: step 188, loss = 3.13 (56.1 examples/sec; 2.282 sec/batch)
2018-10-15 10:34:36.053793: step 189, loss = 2.96 (53.5 examples/sec; 2.392 sec/batch)
2018-10-15 10:34:38.418657: step 190, loss = 3.07 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:34:40.694772: step 191, loss = 3.07 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 10:34:42.961006: step 192, loss = 2.92 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 10:34:45.333426: step 193, loss = 3.06 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 10:34:47.658097: step 194, loss = 3.07 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 10:34:49.969832: step 195, loss = 3.11 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:34:52.285875: step 196, loss = 2.93 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 10:34:54.560744: step 197, loss = 3.10 (56.5 examples/sec; 2.267 sec/batch)
2018-10-15 10:34:56.942606: step 198, loss = 3.05 (53.9 examples/sec; 2.377 sec/batch)
2018-10-15 10:34:59.358927: step 199, loss = 2.91 (53.2 examples/sec; 2.408 sec/batch)
2018-10-15 10:35:01.761153: step 200, loss = 3.09 (53.5 examples/sec; 2.394 sec/batch)
2018-10-15 10:35:04.619819: step 201, loss = 3.18 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 10:35:06.898010: step 202, loss = 3.28 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 10:35:09.205361: step 203, loss = 3.09 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:35:11.523891: step 204, loss = 3.19 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:35:13.821247: step 205, loss = 3.04 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 10:35:16.212663: step 206, loss = 3.04 (53.7 examples/sec; 2.383 sec/batch)
2018-10-15 10:35:18.517794: step 207, loss = 3.27 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:35:20.795634: step 208, loss = 3.02 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 10:35:23.203415: step 209, loss = 3.19 (53.3 examples/sec; 2.399 sec/batch)
2018-10-15 10:35:25.580734: step 210, loss = 2.90 (54.0 examples/sec; 2.372 sec/batch)
2018-10-15 10:35:27.904681: step 211, loss = 2.99 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 10:35:30.193941: step 212, loss = 3.23 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 10:35:32.503997: step 213, loss = 3.01 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 10:35:34.868054: step 214, loss = 2.79 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 10:35:37.147729: step 215, loss = 3.15 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 10:35:39.473511: step 216, loss = 3.21 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 10:35:41.766020: step 217, loss = 3.11 (56.0 examples/sec; 2.284 sec/batch)
2018-10-15 10:35:44.054880: step 218, loss = 3.28 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 10:35:46.378138: step 219, loss = 3.02 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 10:35:48.830769: step 220, loss = 3.06 (52.4 examples/sec; 2.444 sec/batch)
2018-10-15 10:35:51.157591: step 221, loss = 3.04 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 10:35:53.487927: step 222, loss = 2.92 (55.0 examples/sec; 2.325 sec/batch)
2018-10-15 10:35:55.827004: step 223, loss = 3.04 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 10:35:58.144216: step 224, loss = 2.88 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 10:36:00.505423: step 225, loss = 3.18 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 10:36:02.848751: step 226, loss = 3.03 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 10:36:05.224751: step 227, loss = 3.15 (54.0 examples/sec; 2.371 sec/batch)
2018-10-15 10:36:07.571589: step 228, loss = 3.12 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 10:36:09.938198: step 229, loss = 3.20 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 10:36:12.241188: step 230, loss = 2.92 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:36:14.641777: step 231, loss = 3.24 (53.5 examples/sec; 2.393 sec/batch)
2018-10-15 10:36:17.027264: step 232, loss = 3.07 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 10:36:19.331119: step 233, loss = 3.17 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 10:36:21.613163: step 234, loss = 3.25 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 10:36:23.857737: step 235, loss = 3.10 (57.2 examples/sec; 2.236 sec/batch)
2018-10-15 10:36:26.179836: step 236, loss = 3.04 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 10:36:28.499825: step 237, loss = 2.98 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 10:36:30.798563: step 238, loss = 2.90 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 10:36:33.155271: step 239, loss = 3.14 (54.5 examples/sec; 2.349 sec/batch)
2018-10-15 10:36:35.422852: step 240, loss = 3.09 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 10:36:37.749820: step 241, loss = 2.95 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 10:36:40.131421: step 242, loss = 2.91 (53.9 examples/sec; 2.373 sec/batch)
2018-10-15 10:36:42.475440: step 243, loss = 2.99 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 10:36:44.773253: step 244, loss = 3.12 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 10:36:47.096510: step 245, loss = 3.11 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:36:49.346340: step 246, loss = 3.19 (57.1 examples/sec; 2.241 sec/batch)
2018-10-15 10:36:51.713775: step 247, loss = 3.04 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 10:36:54.096040: step 248, loss = 2.96 (53.9 examples/sec; 2.374 sec/batch)
2018-10-15 10:36:56.416250: step 249, loss = 3.09 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:36:58.769248: step 250, loss = 3.00 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 10:37:01.072975: step 251, loss = 3.08 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:37:03.499270: step 252, loss = 3.13 (52.9 examples/sec; 2.418 sec/batch)
2018-10-15 10:37:05.878791: step 253, loss = 3.00 (54.0 examples/sec; 2.372 sec/batch)
2018-10-15 10:37:08.219608: step 254, loss = 3.12 (54.9 examples/sec; 2.333 sec/batch)
2018-10-15 10:37:10.496823: step 255, loss = 3.12 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 10:37:12.754356: step 256, loss = 3.07 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 10:37:15.084671: step 257, loss = 2.99 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 10:37:17.485125: step 258, loss = 2.96 (53.5 examples/sec; 2.393 sec/batch)
2018-10-15 10:37:19.765223: step 259, loss = 3.06 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 10:37:22.086881: step 260, loss = 2.84 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 10:37:24.404470: step 261, loss = 3.09 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:37:26.696669: step 262, loss = 3.11 (56.0 examples/sec; 2.285 sec/batch)
2018-10-15 10:37:29.002791: step 263, loss = 2.84 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 10:37:31.341083: step 264, loss = 3.15 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:37:33.756331: step 265, loss = 2.98 (53.2 examples/sec; 2.407 sec/batch)
2018-10-15 10:37:36.076432: step 266, loss = 3.09 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 10:37:38.345867: step 267, loss = 2.77 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 10:37:40.572951: step 268, loss = 3.13 (57.7 examples/sec; 2.219 sec/batch)
2018-10-15 10:37:43.003410: step 269, loss = 3.16 (52.8 examples/sec; 2.422 sec/batch)
2018-10-15 10:37:45.353194: step 270, loss = 3.08 (54.7 examples/sec; 2.342 sec/batch)
2018-10-15 10:37:47.740332: step 271, loss = 3.04 (53.8 examples/sec; 2.379 sec/batch)
2018-10-15 10:37:50.033983: step 272, loss = 3.07 (56.0 examples/sec; 2.285 sec/batch)
2018-10-15 10:37:52.357943: step 273, loss = 3.03 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 10:37:54.723034: step 274, loss = 2.94 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 10:37:57.092268: step 275, loss = 3.10 (54.2 examples/sec; 2.361 sec/batch)
2018-10-15 10:37:59.409475: step 276, loss = 3.24 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 10:38:01.677796: step 277, loss = 3.18 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 10:38:03.950858: step 278, loss = 3.12 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 10:38:06.262853: step 279, loss = 3.07 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:38:08.583050: step 280, loss = 2.99 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:38:10.898584: step 281, loss = 3.15 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 10:38:13.193292: step 282, loss = 3.07 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 10:38:15.461595: step 283, loss = 3.14 (56.6 examples/sec; 2.260 sec/batch)
2018-10-15 10:38:17.800992: step 284, loss = 2.97 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 10:38:20.130804: step 285, loss = 2.93 (55.2 examples/sec; 2.321 sec/batch)
2018-10-15 10:38:22.553776: step 286, loss = 2.91 (53.0 examples/sec; 2.415 sec/batch)
2018-10-15 10:38:25.011960: step 287, loss = 2.97 (52.2 examples/sec; 2.450 sec/batch)
2018-10-15 10:38:27.374133: step 288, loss = 3.07 (54.4 examples/sec; 2.354 sec/batch)
2018-10-15 10:38:29.673028: step 289, loss = 3.16 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 10:38:32.062340: step 290, loss = 3.09 (53.8 examples/sec; 2.381 sec/batch)
2018-10-15 10:38:34.482342: step 291, loss = 3.02 (53.1 examples/sec; 2.412 sec/batch)
2018-10-15 10:38:36.804635: step 292, loss = 3.12 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:38:39.218595: step 293, loss = 2.97 (53.2 examples/sec; 2.406 sec/batch)
2018-10-15 10:38:41.548250: step 294, loss = 3.08 (55.1 examples/sec; 2.321 sec/batch)
2018-10-15 10:38:43.858158: step 295, loss = 2.84 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:38:46.224087: step 296, loss = 2.92 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 10:38:48.538873: step 297, loss = 3.06 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 10:38:50.897121: step 298, loss = 3.03 (54.4 examples/sec; 2.353 sec/batch)
2018-10-15 10:38:53.228387: step 299, loss = 3.08 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 10:38:55.493212: step 300, loss = 3.09 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 10:38:58.389652: step 301, loss = 2.88 (53.6 examples/sec; 2.388 sec/batch)
2018-10-15 10:39:00.720408: step 302, loss = 3.08 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 10:39:03.067352: step 303, loss = 3.15 (54.7 examples/sec; 2.341 sec/batch)
2018-10-15 10:39:05.334971: step 304, loss = 3.06 (56.6 examples/sec; 2.260 sec/batch)
2018-10-15 10:39:07.622934: step 305, loss = 3.04 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 10:39:10.009764: step 306, loss = 3.10 (53.8 examples/sec; 2.379 sec/batch)
2018-10-15 10:39:12.382137: step 307, loss = 3.18 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 10:39:14.654514: step 308, loss = 3.04 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 10:39:17.065394: step 309, loss = 2.93 (53.3 examples/sec; 2.404 sec/batch)
2018-10-15 10:39:19.458747: step 310, loss = 2.92 (53.7 examples/sec; 2.385 sec/batch)
2018-10-15 10:39:21.819741: step 311, loss = 3.10 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 10:39:24.156839: step 312, loss = 3.01 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 10:39:26.497908: step 313, loss = 2.96 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 10:39:28.913269: step 314, loss = 2.97 (53.1 examples/sec; 2.410 sec/batch)
2018-10-15 10:39:31.226939: step 315, loss = 3.12 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:39:33.516097: step 316, loss = 3.16 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 10:39:35.888136: step 317, loss = 2.97 (54.2 examples/sec; 2.364 sec/batch)
2018-10-15 10:39:38.245525: step 318, loss = 3.19 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 10:39:40.557365: step 319, loss = 3.00 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 10:39:42.895338: step 320, loss = 2.92 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:39:45.229702: step 321, loss = 3.49 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 10:39:47.576757: step 322, loss = 3.21 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 10:39:50.006091: step 323, loss = 3.17 (52.9 examples/sec; 2.421 sec/batch)
2018-10-15 10:39:52.373541: step 324, loss = 3.22 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 10:39:54.677990: step 325, loss = 2.88 (55.7 examples/sec; 2.296 sec/batch)
2018-10-15 10:39:56.957014: step 326, loss = 2.92 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 10:39:59.343193: step 327, loss = 3.09 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 10:40:01.698246: step 328, loss = 3.19 (54.5 examples/sec; 2.347 sec/batch)
2018-10-15 10:40:04.040273: step 329, loss = 3.10 (54.9 examples/sec; 2.333 sec/batch)
2018-10-15 10:40:06.427956: step 330, loss = 3.01 (53.8 examples/sec; 2.379 sec/batch)
2018-10-15 10:40:08.711187: step 331, loss = 2.99 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 10:40:11.068076: step 332, loss = 3.09 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 10:40:13.408458: step 333, loss = 3.08 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 10:40:15.750030: step 334, loss = 3.14 (54.9 examples/sec; 2.333 sec/batch)
2018-10-15 10:40:18.108451: step 335, loss = 3.03 (54.5 examples/sec; 2.349 sec/batch)
2018-10-15 10:40:20.382925: step 336, loss = 3.13 (56.5 examples/sec; 2.266 sec/batch)
2018-10-15 10:40:22.659275: step 337, loss = 3.09 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 10:40:25.018563: step 338, loss = 3.13 (54.4 examples/sec; 2.354 sec/batch)
2018-10-15 10:40:27.473056: step 339, loss = 3.26 (52.4 examples/sec; 2.444 sec/batch)
2018-10-15 10:40:29.839525: step 340, loss = 2.94 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:40:32.198277: step 341, loss = 2.98 (54.5 examples/sec; 2.351 sec/batch)
2018-10-15 10:40:34.469630: step 342, loss = 2.97 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 10:40:36.848256: step 343, loss = 3.00 (54.0 examples/sec; 2.370 sec/batch)
2018-10-15 10:40:39.137211: step 344, loss = 3.06 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 10:40:41.424331: step 345, loss = 3.06 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 10:40:43.802781: step 346, loss = 2.86 (54.0 examples/sec; 2.370 sec/batch)
2018-10-15 10:40:46.140999: step 347, loss = 3.01 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:40:48.464939: step 348, loss = 3.04 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:40:50.857712: step 349, loss = 2.98 (53.6 examples/sec; 2.387 sec/batch)
2018-10-15 10:40:53.175867: step 350, loss = 3.00 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:40:55.539376: step 351, loss = 2.93 (54.4 examples/sec; 2.355 sec/batch)
2018-10-15 10:40:57.818690: step 352, loss = 2.84 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 10:41:00.125678: step 353, loss = 2.86 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 10:41:02.438853: step 354, loss = 2.97 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:41:04.755082: step 355, loss = 2.82 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 10:41:07.069229: step 356, loss = 2.97 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 10:41:09.393543: step 357, loss = 3.01 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 10:41:11.744473: step 358, loss = 2.76 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 10:41:14.082745: step 359, loss = 2.84 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:41:16.508688: step 360, loss = 3.00 (52.9 examples/sec; 2.421 sec/batch)
2018-10-15 10:41:18.857277: step 361, loss = 2.82 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 10:41:21.178448: step 362, loss = 3.09 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 10:41:23.493245: step 363, loss = 2.94 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:41:25.800323: step 364, loss = 2.94 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:41:28.103592: step 365, loss = 2.70 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:41:30.453699: step 366, loss = 2.74 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 10:41:32.773554: step 367, loss = 3.23 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 10:41:35.043493: step 368, loss = 3.08 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 10:41:37.310421: step 369, loss = 2.91 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 10:41:39.732258: step 370, loss = 2.81 (53.0 examples/sec; 2.413 sec/batch)
2018-10-15 10:41:42.047637: step 371, loss = 3.06 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 10:41:44.413452: step 372, loss = 2.92 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 10:41:46.700120: step 373, loss = 3.03 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 10:41:49.040248: step 374, loss = 2.81 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 10:41:51.310429: step 375, loss = 3.08 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 10:41:53.701452: step 376, loss = 2.89 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 10:41:56.095113: step 377, loss = 2.86 (53.6 examples/sec; 2.389 sec/batch)
2018-10-15 10:41:58.471102: step 378, loss = 3.08 (54.0 examples/sec; 2.371 sec/batch)
2018-10-15 10:42:00.796950: step 379, loss = 3.05 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 10:42:03.050641: step 380, loss = 2.96 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 10:42:05.339778: step 381, loss = 3.09 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 10:42:07.679695: step 382, loss = 2.99 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 10:42:10.000559: step 383, loss = 2.98 (55.4 examples/sec; 2.313 sec/batch)
2018-10-15 10:42:12.358596: step 384, loss = 2.97 (54.5 examples/sec; 2.349 sec/batch)
2018-10-15 10:42:14.712931: step 385, loss = 3.11 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 10:42:17.014730: step 386, loss = 3.17 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 10:42:19.354827: step 387, loss = 2.90 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 10:42:21.656308: step 388, loss = 2.78 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 10:42:24.081431: step 389, loss = 2.93 (53.0 examples/sec; 2.417 sec/batch)
2018-10-15 10:42:26.466792: step 390, loss = 3.03 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 10:42:28.751303: step 391, loss = 3.07 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 10:42:31.100387: step 392, loss = 3.10 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 10:42:33.535420: step 393, loss = 3.25 (52.8 examples/sec; 2.426 sec/batch)
2018-10-15 10:42:35.908833: step 394, loss = 2.99 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 10:42:38.249352: step 395, loss = 2.89 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 10:42:40.564521: step 396, loss = 3.16 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 10:42:42.945552: step 397, loss = 3.04 (54.0 examples/sec; 2.372 sec/batch)
2018-10-15 10:42:45.262869: step 398, loss = 2.95 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 10:42:47.568912: step 399, loss = 2.98 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:42:49.978392: step 400, loss = 2.91 (53.3 examples/sec; 2.401 sec/batch)
2018-10-15 10:42:52.963827: step 401, loss = 3.13 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 10:42:55.269644: step 402, loss = 2.98 (55.7 examples/sec; 2.296 sec/batch)
2018-10-15 10:42:57.693661: step 403, loss = 2.86 (52.9 examples/sec; 2.419 sec/batch)
2018-10-15 10:43:00.046137: step 404, loss = 2.82 (54.5 examples/sec; 2.347 sec/batch)
2018-10-15 10:43:02.520003: step 405, loss = 2.82 (51.9 examples/sec; 2.465 sec/batch)
2018-10-15 10:43:04.820822: step 406, loss = 3.08 (55.8 examples/sec; 2.292 sec/batch)
2018-10-15 10:43:07.213468: step 407, loss = 2.86 (53.7 examples/sec; 2.384 sec/batch)
2018-10-15 10:43:09.539327: step 408, loss = 3.17 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 10:43:11.798714: step 409, loss = 3.06 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 10:43:14.216568: step 410, loss = 3.01 (53.1 examples/sec; 2.409 sec/batch)
2018-10-15 10:43:16.498750: step 411, loss = 2.86 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 10:43:18.810036: step 412, loss = 3.11 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 10:43:21.172150: step 413, loss = 3.10 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 10:43:23.560976: step 414, loss = 2.88 (53.8 examples/sec; 2.379 sec/batch)
2018-10-15 10:43:25.865741: step 415, loss = 3.06 (55.7 examples/sec; 2.296 sec/batch)
2018-10-15 10:43:28.162585: step 416, loss = 3.09 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 10:43:30.535083: step 417, loss = 2.78 (54.2 examples/sec; 2.364 sec/batch)
2018-10-15 10:43:32.980864: step 418, loss = 2.86 (52.5 examples/sec; 2.437 sec/batch)
2018-10-15 10:43:35.348952: step 419, loss = 2.84 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 10:43:37.641723: step 420, loss = 2.94 (56.0 examples/sec; 2.285 sec/batch)
2018-10-15 10:43:39.995313: step 421, loss = 2.91 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 10:43:42.277232: step 422, loss = 2.85 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 10:43:44.535947: step 423, loss = 2.92 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 10:43:46.812633: step 424, loss = 3.07 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 10:43:49.236622: step 425, loss = 3.14 (53.0 examples/sec; 2.415 sec/batch)
2018-10-15 10:43:51.788441: step 426, loss = 3.01 (50.3 examples/sec; 2.543 sec/batch)
2018-10-15 10:43:54.073998: step 427, loss = 2.97 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 10:43:56.407628: step 428, loss = 2.85 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 10:43:58.791198: step 429, loss = 2.86 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 10:44:01.104816: step 430, loss = 2.88 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:44:03.458938: step 431, loss = 2.76 (54.5 examples/sec; 2.349 sec/batch)
2018-10-15 10:44:05.862752: step 432, loss = 3.01 (53.4 examples/sec; 2.395 sec/batch)
2018-10-15 10:44:08.150733: step 433, loss = 2.97 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 10:44:10.396437: step 434, loss = 2.85 (57.2 examples/sec; 2.237 sec/batch)
2018-10-15 10:44:12.706727: step 435, loss = 2.95 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 10:44:14.997787: step 436, loss = 2.99 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:44:17.381509: step 437, loss = 2.83 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 10:44:19.709369: step 438, loss = 3.06 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 10:44:21.977384: step 439, loss = 3.07 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 10:44:24.253234: step 440, loss = 2.84 (56.5 examples/sec; 2.267 sec/batch)
2018-10-15 10:44:26.585664: step 441, loss = 3.20 (55.1 examples/sec; 2.324 sec/batch)
2018-10-15 10:44:28.787147: step 442, loss = 3.28 (58.4 examples/sec; 2.191 sec/batch)
2018-10-15 10:44:31.157332: step 443, loss = 3.11 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:44:33.440764: step 444, loss = 2.96 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 10:44:35.654965: step 445, loss = 2.95 (58.0 examples/sec; 2.205 sec/batch)
2018-10-15 10:44:37.930460: step 446, loss = 2.89 (56.5 examples/sec; 2.267 sec/batch)
2018-10-15 10:44:40.331328: step 447, loss = 2.97 (53.5 examples/sec; 2.392 sec/batch)
2018-10-15 10:44:42.643825: step 448, loss = 2.97 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:44:44.944348: step 449, loss = 2.86 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 10:44:47.217434: step 450, loss = 2.88 (56.5 examples/sec; 2.265 sec/batch)
2018-10-15 10:44:49.534453: step 451, loss = 3.00 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:44:51.877191: step 452, loss = 2.97 (54.8 examples/sec; 2.334 sec/batch)
2018-10-15 10:44:54.298333: step 453, loss = 3.01 (53.1 examples/sec; 2.413 sec/batch)
2018-10-15 10:44:56.683301: step 454, loss = 3.00 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 10:44:58.993349: step 455, loss = 2.89 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 10:45:01.317394: step 456, loss = 2.79 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:45:03.688550: step 457, loss = 2.79 (54.1 examples/sec; 2.366 sec/batch)
2018-10-15 10:45:06.084185: step 458, loss = 2.93 (53.5 examples/sec; 2.390 sec/batch)
2018-10-15 10:45:08.459257: step 459, loss = 2.85 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 10:45:10.766085: step 460, loss = 2.76 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:45:13.040339: step 461, loss = 2.85 (56.5 examples/sec; 2.266 sec/batch)
2018-10-15 10:45:15.294993: step 462, loss = 2.81 (56.9 examples/sec; 2.249 sec/batch)
2018-10-15 10:45:17.660034: step 463, loss = 2.78 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 10:45:20.029770: step 464, loss = 2.91 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 10:45:22.387475: step 465, loss = 2.85 (54.4 examples/sec; 2.353 sec/batch)
2018-10-15 10:45:24.761739: step 466, loss = 2.64 (54.1 examples/sec; 2.366 sec/batch)
2018-10-15 10:45:27.043754: step 467, loss = 2.80 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 10:45:29.337293: step 468, loss = 2.82 (56.0 examples/sec; 2.284 sec/batch)
2018-10-15 10:45:31.737974: step 469, loss = 2.88 (53.5 examples/sec; 2.392 sec/batch)
2018-10-15 10:45:34.122189: step 470, loss = 2.73 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 10:45:36.535626: step 471, loss = 2.80 (53.2 examples/sec; 2.405 sec/batch)
2018-10-15 10:45:38.839597: step 472, loss = 2.76 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 10:45:41.262336: step 473, loss = 2.81 (53.0 examples/sec; 2.414 sec/batch)
2018-10-15 10:45:43.630958: step 474, loss = 2.74 (54.2 examples/sec; 2.364 sec/batch)
2018-10-15 10:45:45.898500: step 475, loss = 2.84 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 10:45:48.326731: step 476, loss = 2.79 (52.8 examples/sec; 2.423 sec/batch)
2018-10-15 10:45:50.596976: step 477, loss = 2.90 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 10:45:52.903887: step 478, loss = 2.94 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:45:55.272036: step 479, loss = 2.79 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:45:57.650448: step 480, loss = 3.15 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 10:46:00.008496: step 481, loss = 2.86 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 10:46:02.321896: step 482, loss = 2.99 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:46:04.680493: step 483, loss = 3.03 (54.4 examples/sec; 2.353 sec/batch)
2018-10-15 10:46:06.973996: step 484, loss = 2.84 (56.0 examples/sec; 2.285 sec/batch)
2018-10-15 10:46:09.361339: step 485, loss = 2.94 (53.8 examples/sec; 2.379 sec/batch)
2018-10-15 10:46:11.619792: step 486, loss = 2.90 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 10:46:14.045951: step 487, loss = 2.75 (52.9 examples/sec; 2.421 sec/batch)
2018-10-15 10:46:16.328683: step 488, loss = 2.82 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 10:46:18.553652: step 489, loss = 2.76 (57.7 examples/sec; 2.219 sec/batch)
2018-10-15 10:46:20.957883: step 490, loss = 3.03 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 10:46:23.356393: step 491, loss = 2.91 (53.5 examples/sec; 2.393 sec/batch)
2018-10-15 10:46:25.768757: step 492, loss = 2.96 (53.2 examples/sec; 2.404 sec/batch)
2018-10-15 10:46:28.134890: step 493, loss = 2.86 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 10:46:30.495634: step 494, loss = 2.65 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 10:46:32.816080: step 495, loss = 2.74 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:46:35.221070: step 496, loss = 3.00 (53.4 examples/sec; 2.397 sec/batch)
2018-10-15 10:46:37.503929: step 497, loss = 3.07 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 10:46:39.905894: step 498, loss = 3.11 (53.4 examples/sec; 2.399 sec/batch)
2018-10-15 10:46:42.239290: step 499, loss = 2.94 (55.0 examples/sec; 2.325 sec/batch)
2018-10-15 10:46:44.531637: step 500, loss = 3.05 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 10:46:47.460790: step 501, loss = 3.02 (53.7 examples/sec; 2.385 sec/batch)
2018-10-15 10:46:49.804424: step 502, loss = 2.88 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 10:46:52.218935: step 503, loss = 2.88 (53.2 examples/sec; 2.406 sec/batch)
2018-10-15 10:46:54.479176: step 504, loss = 2.87 (56.8 examples/sec; 2.255 sec/batch)
2018-10-15 10:46:56.741820: step 505, loss = 3.23 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 10:46:59.015466: step 506, loss = 2.93 (56.5 examples/sec; 2.265 sec/batch)
2018-10-15 10:47:01.322271: step 507, loss = 2.85 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 10:47:03.663864: step 508, loss = 2.85 (54.9 examples/sec; 2.333 sec/batch)
2018-10-15 10:47:06.046825: step 509, loss = 2.95 (53.9 examples/sec; 2.374 sec/batch)
2018-10-15 10:47:08.363701: step 510, loss = 2.94 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 10:47:10.621698: step 511, loss = 2.98 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 10:47:12.941334: step 512, loss = 2.87 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 10:47:15.306192: step 513, loss = 3.06 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:47:17.760822: step 514, loss = 3.18 (52.3 examples/sec; 2.446 sec/batch)
2018-10-15 10:47:20.081021: step 515, loss = 2.95 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:47:22.366720: step 516, loss = 3.12 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 10:47:24.713006: step 517, loss = 2.80 (54.8 examples/sec; 2.338 sec/batch)
2018-10-15 10:47:27.095780: step 518, loss = 3.16 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 10:47:29.412034: step 519, loss = 2.86 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 10:47:31.747528: step 520, loss = 2.92 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:47:34.024765: step 521, loss = 2.91 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 10:47:36.263583: step 522, loss = 3.08 (57.3 examples/sec; 2.234 sec/batch)
2018-10-15 10:47:38.645260: step 523, loss = 2.85 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 10:47:40.979693: step 524, loss = 2.68 (55.0 examples/sec; 2.325 sec/batch)
2018-10-15 10:47:43.271632: step 525, loss = 2.85 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 10:47:45.558614: step 526, loss = 2.91 (56.1 examples/sec; 2.282 sec/batch)
2018-10-15 10:47:47.772331: step 527, loss = 2.92 (58.0 examples/sec; 2.205 sec/batch)
2018-10-15 10:47:50.061344: step 528, loss = 2.90 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 10:47:52.453728: step 529, loss = 2.69 (53.7 examples/sec; 2.383 sec/batch)
2018-10-15 10:47:54.808108: step 530, loss = 2.79 (54.6 examples/sec; 2.346 sec/batch)
2018-10-15 10:47:57.146974: step 531, loss = 2.94 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:47:59.371523: step 532, loss = 2.83 (57.8 examples/sec; 2.216 sec/batch)
2018-10-15 10:48:01.678672: step 533, loss = 3.02 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:48:04.027959: step 534, loss = 2.76 (54.7 examples/sec; 2.341 sec/batch)
2018-10-15 10:48:06.349680: step 535, loss = 2.75 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 10:48:08.704780: step 536, loss = 2.87 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 10:48:10.964244: step 537, loss = 3.00 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 10:48:13.188175: step 538, loss = 2.90 (57.8 examples/sec; 2.215 sec/batch)
2018-10-15 10:48:15.528155: step 539, loss = 2.80 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 10:48:17.823353: step 540, loss = 2.86 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 10:48:20.120864: step 541, loss = 2.94 (55.8 examples/sec; 2.292 sec/batch)
2018-10-15 10:48:22.544110: step 542, loss = 2.78 (53.0 examples/sec; 2.415 sec/batch)
2018-10-15 10:48:24.901049: step 543, loss = 2.95 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 10:48:27.129829: step 544, loss = 2.97 (57.7 examples/sec; 2.220 sec/batch)
2018-10-15 10:48:29.410107: step 545, loss = 2.98 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 10:48:31.762945: step 546, loss = 2.98 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 10:48:34.131129: step 547, loss = 2.96 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:48:36.487885: step 548, loss = 2.81 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 10:48:38.767341: step 549, loss = 2.86 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 10:48:41.132771: step 550, loss = 2.75 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 10:48:43.560901: step 551, loss = 2.78 (52.8 examples/sec; 2.423 sec/batch)
2018-10-15 10:48:45.910760: step 552, loss = 2.84 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 10:48:48.205264: step 553, loss = 2.89 (56.0 examples/sec; 2.284 sec/batch)
2018-10-15 10:48:50.514574: step 554, loss = 2.87 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:48:52.806695: step 555, loss = 2.77 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:48:55.257868: step 556, loss = 2.85 (52.4 examples/sec; 2.442 sec/batch)
2018-10-15 10:48:57.723822: step 557, loss = 2.81 (52.1 examples/sec; 2.457 sec/batch)
2018-10-15 10:49:00.030358: step 558, loss = 2.94 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:49:02.337985: step 559, loss = 2.70 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 10:49:04.843272: step 560, loss = 2.86 (51.3 examples/sec; 2.497 sec/batch)
2018-10-15 10:49:07.215348: step 561, loss = 2.68 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 10:49:09.520860: step 562, loss = 2.63 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:49:11.877493: step 563, loss = 2.70 (54.4 examples/sec; 2.351 sec/batch)
2018-10-15 10:49:14.239924: step 564, loss = 2.76 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 10:49:16.545982: step 565, loss = 2.65 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:49:18.945087: step 566, loss = 2.76 (53.6 examples/sec; 2.390 sec/batch)
2018-10-15 10:49:21.263942: step 567, loss = 2.67 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:49:23.598098: step 568, loss = 2.59 (55.0 examples/sec; 2.325 sec/batch)
2018-10-15 10:49:25.945933: step 569, loss = 2.96 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 10:49:28.215978: step 570, loss = 2.64 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 10:49:30.523556: step 571, loss = 2.75 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 10:49:32.877790: step 572, loss = 2.71 (54.5 examples/sec; 2.347 sec/batch)
2018-10-15 10:49:35.167313: step 573, loss = 2.76 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:49:37.587035: step 574, loss = 2.85 (53.1 examples/sec; 2.411 sec/batch)
2018-10-15 10:49:39.932471: step 575, loss = 2.76 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 10:49:42.307529: step 576, loss = 2.66 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 10:49:44.640305: step 577, loss = 2.87 (55.1 examples/sec; 2.324 sec/batch)
2018-10-15 10:49:46.909950: step 578, loss = 2.82 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 10:49:49.278827: step 579, loss = 2.65 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:49:51.604945: step 580, loss = 2.98 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 10:49:53.858443: step 581, loss = 2.81 (57.0 examples/sec; 2.245 sec/batch)
2018-10-15 10:49:56.145278: step 582, loss = 2.98 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 10:49:58.513959: step 583, loss = 2.82 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:50:00.798336: step 584, loss = 2.76 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 10:50:03.183133: step 585, loss = 3.04 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 10:50:05.629324: step 586, loss = 2.81 (52.5 examples/sec; 2.437 sec/batch)
2018-10-15 10:50:07.954871: step 587, loss = 2.95 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 10:50:10.423082: step 588, loss = 2.76 (52.0 examples/sec; 2.463 sec/batch)
2018-10-15 10:50:12.773363: step 589, loss = 2.82 (54.7 examples/sec; 2.342 sec/batch)
2018-10-15 10:50:15.091953: step 590, loss = 2.96 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:50:17.540799: step 591, loss = 2.89 (52.5 examples/sec; 2.440 sec/batch)
2018-10-15 10:50:19.877832: step 592, loss = 2.75 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 10:50:22.237150: step 593, loss = 3.15 (54.4 examples/sec; 2.354 sec/batch)
2018-10-15 10:50:24.570820: step 594, loss = 2.82 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 10:50:26.985049: step 595, loss = 2.66 (53.1 examples/sec; 2.411 sec/batch)
2018-10-15 10:50:29.343954: step 596, loss = 2.76 (54.4 examples/sec; 2.351 sec/batch)
2018-10-15 10:50:31.664366: step 597, loss = 2.86 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 10:50:34.079455: step 598, loss = 2.80 (53.2 examples/sec; 2.407 sec/batch)
2018-10-15 10:50:36.464403: step 599, loss = 2.90 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 10:50:38.741189: step 600, loss = 2.85 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 10:50:41.772602: step 601, loss = 2.96 (52.0 examples/sec; 2.461 sec/batch)
2018-10-15 10:50:44.102632: step 602, loss = 2.71 (55.1 examples/sec; 2.321 sec/batch)
2018-10-15 10:50:46.417469: step 603, loss = 2.84 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 10:50:48.852266: step 604, loss = 3.02 (52.7 examples/sec; 2.429 sec/batch)
2018-10-15 10:50:51.144277: step 605, loss = 2.70 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:50:53.542105: step 606, loss = 2.83 (53.5 examples/sec; 2.393 sec/batch)
2018-10-15 10:50:55.847355: step 607, loss = 2.85 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:50:58.108493: step 608, loss = 2.75 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 10:51:00.515655: step 609, loss = 2.86 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 10:51:02.898926: step 610, loss = 2.88 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 10:51:05.261738: step 611, loss = 2.73 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 10:51:07.627095: step 612, loss = 2.68 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 10:51:09.982163: step 613, loss = 2.63 (54.6 examples/sec; 2.346 sec/batch)
2018-10-15 10:51:12.325989: step 614, loss = 3.12 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 10:51:14.718171: step 615, loss = 2.76 (53.7 examples/sec; 2.384 sec/batch)
2018-10-15 10:51:17.025596: step 616, loss = 3.04 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 10:51:19.429201: step 617, loss = 2.89 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 10:51:21.774117: step 618, loss = 2.74 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 10:51:24.134787: step 619, loss = 2.98 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 10:51:26.439710: step 620, loss = 2.80 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 10:51:28.828593: step 621, loss = 2.81 (53.7 examples/sec; 2.383 sec/batch)
2018-10-15 10:51:31.096162: step 622, loss = 2.81 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 10:51:33.376181: step 623, loss = 2.78 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 10:51:35.694603: step 624, loss = 2.79 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 10:51:37.931831: step 625, loss = 2.82 (57.4 examples/sec; 2.229 sec/batch)
2018-10-15 10:51:40.151809: step 626, loss = 2.87 (57.8 examples/sec; 2.213 sec/batch)
2018-10-15 10:51:42.512130: step 627, loss = 2.54 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 10:51:44.991777: step 628, loss = 2.91 (51.8 examples/sec; 2.473 sec/batch)
2018-10-15 10:51:47.317819: step 629, loss = 2.92 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 10:51:49.619646: step 630, loss = 2.70 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 10:51:51.987443: step 631, loss = 2.88 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 10:51:54.285487: step 632, loss = 3.05 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 10:51:56.652072: step 633, loss = 2.94 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 10:51:58.998847: step 634, loss = 2.76 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 10:52:01.335888: step 635, loss = 2.93 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 10:52:03.774347: step 636, loss = 2.80 (52.6 examples/sec; 2.433 sec/batch)
2018-10-15 10:52:06.102952: step 637, loss = 2.72 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 10:52:08.404450: step 638, loss = 2.77 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 10:52:10.733630: step 639, loss = 2.68 (55.2 examples/sec; 2.321 sec/batch)
2018-10-15 10:52:13.012068: step 640, loss = 2.92 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 10:52:15.479529: step 641, loss = 2.69 (52.1 examples/sec; 2.459 sec/batch)
2018-10-15 10:52:17.819509: step 642, loss = 2.83 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 10:52:20.201900: step 643, loss = 2.86 (53.9 examples/sec; 2.377 sec/batch)
2018-10-15 10:52:22.551670: step 644, loss = 2.84 (54.7 examples/sec; 2.342 sec/batch)
2018-10-15 10:52:24.865790: step 645, loss = 2.72 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 10:52:27.115358: step 646, loss = 3.22 (57.0 examples/sec; 2.244 sec/batch)
2018-10-15 10:52:29.416998: step 647, loss = 2.80 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 10:52:31.897667: step 648, loss = 2.83 (51.8 examples/sec; 2.472 sec/batch)
2018-10-15 10:52:34.246427: step 649, loss = 2.76 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 10:52:36.546010: step 650, loss = 2.75 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 10:52:39.006494: step 651, loss = 2.73 (52.2 examples/sec; 2.454 sec/batch)
2018-10-15 10:52:41.326456: step 652, loss = 2.85 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 10:52:43.602963: step 653, loss = 2.78 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 10:52:45.950507: step 654, loss = 2.72 (54.6 examples/sec; 2.342 sec/batch)
2018-10-15 10:52:48.215895: step 655, loss = 2.71 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 10:52:50.518982: step 656, loss = 2.62 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 10:52:52.910165: step 657, loss = 2.90 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 10:52:55.225861: step 658, loss = 2.78 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 10:52:57.684476: step 659, loss = 2.81 (52.3 examples/sec; 2.449 sec/batch)
2018-10-15 10:53:00.023422: step 660, loss = 2.83 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:53:02.369168: step 661, loss = 2.78 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 10:53:04.698261: step 662, loss = 2.94 (55.1 examples/sec; 2.324 sec/batch)
2018-10-15 10:53:06.992105: step 663, loss = 2.68 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 10:53:09.418048: step 664, loss = 2.75 (52.9 examples/sec; 2.421 sec/batch)
2018-10-15 10:53:11.743459: step 665, loss = 2.87 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 10:53:13.967075: step 666, loss = 2.75 (57.7 examples/sec; 2.217 sec/batch)
2018-10-15 10:53:16.280326: step 667, loss = 2.63 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:53:18.687554: step 668, loss = 2.66 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 10:53:21.096033: step 669, loss = 2.74 (53.4 examples/sec; 2.399 sec/batch)
2018-10-15 10:53:23.461658: step 670, loss = 2.62 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 10:53:25.738412: step 671, loss = 2.77 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 10:53:28.061427: step 672, loss = 2.66 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 10:53:30.417373: step 673, loss = 2.55 (54.5 examples/sec; 2.351 sec/batch)
2018-10-15 10:53:32.775478: step 674, loss = 2.74 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 10:53:35.206165: step 675, loss = 2.53 (52.8 examples/sec; 2.423 sec/batch)
2018-10-15 10:53:37.511615: step 676, loss = 2.54 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:53:39.823075: step 677, loss = 2.78 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 10:53:42.187139: step 678, loss = 2.95 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 10:53:44.572673: step 679, loss = 2.81 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 10:53:46.948555: step 680, loss = 2.56 (54.1 examples/sec; 2.368 sec/batch)
2018-10-15 10:53:49.313192: step 681, loss = 2.59 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:53:51.729055: step 682, loss = 2.75 (53.2 examples/sec; 2.408 sec/batch)
2018-10-15 10:53:54.064533: step 683, loss = 2.87 (54.9 examples/sec; 2.329 sec/batch)
2018-10-15 10:53:56.497937: step 684, loss = 2.80 (52.8 examples/sec; 2.425 sec/batch)
2018-10-15 10:53:58.910217: step 685, loss = 2.91 (53.2 examples/sec; 2.404 sec/batch)
2018-10-15 10:54:01.235589: step 686, loss = 2.75 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 10:54:03.577743: step 687, loss = 2.79 (54.8 examples/sec; 2.334 sec/batch)
2018-10-15 10:54:05.912168: step 688, loss = 2.74 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 10:54:08.265796: step 689, loss = 2.77 (54.6 examples/sec; 2.346 sec/batch)
2018-10-15 10:54:10.596205: step 690, loss = 2.78 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 10:54:12.976465: step 691, loss = 2.76 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 10:54:15.287008: step 692, loss = 2.77 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:54:17.593147: step 693, loss = 2.82 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:54:19.935165: step 694, loss = 2.78 (54.9 examples/sec; 2.334 sec/batch)
2018-10-15 10:54:22.307312: step 695, loss = 2.89 (54.2 examples/sec; 2.364 sec/batch)
2018-10-15 10:54:24.649239: step 696, loss = 2.77 (54.9 examples/sec; 2.333 sec/batch)
2018-10-15 10:54:26.967363: step 697, loss = 2.80 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 10:54:29.234559: step 698, loss = 2.70 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 10:54:31.681759: step 699, loss = 2.71 (52.5 examples/sec; 2.439 sec/batch)
2018-10-15 10:54:34.099549: step 700, loss = 2.69 (53.1 examples/sec; 2.409 sec/batch)
2018-10-15 10:54:37.091583: step 701, loss = 2.70 (54.6 examples/sec; 2.346 sec/batch)
2018-10-15 10:54:39.394780: step 702, loss = 2.81 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:54:41.774578: step 703, loss = 2.59 (54.0 examples/sec; 2.371 sec/batch)
2018-10-15 10:54:44.176334: step 704, loss = 2.91 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 10:54:46.666119: step 705, loss = 2.77 (51.6 examples/sec; 2.482 sec/batch)
2018-10-15 10:54:49.085649: step 706, loss = 2.75 (53.1 examples/sec; 2.412 sec/batch)
2018-10-15 10:54:51.348943: step 707, loss = 2.79 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 10:54:53.665918: step 708, loss = 2.88 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 10:54:56.072011: step 709, loss = 2.76 (53.4 examples/sec; 2.397 sec/batch)
2018-10-15 10:54:58.420495: step 710, loss = 2.85 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 10:55:00.826325: step 711, loss = 2.62 (53.4 examples/sec; 2.397 sec/batch)
2018-10-15 10:55:03.118211: step 712, loss = 2.97 (56.1 examples/sec; 2.284 sec/batch)
2018-10-15 10:55:05.452552: step 713, loss = 3.07 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 10:55:07.848690: step 714, loss = 2.69 (53.6 examples/sec; 2.388 sec/batch)
2018-10-15 10:55:10.261194: step 715, loss = 2.87 (53.3 examples/sec; 2.402 sec/batch)
2018-10-15 10:55:12.666159: step 716, loss = 2.68 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 10:55:15.080305: step 717, loss = 2.61 (53.2 examples/sec; 2.405 sec/batch)
2018-10-15 10:55:17.416186: step 718, loss = 2.82 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 10:55:19.776859: step 719, loss = 2.59 (54.4 examples/sec; 2.353 sec/batch)
2018-10-15 10:55:22.226572: step 720, loss = 2.81 (52.4 examples/sec; 2.441 sec/batch)
2018-10-15 10:55:24.621773: step 721, loss = 2.72 (53.6 examples/sec; 2.387 sec/batch)
2018-10-15 10:55:27.014158: step 722, loss = 2.60 (53.7 examples/sec; 2.384 sec/batch)
2018-10-15 10:55:29.385769: step 723, loss = 2.79 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 10:55:31.753531: step 724, loss = 2.81 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 10:55:34.085418: step 725, loss = 2.73 (55.1 examples/sec; 2.324 sec/batch)
2018-10-15 10:55:36.464466: step 726, loss = 2.82 (53.9 examples/sec; 2.374 sec/batch)
2018-10-15 10:55:38.860970: step 727, loss = 2.82 (53.5 examples/sec; 2.391 sec/batch)
2018-10-15 10:55:41.172200: step 728, loss = 2.68 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 10:55:43.437289: step 729, loss = 2.81 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 10:55:45.754580: step 730, loss = 2.54 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 10:55:48.068547: step 731, loss = 2.68 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:55:50.520831: step 732, loss = 2.86 (52.4 examples/sec; 2.444 sec/batch)
2018-10-15 10:55:52.916649: step 733, loss = 2.75 (53.6 examples/sec; 2.388 sec/batch)
2018-10-15 10:55:55.325273: step 734, loss = 2.74 (53.3 examples/sec; 2.403 sec/batch)
2018-10-15 10:55:57.586550: step 735, loss = 2.82 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 10:55:59.872457: step 736, loss = 3.10 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 10:56:02.265585: step 737, loss = 2.68 (53.6 examples/sec; 2.388 sec/batch)
2018-10-15 10:56:04.649473: step 738, loss = 2.58 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 10:56:06.978302: step 739, loss = 2.51 (55.1 examples/sec; 2.324 sec/batch)
2018-10-15 10:56:09.262637: step 740, loss = 2.55 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 10:56:11.600991: step 741, loss = 2.66 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 10:56:13.974682: step 742, loss = 2.85 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 10:56:16.344362: step 743, loss = 2.70 (54.2 examples/sec; 2.361 sec/batch)
2018-10-15 10:56:18.643985: step 744, loss = 2.74 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 10:56:21.014925: step 745, loss = 2.68 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 10:56:23.324312: step 746, loss = 2.78 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 10:56:25.688011: step 747, loss = 2.60 (54.3 examples/sec; 2.355 sec/batch)
2018-10-15 10:56:28.179172: step 748, loss = 2.76 (51.6 examples/sec; 2.482 sec/batch)
2018-10-15 10:56:30.441699: step 749, loss = 2.69 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 10:56:32.736218: step 750, loss = 2.94 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 10:56:35.113978: step 751, loss = 2.90 (54.0 examples/sec; 2.373 sec/batch)
2018-10-15 10:56:37.340045: step 752, loss = 3.18 (57.7 examples/sec; 2.218 sec/batch)
2018-10-15 10:56:39.647998: step 753, loss = 2.69 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 10:56:41.988629: step 754, loss = 2.78 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 10:56:44.327440: step 755, loss = 2.77 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 10:56:46.735334: step 756, loss = 2.77 (53.3 examples/sec; 2.399 sec/batch)
2018-10-15 10:56:49.109910: step 757, loss = 2.68 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 10:56:51.461820: step 758, loss = 2.69 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 10:56:53.848089: step 759, loss = 2.90 (53.8 examples/sec; 2.381 sec/batch)
2018-10-15 10:56:56.162697: step 760, loss = 2.61 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 10:56:58.551752: step 761, loss = 2.69 (53.8 examples/sec; 2.381 sec/batch)
2018-10-15 10:57:00.829375: step 762, loss = 2.90 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 10:57:03.112166: step 763, loss = 2.71 (56.3 examples/sec; 2.276 sec/batch)
2018-10-15 10:57:05.479211: step 764, loss = 2.90 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 10:57:07.830548: step 765, loss = 2.82 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 10:57:10.221571: step 766, loss = 2.76 (53.7 examples/sec; 2.383 sec/batch)
2018-10-15 10:57:12.494103: step 767, loss = 2.56 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 10:57:14.805665: step 768, loss = 2.70 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 10:57:17.069884: step 769, loss = 2.57 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 10:57:19.453027: step 770, loss = 2.53 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 10:57:21.735346: step 771, loss = 2.58 (56.2 examples/sec; 2.277 sec/batch)
2018-10-15 10:57:24.067183: step 772, loss = 2.73 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 10:57:26.307887: step 773, loss = 2.70 (57.3 examples/sec; 2.233 sec/batch)
2018-10-15 10:57:28.634654: step 774, loss = 2.74 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 10:57:30.982392: step 775, loss = 2.64 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 10:57:33.317036: step 776, loss = 2.57 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 10:57:35.662224: step 777, loss = 2.68 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 10:57:37.975515: step 778, loss = 2.65 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:57:40.275614: step 779, loss = 2.54 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 10:57:42.533202: step 780, loss = 2.56 (56.9 examples/sec; 2.249 sec/batch)
2018-10-15 10:57:44.931152: step 781, loss = 2.70 (53.6 examples/sec; 2.390 sec/batch)
2018-10-15 10:57:47.222841: step 782, loss = 2.68 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:57:49.558884: step 783, loss = 2.56 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 10:57:51.941577: step 784, loss = 2.69 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 10:57:54.245208: step 785, loss = 2.70 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:57:56.555709: step 786, loss = 2.66 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 10:57:58.988154: step 787, loss = 2.73 (52.7 examples/sec; 2.427 sec/batch)
2018-10-15 10:58:01.279678: step 788, loss = 2.75 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 10:58:03.583450: step 789, loss = 2.65 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 10:58:05.919012: step 790, loss = 2.75 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 10:58:08.214593: step 791, loss = 2.47 (55.9 examples/sec; 2.288 sec/batch)
2018-10-15 10:58:10.594713: step 792, loss = 2.71 (54.0 examples/sec; 2.372 sec/batch)
2018-10-15 10:58:12.914719: step 793, loss = 2.92 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 10:58:15.249143: step 794, loss = 2.81 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 10:58:17.541601: step 795, loss = 2.75 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 10:58:19.861032: step 796, loss = 2.86 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 10:58:22.233923: step 797, loss = 2.99 (54.1 examples/sec; 2.368 sec/batch)
2018-10-15 10:58:24.584323: step 798, loss = 2.58 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 10:58:26.862322: step 799, loss = 2.60 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 10:58:29.125181: step 800, loss = 2.67 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 10:58:32.066718: step 801, loss = 2.81 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:58:34.361468: step 802, loss = 2.71 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 10:58:36.727972: step 803, loss = 2.92 (54.2 examples/sec; 2.361 sec/batch)
2018-10-15 10:58:39.081460: step 804, loss = 2.76 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 10:58:41.331339: step 805, loss = 2.96 (57.1 examples/sec; 2.243 sec/batch)
2018-10-15 10:58:43.626882: step 806, loss = 2.81 (55.9 examples/sec; 2.288 sec/batch)
2018-10-15 10:58:45.933706: step 807, loss = 2.73 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:58:48.242629: step 808, loss = 2.59 (55.6 examples/sec; 2.300 sec/batch)
2018-10-15 10:58:50.579870: step 809, loss = 2.77 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:58:52.917643: step 810, loss = 2.99 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 10:58:55.211875: step 811, loss = 2.68 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 10:58:57.524184: step 812, loss = 2.64 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 10:58:59.840656: step 813, loss = 2.58 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 10:59:02.188110: step 814, loss = 2.76 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 10:59:04.430830: step 815, loss = 2.78 (57.2 examples/sec; 2.237 sec/batch)
2018-10-15 10:59:06.685883: step 816, loss = 2.69 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 10:59:08.935398: step 817, loss = 2.84 (57.1 examples/sec; 2.241 sec/batch)
2018-10-15 10:59:11.219497: step 818, loss = 2.61 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 10:59:13.522781: step 819, loss = 2.47 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:59:15.875389: step 820, loss = 2.52 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 10:59:18.312501: step 821, loss = 2.67 (52.6 examples/sec; 2.432 sec/batch)
2018-10-15 10:59:20.587783: step 822, loss = 2.76 (56.5 examples/sec; 2.266 sec/batch)
2018-10-15 10:59:22.964954: step 823, loss = 2.72 (54.0 examples/sec; 2.368 sec/batch)
2018-10-15 10:59:25.336954: step 824, loss = 2.73 (54.2 examples/sec; 2.364 sec/batch)
2018-10-15 10:59:27.672976: step 825, loss = 2.76 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 10:59:30.043038: step 826, loss = 2.62 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 10:59:32.363769: step 827, loss = 2.77 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 10:59:34.649286: step 828, loss = 2.72 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 10:59:36.988452: step 829, loss = 2.86 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 10:59:39.252453: step 830, loss = 2.74 (56.8 examples/sec; 2.255 sec/batch)
2018-10-15 10:59:41.623307: step 831, loss = 2.75 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 10:59:44.037006: step 832, loss = 2.66 (53.2 examples/sec; 2.404 sec/batch)
2018-10-15 10:59:46.328991: step 833, loss = 2.57 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 10:59:48.583599: step 834, loss = 2.61 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 10:59:50.988143: step 835, loss = 2.62 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 10:59:53.332205: step 836, loss = 2.73 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 10:59:55.638926: step 837, loss = 2.75 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 10:59:57.911247: step 838, loss = 2.67 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 11:00:00.151165: step 839, loss = 2.52 (57.3 examples/sec; 2.233 sec/batch)
2018-10-15 11:00:02.417664: step 840, loss = 2.95 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 11:00:04.777572: step 841, loss = 2.50 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 11:00:07.182137: step 842, loss = 2.77 (53.3 examples/sec; 2.399 sec/batch)
2018-10-15 11:00:09.561058: step 843, loss = 2.54 (54.0 examples/sec; 2.371 sec/batch)
2018-10-15 11:00:11.792915: step 844, loss = 2.67 (57.6 examples/sec; 2.224 sec/batch)
2018-10-15 11:00:14.030109: step 845, loss = 2.58 (57.3 examples/sec; 2.232 sec/batch)
2018-10-15 11:00:16.382853: step 846, loss = 2.65 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 11:00:18.701750: step 847, loss = 2.56 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 11:00:21.022510: step 848, loss = 2.69 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 11:00:23.315442: step 849, loss = 2.62 (56.0 examples/sec; 2.285 sec/batch)
2018-10-15 11:00:25.592090: step 850, loss = 2.69 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 11:00:27.897724: step 851, loss = 2.72 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 11:00:30.242600: step 852, loss = 2.68 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 11:00:32.641984: step 853, loss = 2.79 (53.5 examples/sec; 2.391 sec/batch)
2018-10-15 11:00:35.022073: step 854, loss = 2.95 (54.0 examples/sec; 2.372 sec/batch)
2018-10-15 11:00:37.221408: step 855, loss = 2.74 (58.4 examples/sec; 2.191 sec/batch)
2018-10-15 11:00:39.501500: step 856, loss = 2.81 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 11:00:41.834855: step 857, loss = 2.85 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 11:00:44.182754: step 858, loss = 2.64 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 11:00:46.509173: step 859, loss = 2.65 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 11:00:48.809850: step 860, loss = 2.74 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 11:00:51.113549: step 861, loss = 2.66 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 11:00:53.495122: step 862, loss = 2.64 (53.9 examples/sec; 2.373 sec/batch)
2018-10-15 11:00:55.796781: step 863, loss = 2.69 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 11:00:58.165684: step 864, loss = 2.86 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 11:01:00.535563: step 865, loss = 2.81 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 11:01:02.791075: step 866, loss = 2.57 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 11:01:05.124930: step 867, loss = 2.56 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 11:01:07.417908: step 868, loss = 2.76 (56.0 examples/sec; 2.284 sec/batch)
2018-10-15 11:01:09.701630: step 869, loss = 3.12 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 11:01:12.096041: step 870, loss = 2.54 (53.6 examples/sec; 2.387 sec/batch)
2018-10-15 11:01:14.369194: step 871, loss = 2.48 (56.5 examples/sec; 2.265 sec/batch)
2018-10-15 11:01:16.704491: step 872, loss = 2.57 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 11:01:19.003596: step 873, loss = 2.62 (55.9 examples/sec; 2.290 sec/batch)
2018-10-15 11:01:21.286333: step 874, loss = 2.49 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 11:01:23.691587: step 875, loss = 2.71 (53.4 examples/sec; 2.399 sec/batch)
2018-10-15 11:01:26.085991: step 876, loss = 2.56 (53.7 examples/sec; 2.386 sec/batch)
2018-10-15 11:01:28.350242: step 877, loss = 2.48 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 11:01:30.673122: step 878, loss = 2.64 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 11:01:32.961163: step 879, loss = 2.56 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 11:01:35.303752: step 880, loss = 2.79 (54.8 examples/sec; 2.334 sec/batch)
2018-10-15 11:01:37.702718: step 881, loss = 2.45 (53.5 examples/sec; 2.391 sec/batch)
2018-10-15 11:01:39.939040: step 882, loss = 2.61 (57.4 examples/sec; 2.228 sec/batch)
2018-10-15 11:01:42.255138: step 883, loss = 2.54 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 11:01:44.525572: step 884, loss = 2.59 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 11:01:46.843086: step 885, loss = 2.59 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 11:01:49.224422: step 886, loss = 2.58 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 11:01:51.480326: step 887, loss = 2.61 (56.9 examples/sec; 2.248 sec/batch)
2018-10-15 11:01:53.748728: step 888, loss = 2.53 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 11:01:56.075018: step 889, loss = 2.59 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 11:01:58.390688: step 890, loss = 2.58 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 11:02:00.712410: step 891, loss = 2.58 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 11:02:03.063784: step 892, loss = 2.84 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 11:02:05.270052: step 893, loss = 2.65 (58.1 examples/sec; 2.201 sec/batch)
2018-10-15 11:02:07.561660: step 894, loss = 2.67 (56.0 examples/sec; 2.285 sec/batch)
2018-10-15 11:02:09.868151: step 895, loss = 2.85 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 11:02:12.178011: step 896, loss = 2.56 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 11:02:14.455832: step 897, loss = 2.71 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 11:02:16.783194: step 898, loss = 2.70 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 11:02:19.043197: step 899, loss = 2.74 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 11:02:21.391400: step 900, loss = 2.67 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 11:02:24.258209: step 901, loss = 2.44 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 11:02:26.610944: step 902, loss = 2.77 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 11:02:28.937138: step 903, loss = 2.77 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 11:02:31.251797: step 904, loss = 2.80 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 11:02:33.588010: step 905, loss = 2.69 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 11:02:35.910590: step 906, loss = 2.49 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 11:02:38.235232: step 907, loss = 2.79 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 11:02:40.557132: step 908, loss = 2.68 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 11:02:42.834575: step 909, loss = 2.69 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 11:02:45.138649: step 910, loss = 2.60 (55.7 examples/sec; 2.296 sec/batch)
2018-10-15 11:02:47.423605: step 911, loss = 2.89 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 11:02:49.837888: step 912, loss = 2.64 (53.2 examples/sec; 2.406 sec/batch)
2018-10-15 11:02:52.159399: step 913, loss = 2.82 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 11:02:54.584667: step 914, loss = 2.61 (52.9 examples/sec; 2.420 sec/batch)
2018-10-15 11:02:56.821627: step 915, loss = 2.58 (57.3 examples/sec; 2.232 sec/batch)
2018-10-15 11:02:59.077513: step 916, loss = 2.70 (57.0 examples/sec; 2.248 sec/batch)
2018-10-15 11:03:01.359671: step 917, loss = 2.62 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 11:03:03.661882: step 918, loss = 2.81 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 11:03:06.067031: step 919, loss = 2.58 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 11:03:08.384740: step 920, loss = 2.83 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 11:03:10.713937: step 921, loss = 2.88 (55.2 examples/sec; 2.321 sec/batch)
2018-10-15 11:03:13.044784: step 922, loss = 2.97 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 11:03:15.437343: step 923, loss = 2.59 (53.7 examples/sec; 2.383 sec/batch)
2018-10-15 11:03:17.791643: step 924, loss = 2.65 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 11:03:20.073562: step 925, loss = 2.57 (56.2 examples/sec; 2.277 sec/batch)
2018-10-15 11:03:22.343526: step 926, loss = 2.48 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 11:03:24.625316: step 927, loss = 2.72 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 11:03:26.999843: step 928, loss = 2.68 (54.1 examples/sec; 2.366 sec/batch)
2018-10-15 11:03:29.359504: step 929, loss = 2.93 (54.4 examples/sec; 2.351 sec/batch)
2018-10-15 11:03:31.715976: step 930, loss = 2.80 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 11:03:33.975394: step 931, loss = 2.54 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 11:03:36.241564: step 932, loss = 2.67 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 11:03:38.586959: step 933, loss = 2.69 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 11:03:40.919385: step 934, loss = 2.64 (55.1 examples/sec; 2.324 sec/batch)
2018-10-15 11:03:43.276927: step 935, loss = 2.72 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 11:03:45.561327: step 936, loss = 2.59 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 11:03:47.821958: step 937, loss = 2.61 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 11:03:50.129929: step 938, loss = 2.53 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 11:03:52.430953: step 939, loss = 2.67 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 11:03:54.749064: step 940, loss = 2.57 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 11:03:57.120951: step 941, loss = 2.75 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 11:03:59.419830: step 942, loss = 2.54 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 11:04:01.652728: step 943, loss = 2.77 (57.5 examples/sec; 2.224 sec/batch)
2018-10-15 11:04:03.923784: step 944, loss = 2.87 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 11:04:06.374663: step 945, loss = 2.58 (52.3 examples/sec; 2.446 sec/batch)
2018-10-15 11:04:08.705379: step 946, loss = 2.72 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 11:04:11.018906: step 947, loss = 2.56 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 11:04:13.306929: step 948, loss = 2.54 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 11:04:15.662641: step 949, loss = 2.65 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 11:04:17.946445: step 950, loss = 2.64 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 11:04:20.305967: step 951, loss = 2.65 (54.4 examples/sec; 2.351 sec/batch)
2018-10-15 11:04:22.659547: step 952, loss = 2.83 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 11:04:24.924889: step 953, loss = 2.56 (56.6 examples/sec; 2.260 sec/batch)
2018-10-15 11:04:27.181413: step 954, loss = 2.61 (56.9 examples/sec; 2.248 sec/batch)
2018-10-15 11:04:29.481527: step 955, loss = 2.77 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 11:04:31.850372: step 956, loss = 2.70 (54.2 examples/sec; 2.364 sec/batch)
2018-10-15 11:04:34.189170: step 957, loss = 2.56 (54.9 examples/sec; 2.334 sec/batch)
2018-10-15 11:04:36.473562: step 958, loss = 2.75 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 11:04:38.702038: step 959, loss = 2.78 (57.7 examples/sec; 2.220 sec/batch)
2018-10-15 11:04:40.947342: step 960, loss = 2.74 (57.2 examples/sec; 2.237 sec/batch)
2018-10-15 11:04:43.316391: step 961, loss = 2.77 (54.2 examples/sec; 2.361 sec/batch)
2018-10-15 11:04:45.691465: step 962, loss = 2.78 (54.1 examples/sec; 2.366 sec/batch)
2018-10-15 11:04:48.087950: step 963, loss = 2.82 (53.6 examples/sec; 2.388 sec/batch)
2018-10-15 11:04:50.431984: step 964, loss = 2.62 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 11:04:52.716175: step 965, loss = 2.57 (56.3 examples/sec; 2.276 sec/batch)
2018-10-15 11:04:55.083898: step 966, loss = 2.50 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 11:04:57.373958: step 967, loss = 2.70 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 11:04:59.744522: step 968, loss = 2.55 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 11:05:02.098033: step 969, loss = 2.80 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 11:05:04.360214: step 970, loss = 2.66 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 11:05:06.682575: step 971, loss = 2.62 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 11:05:08.995480: step 972, loss = 2.40 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 11:05:11.303565: step 973, loss = 2.79 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 11:05:13.686577: step 974, loss = 2.57 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 11:05:15.994077: step 975, loss = 2.91 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 11:05:18.264734: step 976, loss = 2.52 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 11:05:20.523171: step 977, loss = 2.46 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 11:05:22.786671: step 978, loss = 2.66 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 11:05:25.174368: step 979, loss = 2.57 (53.8 examples/sec; 2.380 sec/batch)
2018-10-15 11:05:27.491257: step 980, loss = 2.59 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 11:05:29.749826: step 981, loss = 2.60 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 11:05:31.989692: step 982, loss = 2.37 (57.3 examples/sec; 2.233 sec/batch)
2018-10-15 11:05:34.254154: step 983, loss = 2.57 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 11:05:36.590357: step 984, loss = 2.52 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 11:05:38.994288: step 985, loss = 2.70 (53.4 examples/sec; 2.399 sec/batch)
2018-10-15 11:05:41.365241: step 986, loss = 2.44 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 11:05:43.737825: step 987, loss = 2.52 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 11:05:45.990704: step 988, loss = 2.61 (57.0 examples/sec; 2.245 sec/batch)
2018-10-15 11:05:48.264113: step 989, loss = 2.52 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 11:05:50.544825: step 990, loss = 2.58 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 11:05:52.877654: step 991, loss = 2.50 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 11:05:55.317162: step 992, loss = 2.57 (52.6 examples/sec; 2.432 sec/batch)
2018-10-15 11:05:57.641710: step 993, loss = 2.64 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 11:05:59.938009: step 994, loss = 2.65 (55.9 examples/sec; 2.288 sec/batch)
2018-10-15 11:06:02.223819: step 995, loss = 2.71 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 11:06:04.714727: step 996, loss = 2.47 (51.6 examples/sec; 2.482 sec/batch)
2018-10-15 11:06:07.034509: step 997, loss = 2.58 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 11:06:09.438531: step 998, loss = 2.56 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 11:06:11.688787: step 999, loss = 2.46 (57.0 examples/sec; 2.244 sec/batch)
2018-10-15 11:06:14.016795: step 1000, loss = 2.64 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 11:06:16.923921: step 1001, loss = 2.51 (54.3 examples/sec; 2.355 sec/batch)
2018-10-15 11:06:19.329709: step 1002, loss = 2.84 (53.4 examples/sec; 2.397 sec/batch)
2018-10-15 11:06:21.716804: step 1003, loss = 2.56 (53.8 examples/sec; 2.379 sec/batch)
2018-10-15 11:06:23.967538: step 1004, loss = 2.51 (57.1 examples/sec; 2.242 sec/batch)
2018-10-15 11:06:26.200256: step 1005, loss = 2.63 (57.6 examples/sec; 2.224 sec/batch)
2018-10-15 11:06:28.498426: step 1006, loss = 2.68 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 11:06:30.918616: step 1007, loss = 2.56 (53.1 examples/sec; 2.412 sec/batch)
2018-10-15 11:06:33.301357: step 1008, loss = 2.75 (53.9 examples/sec; 2.374 sec/batch)
2018-10-15 11:06:35.628001: step 1009, loss = 2.60 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 11:06:37.870225: step 1010, loss = 2.64 (57.3 examples/sec; 2.233 sec/batch)
2018-10-15 11:06:40.216771: step 1011, loss = 2.64 (54.7 examples/sec; 2.341 sec/batch)
2018-10-15 11:06:42.533208: step 1012, loss = 2.57 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 11:06:44.858138: step 1013, loss = 2.56 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 11:06:47.223834: step 1014, loss = 2.83 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 11:06:49.531210: step 1015, loss = 2.46 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 11:06:51.818212: step 1016, loss = 2.74 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 11:06:54.116645: step 1017, loss = 2.48 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 11:06:56.438736: step 1018, loss = 2.76 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 11:06:58.828714: step 1019, loss = 2.55 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 11:07:01.185072: step 1020, loss = 2.55 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 11:07:03.388644: step 1021, loss = 2.47 (58.3 examples/sec; 2.195 sec/batch)
2018-10-15 11:07:05.701819: step 1022, loss = 2.61 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 11:07:08.049740: step 1023, loss = 2.69 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 11:07:10.418175: step 1024, loss = 2.54 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 11:07:12.905475: step 1025, loss = 2.45 (51.6 examples/sec; 2.482 sec/batch)
2018-10-15 11:07:15.209548: step 1026, loss = 2.49 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 11:07:17.520157: step 1027, loss = 2.72 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 11:07:19.824885: step 1028, loss = 2.31 (55.7 examples/sec; 2.296 sec/batch)
2018-10-15 11:07:22.243170: step 1029, loss = 2.57 (53.1 examples/sec; 2.410 sec/batch)
2018-10-15 11:07:24.626258: step 1030, loss = 2.65 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 11:07:26.885924: step 1031, loss = 2.49 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 11:07:29.230213: step 1032, loss = 2.63 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 11:07:31.585635: step 1033, loss = 2.79 (54.5 examples/sec; 2.347 sec/batch)
2018-10-15 11:07:33.933481: step 1034, loss = 2.70 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 11:07:36.384401: step 1035, loss = 2.52 (52.4 examples/sec; 2.442 sec/batch)
2018-10-15 11:07:38.725112: step 1036, loss = 2.73 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 11:07:41.024155: step 1037, loss = 2.79 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 11:07:43.253503: step 1038, loss = 2.52 (57.6 examples/sec; 2.221 sec/batch)
2018-10-15 11:07:45.544569: step 1039, loss = 2.53 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 11:07:47.793106: step 1040, loss = 2.77 (57.1 examples/sec; 2.240 sec/batch)
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 267, in <module>
    benchmark_name=args.dataset, batch_num=args.batch_num, batch_size=args.batch_size)
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 184, in train
    _, loss_value, summary = sess.run([train_op, total_loss, summary_op])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_flowers_data/parallel_read/common_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[{{node flowers_data/parallel_read/common_queue_Dequeue}} = QueueDequeueV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:ps/replica:0/task:0/device:CPU:0"](flowers_data/parallel_read/common_queue)]]
         [[{{node model/conv2/biases/read_S37}} = _Recv[client_terminated=false, recv_device="/job:worker/replica:0/task:0/device:CPU:0", send_device="/job:ps/replica:0/task:0/device:CPU:0", send_device_incarnation=5996376560186214185, tensor_name="edge_685_model/conv2/biases/read", tensor_type=DT_FLOAT, _device="/job:worker/replica:0/task:0/device:CPU:0"]()]]

Caused by op u'flowers_data/parallel_read/common_queue_Dequeue', defined at:
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 267, in <module>
    benchmark_name=args.dataset, batch_num=args.batch_num, batch_size=args.batch_size)
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 135, in train
    images, labels, num_classes = input_data(batch_size, batch_num)
  File "AlexNet/datasets/__init__.py", line 54, in flowers_data
    num_epochs=num_epochs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py", line 96, in __init__
    scope=scope)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py", line 262, in parallel_read
    reader_kwargs=reader_kwargs).read(filename_queue)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py", line 131, in read
    return self._common_queue.dequeue(name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py", line 433, in dequeue
    self._queue_ref, self._dtypes, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 3735, in queue_dequeue_v2
    timeout_ms=timeout_ms, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3272, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

OutOfRangeError (see above for traceback): RandomShuffleQueue '_1_flowers_data/parallel_read/common_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[{{node flowers_data/parallel_read/common_queue_Dequeue}} = QueueDequeueV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:ps/replica:0/task:0/device:CPU:0"](flowers_data/parallel_read/common_queue)]]
         [[{{node model/conv2/biases/read_S37}} = _Recv[client_terminated=false, recv_device="/job:worker/replica:0/task:0/device:CPU:0", send_device="/job:ps/replica:0/task:0/device:CPU:0", send_device_incarnation=5996376560186214185, tensor_name="edge_685_model/conv2/biases/read", tensor_type=DT_FLOAT, _device="/job:worker/replica:0/task:0/device:CPU:0"]()]]
