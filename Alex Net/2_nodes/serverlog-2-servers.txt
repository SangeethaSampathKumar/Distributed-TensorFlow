yan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode cluster --dataset flowers --batch_num 2048
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:ps/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:ps/task:0)
Input batch shape: images: (128, 256, 256, 3) labels: (128,)
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 09:11:21.110498: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 5953ce1ac8322e3c with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 09:11:28.198948: step 0, loss = 4.04 (28.5 examples/sec; 4.492 sec/batch)
2018-10-15 09:11:31.604861: step 1, loss = 5.28 (48.0 examples/sec; 2.665 sec/batch)
2018-10-15 09:11:34.242037: step 2, loss = 6.32 (48.7 examples/sec; 2.630 sec/batch)
2018-10-15 09:11:36.783521: step 3, loss = 7.40 (50.5 examples/sec; 2.534 sec/batch)
2018-10-15 09:11:39.501032: step 4, loss = 6.74 (47.2 examples/sec; 2.713 sec/batch)
2018-10-15 09:11:42.203727: step 5, loss = 8.14 (47.5 examples/sec; 2.695 sec/batch)
2018-10-15 09:11:44.814265: step 6, loss = 6.15 (49.2 examples/sec; 2.604 sec/batch)
2018-10-15 09:11:47.455937: step 7, loss = 5.45 (48.6 examples/sec; 2.635 sec/batch)
2018-10-15 09:11:50.028799: step 8, loss = 5.25 (49.8 examples/sec; 2.568 sec/batch)
2018-10-15 09:11:52.693052: step 9, loss = 7.40 (48.2 examples/sec; 2.656 sec/batch)
2018-10-15 09:11:55.215456: step 10, loss = 7.40 (50.8 examples/sec; 2.519 sec/batch)
2018-10-15 09:11:57.854031: step 11, loss = 3.73 (48.7 examples/sec; 2.629 sec/batch)
2018-10-15 09:12:00.540256: step 12, loss = 4.97 (47.7 examples/sec; 2.683 sec/batch)
2018-10-15 09:12:03.106677: step 13, loss = 5.28 (50.0 examples/sec; 2.561 sec/batch)
2018-10-15 09:12:05.715317: step 14, loss = 4.95 (49.1 examples/sec; 2.604 sec/batch)
2018-10-15 09:12:08.349578: step 15, loss = 4.31 (48.8 examples/sec; 2.625 sec/batch)
2018-10-15 09:12:10.751018: step 16, loss = 4.46 (53.4 examples/sec; 2.397 sec/batch)
2018-10-15 09:12:13.330375: step 17, loss = 4.55 (49.8 examples/sec; 2.570 sec/batch)
2018-10-15 09:12:15.981694: step 18, loss = 4.07 (48.5 examples/sec; 2.642 sec/batch)
2018-10-15 09:12:18.845149: step 19, loss = 4.06 (44.8 examples/sec; 2.856 sec/batch)
2018-10-15 09:12:21.399165: step 20, loss = 3.92 (50.3 examples/sec; 2.546 sec/batch)
2018-10-15 09:12:23.958808: step 21, loss = 3.93 (50.1 examples/sec; 2.555 sec/batch)
2018-10-15 09:12:26.561648: step 22, loss = 4.26 (49.3 examples/sec; 2.595 sec/batch)
2018-10-15 09:12:29.170330: step 23, loss = 3.56 (49.2 examples/sec; 2.603 sec/batch)
2018-10-15 09:12:31.819249: step 24, loss = 4.09 (48.5 examples/sec; 2.640 sec/batch)
2018-10-15 09:12:34.406987: step 25, loss = 4.40 (49.5 examples/sec; 2.583 sec/batch)
2018-10-15 09:12:37.037102: step 26, loss = 4.68 (48.8 examples/sec; 2.621 sec/batch)
2018-10-15 09:12:39.617358: step 27, loss = 3.89 (49.8 examples/sec; 2.572 sec/batch)
2018-10-15 09:12:42.392525: step 28, loss = 3.94 (46.2 examples/sec; 2.768 sec/batch)
2018-10-15 09:12:44.981980: step 29, loss = 4.05 (49.5 examples/sec; 2.584 sec/batch)
2018-10-15 09:12:47.527912: step 30, loss = 3.76 (50.4 examples/sec; 2.539 sec/batch)
2018-10-15 09:12:50.152078: step 31, loss = 3.56 (49.0 examples/sec; 2.610 sec/batch)
2018-10-15 09:12:52.724655: step 32, loss = 3.84 (49.9 examples/sec; 2.567 sec/batch)
2018-10-15 09:12:55.371947: step 33, loss = 4.18 (48.5 examples/sec; 2.642 sec/batch)
2018-10-15 09:12:57.935147: step 34, loss = 3.67 (50.1 examples/sec; 2.557 sec/batch)
2018-10-15 09:13:00.604924: step 35, loss = 3.68 (48.1 examples/sec; 2.659 sec/batch)
2018-10-15 09:13:02.920551: step 36, loss = 3.50 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 09:13:05.145049: step 37, loss = 3.94 (57.8 examples/sec; 2.216 sec/batch)
2018-10-15 09:13:07.399083: step 38, loss = 3.73 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 09:13:09.678802: step 39, loss = 3.53 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 09:13:11.991177: step 40, loss = 3.76 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:13:14.269502: step 41, loss = 4.07 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 09:13:16.531293: step 42, loss = 3.54 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 09:13:18.831064: step 43, loss = 3.58 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 09:13:21.067962: step 44, loss = 3.42 (57.4 examples/sec; 2.229 sec/batch)
2018-10-15 09:13:23.316184: step 45, loss = 3.70 (57.1 examples/sec; 2.240 sec/batch)
2018-10-15 09:13:25.578904: step 46, loss = 3.53 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 09:13:27.827331: step 47, loss = 3.49 (57.1 examples/sec; 2.240 sec/batch)
2018-10-15 09:13:30.063902: step 48, loss = 3.64 (57.4 examples/sec; 2.231 sec/batch)
2018-10-15 09:13:32.293933: step 49, loss = 3.50 (57.6 examples/sec; 2.222 sec/batch)
2018-10-15 09:13:34.527973: step 50, loss = 3.56 (57.5 examples/sec; 2.226 sec/batch)
2018-10-15 09:13:36.811301: step 51, loss = 3.54 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 09:13:39.067686: step 52, loss = 3.53 (56.9 examples/sec; 2.248 sec/batch)
2018-10-15 09:13:41.332432: step 53, loss = 3.74 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 09:13:43.608765: step 54, loss = 3.64 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 09:13:45.894732: step 55, loss = 3.46 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 09:13:48.166128: step 56, loss = 3.65 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:13:50.435546: step 57, loss = 3.75 (56.6 examples/sec; 2.260 sec/batch)
2018-10-15 09:13:52.661062: step 58, loss = 3.51 (57.6 examples/sec; 2.220 sec/batch)
2018-10-15 09:13:54.894521: step 59, loss = 3.63 (57.5 examples/sec; 2.225 sec/batch)
2018-10-15 09:13:57.105923: step 60, loss = 3.50 (58.1 examples/sec; 2.203 sec/batch)
2018-10-15 09:13:59.360292: step 61, loss = 3.68 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 09:14:01.651012: step 62, loss = 3.69 (56.1 examples/sec; 2.282 sec/batch)
2018-10-15 09:14:03.888760: step 63, loss = 3.49 (57.4 examples/sec; 2.230 sec/batch)
2018-10-15 09:14:06.064033: step 64, loss = 3.56 (59.1 examples/sec; 2.166 sec/batch)
2018-10-15 09:14:08.323872: step 65, loss = 3.43 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 09:14:10.595128: step 66, loss = 3.70 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:14:12.811369: step 67, loss = 3.44 (58.0 examples/sec; 2.208 sec/batch)
2018-10-15 09:14:15.047970: step 68, loss = 3.41 (57.4 examples/sec; 2.229 sec/batch)
2018-10-15 09:14:17.391016: step 69, loss = 3.53 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 09:14:19.671774: step 70, loss = 3.47 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 09:14:21.984285: step 71, loss = 3.52 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:14:24.302335: step 72, loss = 3.62 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 09:14:26.549653: step 73, loss = 3.58 (57.2 examples/sec; 2.239 sec/batch)
2018-10-15 09:14:28.791714: step 74, loss = 3.49 (57.3 examples/sec; 2.233 sec/batch)
2018-10-15 09:14:31.065043: step 75, loss = 3.58 (56.5 examples/sec; 2.265 sec/batch)
2018-10-15 09:14:33.306774: step 76, loss = 3.57 (57.3 examples/sec; 2.234 sec/batch)
2018-10-15 09:14:35.546956: step 77, loss = 3.45 (57.3 examples/sec; 2.232 sec/batch)
2018-10-15 09:14:37.882440: step 78, loss = 3.47 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 09:14:40.218790: step 79, loss = 3.39 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 09:14:42.486051: step 80, loss = 3.44 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 09:14:44.767022: step 81, loss = 3.53 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 09:14:46.986596: step 82, loss = 3.38 (57.9 examples/sec; 2.212 sec/batch)
2018-10-15 09:14:49.282105: step 83, loss = 3.67 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:14:51.639263: step 84, loss = 3.45 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 09:14:53.967837: step 85, loss = 3.59 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:14:56.336228: step 86, loss = 3.75 (54.2 examples/sec; 2.361 sec/batch)
2018-10-15 09:14:58.645767: step 87, loss = 3.61 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 09:15:00.969527: step 88, loss = 3.40 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:15:03.281461: step 89, loss = 3.55 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 09:15:05.549230: step 90, loss = 4.03 (56.6 examples/sec; 2.260 sec/batch)
2018-10-15 09:15:07.815777: step 91, loss = 3.58 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 09:15:10.076612: step 92, loss = 3.73 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 09:15:12.393238: step 93, loss = 3.74 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:15:14.701450: step 94, loss = 3.51 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 09:15:17.001331: step 95, loss = 3.51 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 09:15:19.316325: step 96, loss = 3.76 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:15:21.670667: step 97, loss = 3.64 (54.6 examples/sec; 2.346 sec/batch)
2018-10-15 09:15:23.976901: step 98, loss = 3.69 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 09:15:26.290851: step 99, loss = 3.61 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 09:15:28.557734: step 100, loss = 3.57 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 09:15:31.435211: step 101, loss = 3.63 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 09:15:33.686386: step 102, loss = 3.41 (57.1 examples/sec; 2.243 sec/batch)
2018-10-15 09:15:35.991606: step 103, loss = 3.50 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 09:15:38.341730: step 104, loss = 3.52 (54.6 examples/sec; 2.342 sec/batch)
2018-10-15 09:15:40.691763: step 105, loss = 3.57 (54.7 examples/sec; 2.342 sec/batch)
2018-10-15 09:15:43.002529: step 106, loss = 3.62 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 09:15:45.222103: step 107, loss = 3.40 (57.9 examples/sec; 2.211 sec/batch)
2018-10-15 09:15:47.435006: step 108, loss = 3.45 (58.1 examples/sec; 2.205 sec/batch)
2018-10-15 09:15:49.700038: step 109, loss = 3.70 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 09:15:52.021660: step 110, loss = 3.69 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:15:54.343205: step 111, loss = 3.33 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:15:56.654922: step 112, loss = 3.52 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:15:58.920564: step 113, loss = 3.55 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 09:16:01.226353: step 114, loss = 3.49 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 09:16:03.585872: step 115, loss = 3.39 (54.5 examples/sec; 2.351 sec/batch)
2018-10-15 09:16:05.905568: step 116, loss = 3.40 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:16:08.279187: step 117, loss = 3.50 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:16:10.579101: step 118, loss = 3.44 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 09:16:12.869919: step 119, loss = 3.42 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 09:16:15.159971: step 120, loss = 3.42 (56.1 examples/sec; 2.282 sec/batch)
2018-10-15 09:16:17.476477: step 121, loss = 3.31 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:16:19.842991: step 122, loss = 3.36 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 09:16:22.130300: step 123, loss = 3.37 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:16:24.473114: step 124, loss = 3.48 (54.8 examples/sec; 2.334 sec/batch)
2018-10-15 09:16:26.702749: step 125, loss = 3.45 (57.6 examples/sec; 2.222 sec/batch)
2018-10-15 09:16:29.041796: step 126, loss = 3.35 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:16:31.371551: step 127, loss = 3.26 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:16:33.840476: step 128, loss = 3.41 (52.0 examples/sec; 2.460 sec/batch)
2018-10-15 09:16:36.157439: step 129, loss = 3.41 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:16:38.456869: step 130, loss = 3.42 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 09:16:40.735846: step 131, loss = 3.43 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 09:16:43.098949: step 132, loss = 3.36 (54.3 examples/sec; 2.355 sec/batch)
2018-10-15 09:16:45.479662: step 133, loss = 3.30 (54.0 examples/sec; 2.372 sec/batch)
2018-10-15 09:16:47.866356: step 134, loss = 3.32 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 09:16:50.152133: step 135, loss = 3.43 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:16:52.439461: step 136, loss = 3.39 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:16:54.893407: step 137, loss = 3.37 (52.3 examples/sec; 2.445 sec/batch)
2018-10-15 09:16:57.280240: step 138, loss = 3.28 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 09:16:59.724662: step 139, loss = 3.29 (52.6 examples/sec; 2.436 sec/batch)
2018-10-15 09:17:02.001054: step 140, loss = 3.43 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 09:17:04.284061: step 141, loss = 3.29 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:17:06.697819: step 142, loss = 3.22 (53.2 examples/sec; 2.406 sec/batch)
2018-10-15 09:17:08.970766: step 143, loss = 3.26 (56.5 examples/sec; 2.265 sec/batch)
2018-10-15 09:17:11.246560: step 144, loss = 3.39 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 09:17:13.497185: step 145, loss = 3.21 (57.1 examples/sec; 2.242 sec/batch)
2018-10-15 09:17:15.853996: step 146, loss = 3.25 (54.5 examples/sec; 2.349 sec/batch)
2018-10-15 09:17:18.197541: step 147, loss = 3.26 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 09:17:20.547819: step 148, loss = 3.16 (54.7 examples/sec; 2.342 sec/batch)
2018-10-15 09:17:22.940708: step 149, loss = 3.22 (53.7 examples/sec; 2.385 sec/batch)
2018-10-15 09:17:25.307605: step 150, loss = 3.13 (54.2 examples/sec; 2.361 sec/batch)
2018-10-15 09:17:27.624772: step 151, loss = 3.09 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:17:29.980887: step 152, loss = 3.29 (54.4 examples/sec; 2.351 sec/batch)
2018-10-15 09:17:32.348738: step 153, loss = 3.24 (54.2 examples/sec; 2.361 sec/batch)
2018-10-15 09:17:34.665777: step 154, loss = 3.13 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 09:17:37.140301: step 155, loss = 3.19 (51.9 examples/sec; 2.467 sec/batch)
2018-10-15 09:17:39.489025: step 156, loss = 3.16 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 09:17:41.734034: step 157, loss = 3.08 (57.3 examples/sec; 2.234 sec/batch)
2018-10-15 09:17:44.054566: step 158, loss = 3.25 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:17:46.422949: step 159, loss = 3.27 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 09:17:48.795374: step 160, loss = 3.16 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 09:17:51.158030: step 161, loss = 3.32 (54.4 examples/sec; 2.355 sec/batch)
2018-10-15 09:17:53.487497: step 162, loss = 3.28 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 09:17:55.878128: step 163, loss = 3.18 (53.7 examples/sec; 2.384 sec/batch)
2018-10-15 09:17:58.192239: step 164, loss = 3.22 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 09:18:00.479580: step 165, loss = 3.09 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:18:02.750361: step 166, loss = 3.08 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 09:18:04.943333: step 167, loss = 3.20 (58.6 examples/sec; 2.185 sec/batch)
2018-10-15 09:18:07.233199: step 168, loss = 3.04 (56.1 examples/sec; 2.282 sec/batch)
2018-10-15 09:18:09.603943: step 169, loss = 3.21 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 09:18:11.963864: step 170, loss = 3.16 (54.4 examples/sec; 2.355 sec/batch)
2018-10-15 09:18:14.337293: step 171, loss = 3.13 (54.1 examples/sec; 2.368 sec/batch)
2018-10-15 09:18:16.595289: step 172, loss = 3.35 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 09:18:18.985337: step 173, loss = 3.23 (53.7 examples/sec; 2.383 sec/batch)
2018-10-15 09:18:21.314277: step 174, loss = 3.18 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:18:23.681172: step 175, loss = 3.15 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 09:18:26.139224: step 176, loss = 3.10 (52.3 examples/sec; 2.450 sec/batch)
2018-10-15 09:18:28.409466: step 177, loss = 3.13 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 09:18:30.743294: step 178, loss = 2.99 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 09:18:33.106142: step 179, loss = 3.28 (54.4 examples/sec; 2.355 sec/batch)
2018-10-15 09:18:35.461184: step 180, loss = 2.98 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 09:18:37.898217: step 181, loss = 3.24 (52.7 examples/sec; 2.429 sec/batch)
2018-10-15 09:18:40.104380: step 182, loss = 3.06 (58.2 examples/sec; 2.198 sec/batch)
2018-10-15 09:18:42.385736: step 183, loss = 3.07 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 09:18:44.772256: step 184, loss = 3.29 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 09:18:47.067955: step 185, loss = 3.32 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 09:18:49.327437: step 186, loss = 3.18 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 09:18:51.734491: step 187, loss = 3.27 (53.3 examples/sec; 2.402 sec/batch)
2018-10-15 09:18:53.987759: step 188, loss = 3.31 (57.0 examples/sec; 2.244 sec/batch)
2018-10-15 09:18:56.328659: step 189, loss = 3.08 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:18:58.664788: step 190, loss = 3.07 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 09:19:01.051556: step 191, loss = 3.10 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 09:19:03.344916: step 192, loss = 3.16 (56.0 examples/sec; 2.284 sec/batch)
2018-10-15 09:19:05.645425: step 193, loss = 3.15 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 09:19:08.018629: step 194, loss = 3.04 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:19:10.403819: step 195, loss = 3.03 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 09:19:12.664843: step 196, loss = 3.15 (56.8 examples/sec; 2.253 sec/batch)
2018-10-15 09:19:15.011705: step 197, loss = 3.17 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:19:17.267249: step 198, loss = 3.08 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 09:19:19.590041: step 199, loss = 3.14 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 09:19:21.964006: step 200, loss = 3.16 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:19:24.783362: step 201, loss = 3.30 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 09:19:27.127184: step 202, loss = 3.30 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 09:19:29.411825: step 203, loss = 3.19 (56.2 examples/sec; 2.280 sec/batch)
2018-10-15 09:19:31.676900: step 204, loss = 3.19 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 09:19:34.084961: step 205, loss = 3.10 (53.3 examples/sec; 2.399 sec/batch)
2018-10-15 09:19:36.475720: step 206, loss = 3.24 (53.7 examples/sec; 2.383 sec/batch)
2018-10-15 09:19:38.899301: step 207, loss = 3.26 (53.0 examples/sec; 2.415 sec/batch)
2018-10-15 09:19:41.283325: step 208, loss = 3.30 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 09:19:43.687009: step 209, loss = 3.18 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 09:19:46.016767: step 210, loss = 3.16 (55.1 examples/sec; 2.321 sec/batch)
2018-10-15 09:19:48.347620: step 211, loss = 3.27 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 09:19:50.656464: step 212, loss = 3.27 (55.6 examples/sec; 2.300 sec/batch)
2018-10-15 09:19:52.987757: step 213, loss = 2.97 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 09:19:55.294332: step 214, loss = 3.03 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 09:19:57.663354: step 215, loss = 3.23 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 09:20:00.150321: step 216, loss = 3.17 (51.6 examples/sec; 2.478 sec/batch)
2018-10-15 09:20:02.497831: step 217, loss = 3.25 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:20:04.860387: step 218, loss = 3.17 (54.4 examples/sec; 2.354 sec/batch)
2018-10-15 09:20:07.180242: step 219, loss = 3.14 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:20:09.513614: step 220, loss = 2.96 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 09:20:11.963445: step 221, loss = 3.01 (52.4 examples/sec; 2.442 sec/batch)
2018-10-15 09:20:14.315499: step 222, loss = 3.00 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 09:20:16.693291: step 223, loss = 3.09 (54.0 examples/sec; 2.369 sec/batch)
2018-10-15 09:20:18.955624: step 224, loss = 3.14 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 09:20:21.252661: step 225, loss = 3.10 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:20:23.615456: step 226, loss = 3.25 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 09:20:25.973473: step 227, loss = 3.10 (54.4 examples/sec; 2.353 sec/batch)
2018-10-15 09:20:28.367041: step 228, loss = 3.02 (53.7 examples/sec; 2.385 sec/batch)
2018-10-15 09:20:30.707157: step 229, loss = 2.98 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 09:20:33.022302: step 230, loss = 3.13 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:20:35.426261: step 231, loss = 3.25 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 09:20:37.735051: step 232, loss = 3.11 (55.6 examples/sec; 2.300 sec/batch)
2018-10-15 09:20:40.098507: step 233, loss = 3.13 (54.3 examples/sec; 2.355 sec/batch)
2018-10-15 09:20:42.420959: step 234, loss = 3.27 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 09:20:44.758301: step 235, loss = 3.17 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 09:20:47.142119: step 236, loss = 3.13 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 09:20:49.448794: step 237, loss = 3.12 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 09:20:51.817907: step 238, loss = 3.27 (54.2 examples/sec; 2.361 sec/batch)
2018-10-15 09:20:54.102611: step 239, loss = 2.94 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 09:20:56.415905: step 240, loss = 2.94 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:20:58.867050: step 241, loss = 3.08 (52.4 examples/sec; 2.443 sec/batch)
2018-10-15 09:21:01.172145: step 242, loss = 3.15 (55.7 examples/sec; 2.300 sec/batch)
2018-10-15 09:21:03.536124: step 243, loss = 2.95 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:21:05.881092: step 244, loss = 3.19 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 09:21:08.274704: step 245, loss = 2.96 (53.6 examples/sec; 2.388 sec/batch)
2018-10-15 09:21:10.575690: step 246, loss = 3.00 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 09:21:12.865030: step 247, loss = 3.03 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 09:21:15.209217: step 248, loss = 2.98 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 09:21:17.536037: step 249, loss = 2.94 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 09:21:19.822976: step 250, loss = 3.01 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:21:22.149990: step 251, loss = 3.05 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:21:24.532802: step 252, loss = 3.09 (53.9 examples/sec; 2.374 sec/batch)
2018-10-15 09:21:26.967408: step 253, loss = 2.73 (52.7 examples/sec; 2.429 sec/batch)
2018-10-15 09:21:29.272722: step 254, loss = 3.03 (55.7 examples/sec; 2.296 sec/batch)
2018-10-15 09:21:31.585786: step 255, loss = 3.18 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:21:33.976403: step 256, loss = 2.94 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 09:21:36.288956: step 257, loss = 2.94 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:21:38.615074: step 258, loss = 3.10 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 09:21:40.969583: step 259, loss = 2.88 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 09:21:43.282485: step 260, loss = 3.18 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:21:45.630855: step 261, loss = 3.02 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 09:21:48.017349: step 262, loss = 3.07 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 09:21:50.345160: step 263, loss = 3.01 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 09:21:52.712861: step 264, loss = 3.09 (54.2 examples/sec; 2.359 sec/batch)
2018-10-15 09:21:55.039440: step 265, loss = 2.86 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 09:21:57.378263: step 266, loss = 3.36 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 09:21:59.821467: step 267, loss = 3.23 (52.6 examples/sec; 2.435 sec/batch)
2018-10-15 09:22:02.149314: step 268, loss = 2.90 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 09:22:04.546879: step 269, loss = 2.92 (53.6 examples/sec; 2.389 sec/batch)
2018-10-15 09:22:06.890891: step 270, loss = 3.29 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 09:22:09.215928: step 271, loss = 3.02 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:22:11.562446: step 272, loss = 3.10 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 09:22:13.944422: step 273, loss = 3.45 (53.9 examples/sec; 2.374 sec/batch)
2018-10-15 09:22:16.288432: step 274, loss = 3.27 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 09:22:18.648216: step 275, loss = 2.89 (54.4 examples/sec; 2.355 sec/batch)
2018-10-15 09:22:21.054748: step 276, loss = 3.21 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 09:22:23.434015: step 277, loss = 3.20 (54.0 examples/sec; 2.371 sec/batch)
2018-10-15 09:22:25.767431: step 278, loss = 3.25 (55.0 examples/sec; 2.325 sec/batch)
2018-10-15 09:22:28.086496: step 279, loss = 3.17 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:22:30.340630: step 280, loss = 3.17 (56.9 examples/sec; 2.249 sec/batch)
2018-10-15 09:22:32.639699: step 281, loss = 3.00 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:22:35.047096: step 282, loss = 3.08 (53.4 examples/sec; 2.399 sec/batch)
2018-10-15 09:22:37.370262: step 283, loss = 2.92 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 09:22:39.669948: step 284, loss = 3.08 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 09:22:42.021474: step 285, loss = 2.87 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:22:44.466685: step 286, loss = 3.04 (52.5 examples/sec; 2.437 sec/batch)
2018-10-15 09:22:46.855553: step 287, loss = 2.99 (53.8 examples/sec; 2.380 sec/batch)
2018-10-15 09:22:49.197526: step 288, loss = 3.08 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 09:22:51.501328: step 289, loss = 2.99 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 09:22:53.885897: step 290, loss = 3.02 (53.8 examples/sec; 2.379 sec/batch)
2018-10-15 09:22:56.208581: step 291, loss = 3.17 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 09:22:58.517328: step 292, loss = 3.06 (55.7 examples/sec; 2.300 sec/batch)
2018-10-15 09:23:00.784900: step 293, loss = 3.01 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 09:23:03.043098: step 294, loss = 2.91 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 09:23:05.370601: step 295, loss = 3.12 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:23:07.773525: step 296, loss = 3.09 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 09:23:10.102020: step 297, loss = 3.09 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 09:23:12.448039: step 298, loss = 2.99 (54.8 examples/sec; 2.338 sec/batch)
2018-10-15 09:23:14.768453: step 299, loss = 3.11 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:23:17.050035: step 300, loss = 3.04 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 09:23:19.905218: step 301, loss = 2.87 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:23:22.200478: step 302, loss = 2.97 (55.9 examples/sec; 2.290 sec/batch)
2018-10-15 09:23:24.457590: step 303, loss = 2.95 (56.9 examples/sec; 2.249 sec/batch)
2018-10-15 09:23:26.714478: step 304, loss = 3.05 (56.8 examples/sec; 2.253 sec/batch)
2018-10-15 09:23:29.040027: step 305, loss = 3.09 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 09:23:31.376977: step 306, loss = 3.14 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 09:23:33.745075: step 307, loss = 3.03 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 09:23:36.090117: step 308, loss = 3.06 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 09:23:38.384328: step 309, loss = 3.09 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 09:23:40.637442: step 310, loss = 3.10 (57.0 examples/sec; 2.245 sec/batch)
2018-10-15 09:23:42.923269: step 311, loss = 3.27 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:23:45.262649: step 312, loss = 3.02 (54.8 examples/sec; 2.334 sec/batch)
2018-10-15 09:23:47.655043: step 313, loss = 2.95 (53.7 examples/sec; 2.385 sec/batch)
2018-10-15 09:23:49.966757: step 314, loss = 2.98 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:23:52.224766: step 315, loss = 2.94 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 09:23:54.536507: step 316, loss = 3.08 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:23:56.855145: step 317, loss = 3.16 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:23:59.175949: step 318, loss = 3.11 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:24:01.511816: step 319, loss = 3.14 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 09:24:03.762313: step 320, loss = 2.98 (57.1 examples/sec; 2.241 sec/batch)
2018-10-15 09:24:06.062276: step 321, loss = 3.32 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 09:24:08.478096: step 322, loss = 2.94 (53.2 examples/sec; 2.407 sec/batch)
2018-10-15 09:24:10.764159: step 323, loss = 2.98 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 09:24:13.086174: step 324, loss = 2.81 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:24:15.456746: step 325, loss = 3.02 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 09:24:17.728709: step 326, loss = 2.90 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 09:24:20.089645: step 327, loss = 3.11 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 09:24:22.348777: step 328, loss = 3.00 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 09:24:24.663588: step 329, loss = 3.13 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 09:24:26.953650: step 330, loss = 3.22 (56.0 examples/sec; 2.285 sec/batch)
2018-10-15 09:24:29.195377: step 331, loss = 3.27 (57.3 examples/sec; 2.233 sec/batch)
2018-10-15 09:24:31.542624: step 332, loss = 3.05 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:24:33.813363: step 333, loss = 3.23 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 09:24:36.160238: step 334, loss = 3.15 (54.8 examples/sec; 2.338 sec/batch)
2018-10-15 09:24:38.426028: step 335, loss = 2.83 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 09:24:40.717925: step 336, loss = 3.14 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 09:24:43.082818: step 337, loss = 3.06 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:24:45.398536: step 338, loss = 3.01 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:24:47.665478: step 339, loss = 3.05 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 09:24:49.917390: step 340, loss = 2.99 (57.1 examples/sec; 2.243 sec/batch)
2018-10-15 09:24:52.240337: step 341, loss = 2.92 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 09:24:54.539895: step 342, loss = 2.94 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 09:24:56.862024: step 343, loss = 2.88 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 09:24:59.188799: step 344, loss = 2.88 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 09:25:01.518964: step 345, loss = 2.94 (55.0 examples/sec; 2.325 sec/batch)
2018-10-15 09:25:03.799554: step 346, loss = 2.90 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 09:25:06.079292: step 347, loss = 3.09 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 09:25:08.356589: step 348, loss = 3.10 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 09:25:10.685878: step 349, loss = 2.99 (55.1 examples/sec; 2.321 sec/batch)
2018-10-15 09:25:12.989765: step 350, loss = 2.99 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 09:25:15.247074: step 351, loss = 2.88 (56.9 examples/sec; 2.249 sec/batch)
2018-10-15 09:25:17.526461: step 352, loss = 3.05 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 09:25:19.964165: step 353, loss = 2.87 (52.7 examples/sec; 2.430 sec/batch)
2018-10-15 09:25:22.300831: step 354, loss = 3.09 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 09:25:24.687394: step 355, loss = 2.83 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 09:25:26.993398: step 356, loss = 3.05 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 09:25:29.262308: step 357, loss = 3.07 (56.6 examples/sec; 2.260 sec/batch)
2018-10-15 09:25:31.564605: step 358, loss = 2.92 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 09:25:33.962109: step 359, loss = 2.98 (53.6 examples/sec; 2.389 sec/batch)
2018-10-15 09:25:36.255944: step 360, loss = 2.95 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:25:38.617386: step 361, loss = 2.89 (54.4 examples/sec; 2.353 sec/batch)
2018-10-15 09:25:40.855281: step 362, loss = 2.79 (57.4 examples/sec; 2.230 sec/batch)
2018-10-15 09:25:43.214377: step 363, loss = 2.81 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 09:25:45.499339: step 364, loss = 2.97 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 09:25:47.805636: step 365, loss = 2.98 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 09:25:50.171200: step 366, loss = 2.97 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 09:25:52.436758: step 367, loss = 2.89 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 09:25:54.720498: step 368, loss = 2.91 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:25:57.033394: step 369, loss = 2.87 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:25:59.275644: step 370, loss = 2.92 (57.3 examples/sec; 2.234 sec/batch)
2018-10-15 09:26:01.647106: step 371, loss = 3.14 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 09:26:03.926070: step 372, loss = 3.20 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 09:26:06.243254: step 373, loss = 2.94 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:26:08.526130: step 374, loss = 3.01 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 09:26:10.835042: step 375, loss = 2.87 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:26:13.207083: step 376, loss = 3.06 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 09:26:15.526404: step 377, loss = 3.09 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:26:17.850497: step 378, loss = 3.15 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:26:20.165564: step 379, loss = 3.01 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:26:22.538653: step 380, loss = 2.95 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:26:24.865120: step 381, loss = 3.14 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 09:26:27.127143: step 382, loss = 2.95 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 09:26:29.451655: step 383, loss = 3.04 (55.2 examples/sec; 2.319 sec/batch)
2018-10-15 09:26:31.863188: step 384, loss = 3.04 (53.1 examples/sec; 2.409 sec/batch)
2018-10-15 09:26:34.240850: step 385, loss = 2.91 (54.0 examples/sec; 2.369 sec/batch)
2018-10-15 09:26:36.598689: step 386, loss = 2.85 (54.5 examples/sec; 2.349 sec/batch)
2018-10-15 09:26:38.864177: step 387, loss = 2.91 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 09:26:41.177102: step 388, loss = 2.85 (55.5 examples/sec; 2.304 sec/batch)
2018-10-15 09:26:43.466467: step 389, loss = 2.94 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:26:45.790369: step 390, loss = 2.88 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:26:48.145838: step 391, loss = 2.98 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 09:26:50.465985: step 392, loss = 2.98 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:26:52.762434: step 393, loss = 2.93 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:26:55.211303: step 394, loss = 2.90 (52.5 examples/sec; 2.440 sec/batch)
2018-10-15 09:26:57.488463: step 395, loss = 3.04 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 09:26:59.918613: step 396, loss = 2.99 (52.8 examples/sec; 2.422 sec/batch)
2018-10-15 09:27:02.282060: step 397, loss = 3.26 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 09:27:04.537025: step 398, loss = 2.81 (57.0 examples/sec; 2.247 sec/batch)
2018-10-15 09:27:06.839013: step 399, loss = 3.11 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 09:27:09.256757: step 400, loss = 2.89 (53.1 examples/sec; 2.409 sec/batch)
2018-10-15 09:27:12.358058: step 401, loss = 3.06 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 09:27:14.632859: step 402, loss = 2.81 (56.5 examples/sec; 2.266 sec/batch)
2018-10-15 09:27:17.016008: step 403, loss = 3.06 (56.0 examples/sec; 2.288 sec/batch)
2018-10-15 09:27:19.303359: step 404, loss = 3.21 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:27:21.706975: step 405, loss = 3.23 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 09:27:24.089834: step 406, loss = 2.88 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 09:27:26.512220: step 407, loss = 2.91 (53.0 examples/sec; 2.415 sec/batch)
2018-10-15 09:27:28.905697: step 408, loss = 3.16 (53.7 examples/sec; 2.385 sec/batch)
2018-10-15 09:27:31.176851: step 409, loss = 3.12 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:27:33.565658: step 410, loss = 3.21 (53.8 examples/sec; 2.380 sec/batch)
2018-10-15 09:27:35.874517: step 411, loss = 3.25 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 09:27:38.195131: step 412, loss = 3.03 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:27:40.555094: step 413, loss = 2.96 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 09:27:42.843563: step 414, loss = 3.18 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 09:27:45.179229: step 415, loss = 3.32 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 09:27:47.569304: step 416, loss = 3.05 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 09:27:49.946111: step 417, loss = 2.93 (54.0 examples/sec; 2.369 sec/batch)
2018-10-15 09:27:52.278025: step 418, loss = 3.12 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 09:27:54.602991: step 419, loss = 3.05 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 09:27:56.982672: step 420, loss = 3.11 (54.0 examples/sec; 2.372 sec/batch)
2018-10-15 09:27:59.321079: step 421, loss = 3.01 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 09:28:01.659783: step 422, loss = 2.83 (54.9 examples/sec; 2.330 sec/batch)
2018-10-15 09:28:04.037272: step 423, loss = 2.85 (54.0 examples/sec; 2.369 sec/batch)
2018-10-15 09:28:06.326527: step 424, loss = 3.11 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:28:08.609256: step 425, loss = 2.93 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:28:10.974215: step 426, loss = 2.94 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:28:13.338762: step 427, loss = 3.06 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:28:15.705973: step 428, loss = 2.73 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 09:28:18.087698: step 429, loss = 2.84 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 09:28:20.404183: step 430, loss = 2.78 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:28:22.691399: step 431, loss = 3.03 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:28:24.958028: step 432, loss = 3.06 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 09:28:27.337432: step 433, loss = 2.89 (53.9 examples/sec; 2.373 sec/batch)
2018-10-15 09:28:29.636458: step 434, loss = 3.19 (55.8 examples/sec; 2.292 sec/batch)
2018-10-15 09:28:31.917915: step 435, loss = 3.06 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:28:34.271798: step 436, loss = 3.09 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 09:28:36.648710: step 437, loss = 2.97 (54.0 examples/sec; 2.369 sec/batch)
2018-10-15 09:28:38.920706: step 438, loss = 3.29 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:28:41.245979: step 439, loss = 3.01 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 09:28:43.532265: step 440, loss = 3.06 (56.2 examples/sec; 2.277 sec/batch)
2018-10-15 09:28:45.803726: step 441, loss = 3.37 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:28:48.201543: step 442, loss = 3.31 (53.5 examples/sec; 2.393 sec/batch)
2018-10-15 09:28:50.473704: step 443, loss = 2.93 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 09:28:52.804483: step 444, loss = 3.12 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:28:55.058803: step 445, loss = 3.05 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 09:28:57.334382: step 446, loss = 3.31 (56.5 examples/sec; 2.267 sec/batch)
2018-10-15 09:28:59.654365: step 447, loss = 3.20 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 09:29:01.949727: step 448, loss = 2.97 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 09:29:04.306823: step 449, loss = 3.26 (54.4 examples/sec; 2.352 sec/batch)
2018-10-15 09:29:06.573875: step 450, loss = 3.18 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 09:29:08.904588: step 451, loss = 2.96 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:29:11.233354: step 452, loss = 3.06 (55.2 examples/sec; 2.321 sec/batch)
2018-10-15 09:29:13.637310: step 453, loss = 3.21 (53.4 examples/sec; 2.395 sec/batch)
2018-10-15 09:29:16.009885: step 454, loss = 3.02 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 09:29:18.302080: step 455, loss = 2.99 (56.1 examples/sec; 2.284 sec/batch)
2018-10-15 09:29:20.642102: step 456, loss = 2.92 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:29:23.056033: step 457, loss = 2.86 (53.2 examples/sec; 2.405 sec/batch)
2018-10-15 09:29:25.547477: step 458, loss = 2.90 (51.4 examples/sec; 2.488 sec/batch)
2018-10-15 09:29:27.898007: step 459, loss = 2.94 (54.7 examples/sec; 2.341 sec/batch)
2018-10-15 09:29:30.183349: step 460, loss = 2.94 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 09:29:32.494328: step 461, loss = 2.85 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 09:29:34.742756: step 462, loss = 2.79 (57.1 examples/sec; 2.243 sec/batch)
2018-10-15 09:29:37.194458: step 463, loss = 3.01 (52.3 examples/sec; 2.447 sec/batch)
2018-10-15 09:29:39.498306: step 464, loss = 2.80 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 09:29:41.812694: step 465, loss = 2.85 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 09:29:44.140060: step 466, loss = 2.85 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:29:46.480345: step 467, loss = 2.91 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 09:29:48.780980: step 468, loss = 2.86 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 09:29:51.179076: step 469, loss = 2.78 (53.6 examples/sec; 2.390 sec/batch)
2018-10-15 09:29:53.511907: step 470, loss = 2.98 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 09:29:55.822407: step 471, loss = 3.00 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 09:29:58.182507: step 472, loss = 3.07 (54.4 examples/sec; 2.355 sec/batch)
2018-10-15 09:30:00.496445: step 473, loss = 2.93 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:30:02.823514: step 474, loss = 3.00 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:30:05.275528: step 475, loss = 2.91 (52.4 examples/sec; 2.444 sec/batch)
2018-10-15 09:30:07.594159: step 476, loss = 3.04 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:30:09.895314: step 477, loss = 2.95 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 09:30:12.203368: step 478, loss = 2.80 (55.7 examples/sec; 2.300 sec/batch)
2018-10-15 09:30:14.565224: step 479, loss = 2.87 (54.4 examples/sec; 2.354 sec/batch)
2018-10-15 09:30:16.890248: step 480, loss = 3.24 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 09:30:19.186001: step 481, loss = 2.86 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 09:30:21.435234: step 482, loss = 3.07 (57.1 examples/sec; 2.241 sec/batch)
2018-10-15 09:30:23.719291: step 483, loss = 3.00 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 09:30:26.075636: step 484, loss = 2.97 (54.4 examples/sec; 2.351 sec/batch)
2018-10-15 09:30:28.470033: step 485, loss = 2.91 (53.6 examples/sec; 2.386 sec/batch)
2018-10-15 09:30:30.725901: step 486, loss = 2.90 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 09:30:33.062694: step 487, loss = 2.96 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 09:30:35.483852: step 488, loss = 2.76 (53.1 examples/sec; 2.412 sec/batch)
2018-10-15 09:30:37.789656: step 489, loss = 2.88 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 09:30:40.088039: step 490, loss = 2.98 (55.9 examples/sec; 2.290 sec/batch)
2018-10-15 09:30:42.507932: step 491, loss = 2.78 (53.1 examples/sec; 2.412 sec/batch)
2018-10-15 09:30:44.861351: step 492, loss = 2.88 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 09:30:47.186286: step 493, loss = 2.90 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 09:30:49.539420: step 494, loss = 2.92 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 09:30:51.819978: step 495, loss = 2.77 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 09:30:54.191105: step 496, loss = 2.81 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 09:30:56.497677: step 497, loss = 2.81 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 09:30:58.787098: step 498, loss = 2.80 (56.1 examples/sec; 2.282 sec/batch)
2018-10-15 09:31:01.139506: step 499, loss = 2.90 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 09:31:03.406216: step 500, loss = 2.88 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 09:31:06.318385: step 501, loss = 2.79 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:31:08.674362: step 502, loss = 2.98 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 09:31:11.044707: step 503, loss = 2.87 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:31:13.406292: step 504, loss = 2.86 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:31:15.713592: step 505, loss = 2.91 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 09:31:18.084876: step 506, loss = 2.80 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 09:31:20.457987: step 507, loss = 2.75 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:31:22.770726: step 508, loss = 2.84 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:31:25.121688: step 509, loss = 2.82 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:31:27.450073: step 510, loss = 2.69 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:31:29.891895: step 511, loss = 2.69 (52.6 examples/sec; 2.433 sec/batch)
2018-10-15 09:31:32.204943: step 512, loss = 2.91 (55.5 examples/sec; 2.304 sec/batch)
2018-10-15 09:31:34.552504: step 513, loss = 3.03 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:31:36.812809: step 514, loss = 3.07 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 09:31:39.127885: step 515, loss = 2.78 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:31:41.495033: step 516, loss = 2.86 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 09:31:43.841889: step 517, loss = 3.00 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:31:46.185631: step 518, loss = 2.86 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 09:31:48.479879: step 519, loss = 2.93 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 09:31:50.776150: step 520, loss = 2.91 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:31:53.182766: step 521, loss = 2.87 (53.4 examples/sec; 2.399 sec/batch)
2018-10-15 09:31:55.612457: step 522, loss = 2.89 (52.9 examples/sec; 2.421 sec/batch)
2018-10-15 09:31:57.835898: step 523, loss = 2.92 (57.7 examples/sec; 2.218 sec/batch)
2018-10-15 09:32:00.117968: step 524, loss = 2.95 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:32:02.471427: step 525, loss = 2.81 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 09:32:04.801881: step 526, loss = 2.66 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:32:07.119526: step 527, loss = 2.90 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:32:09.376051: step 528, loss = 2.83 (56.9 examples/sec; 2.249 sec/batch)
2018-10-15 09:32:11.662999: step 529, loss = 2.99 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:32:13.974723: step 530, loss = 2.85 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:32:16.270243: step 531, loss = 2.85 (55.9 examples/sec; 2.290 sec/batch)
2018-10-15 09:32:18.603193: step 532, loss = 2.75 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 09:32:20.919573: step 533, loss = 2.86 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:32:23.214080: step 534, loss = 3.02 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 09:32:25.511123: step 535, loss = 2.72 (55.9 examples/sec; 2.288 sec/batch)
2018-10-15 09:32:27.814892: step 536, loss = 2.84 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 09:32:30.197400: step 537, loss = 3.08 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 09:32:32.504758: step 538, loss = 2.89 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 09:32:34.756455: step 539, loss = 2.70 (57.0 examples/sec; 2.247 sec/batch)
2018-10-15 09:32:37.120480: step 540, loss = 2.96 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:32:39.432986: step 541, loss = 2.94 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:32:41.735661: step 542, loss = 3.06 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 09:32:44.036651: step 543, loss = 2.91 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 09:32:46.357109: step 544, loss = 2.93 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:32:48.724184: step 545, loss = 3.25 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:32:51.082136: step 546, loss = 2.94 (54.5 examples/sec; 2.349 sec/batch)
2018-10-15 09:32:53.466966: step 547, loss = 2.88 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 09:32:55.847339: step 548, loss = 2.80 (54.0 examples/sec; 2.372 sec/batch)
2018-10-15 09:32:58.153458: step 549, loss = 2.78 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 09:33:00.487294: step 550, loss = 2.92 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 09:33:02.883849: step 551, loss = 2.87 (53.6 examples/sec; 2.388 sec/batch)
2018-10-15 09:33:05.217391: step 552, loss = 2.92 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 09:33:07.603313: step 553, loss = 2.80 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 09:33:09.899051: step 554, loss = 2.87 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 09:33:12.179254: step 555, loss = 2.76 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 09:33:14.494991: step 556, loss = 2.95 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:33:16.877206: step 557, loss = 2.82 (53.9 examples/sec; 2.374 sec/batch)
2018-10-15 09:33:19.106700: step 558, loss = 2.87 (57.6 examples/sec; 2.221 sec/batch)
2018-10-15 09:33:21.515573: step 559, loss = 2.87 (53.3 examples/sec; 2.404 sec/batch)
2018-10-15 09:33:23.826961: step 560, loss = 2.61 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 09:33:26.121347: step 561, loss = 2.70 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 09:33:28.495983: step 562, loss = 2.83 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 09:33:30.752436: step 563, loss = 2.79 (56.9 examples/sec; 2.248 sec/batch)
2018-10-15 09:33:33.057712: step 564, loss = 2.68 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 09:33:35.351918: step 565, loss = 2.74 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:33:37.684022: step 566, loss = 2.78 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 09:33:40.037377: step 567, loss = 2.81 (54.5 examples/sec; 2.346 sec/batch)
2018-10-15 09:33:42.431043: step 568, loss = 2.80 (53.7 examples/sec; 2.385 sec/batch)
2018-10-15 09:33:44.825242: step 569, loss = 2.98 (53.6 examples/sec; 2.386 sec/batch)
2018-10-15 09:33:47.203068: step 570, loss = 2.73 (54.0 examples/sec; 2.370 sec/batch)
2018-10-15 09:33:49.488809: step 571, loss = 2.79 (56.2 examples/sec; 2.277 sec/batch)
2018-10-15 09:33:51.854946: step 572, loss = 2.58 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 09:33:54.172262: step 573, loss = 2.67 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 09:33:56.484104: step 574, loss = 2.76 (55.6 examples/sec; 2.303 sec/batch)
2018-10-15 09:33:58.898845: step 575, loss = 2.91 (53.2 examples/sec; 2.406 sec/batch)
2018-10-15 09:34:01.158721: step 576, loss = 2.89 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 09:34:03.531268: step 577, loss = 2.68 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 09:34:05.887982: step 578, loss = 2.79 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 09:34:08.301337: step 579, loss = 2.84 (53.2 examples/sec; 2.406 sec/batch)
2018-10-15 09:34:10.708128: step 580, loss = 2.83 (53.4 examples/sec; 2.399 sec/batch)
2018-10-15 09:34:13.075548: step 581, loss = 3.19 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 09:34:15.428172: step 582, loss = 2.85 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 09:34:17.831182: step 583, loss = 2.76 (53.5 examples/sec; 2.394 sec/batch)
2018-10-15 09:34:20.170948: step 584, loss = 2.74 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:34:22.547900: step 585, loss = 2.96 (54.0 examples/sec; 2.369 sec/batch)
2018-10-15 09:34:24.967550: step 586, loss = 2.99 (53.1 examples/sec; 2.412 sec/batch)
2018-10-15 09:34:27.333315: step 587, loss = 2.78 (54.3 examples/sec; 2.357 sec/batch)
2018-10-15 09:34:29.716992: step 588, loss = 2.70 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 09:34:32.014999: step 589, loss = 2.79 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:34:34.346671: step 590, loss = 2.86 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 09:34:36.689455: step 591, loss = 2.97 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 09:34:39.014034: step 592, loss = 2.83 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:34:41.274306: step 593, loss = 2.75 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 09:34:43.607506: step 594, loss = 2.99 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 09:34:45.999846: step 595, loss = 2.79 (53.7 examples/sec; 2.386 sec/batch)
2018-10-15 09:34:48.315384: step 596, loss = 2.97 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 09:34:50.645059: step 597, loss = 2.83 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:34:53.023034: step 598, loss = 2.86 (54.0 examples/sec; 2.370 sec/batch)
2018-10-15 09:34:55.278306: step 599, loss = 2.67 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 09:34:57.725529: step 600, loss = 2.56 (52.5 examples/sec; 2.439 sec/batch)
2018-10-15 09:35:00.513039: step 601, loss = 2.82 (57.4 examples/sec; 2.232 sec/batch)
2018-10-15 09:35:02.788870: step 602, loss = 2.84 (56.5 examples/sec; 2.267 sec/batch)
2018-10-15 09:35:05.163297: step 603, loss = 2.85 (54.1 examples/sec; 2.366 sec/batch)
2018-10-15 09:35:07.512592: step 604, loss = 2.93 (54.7 examples/sec; 2.341 sec/batch)
2018-10-15 09:35:09.877187: step 605, loss = 2.70 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:35:12.315840: step 606, loss = 2.85 (52.7 examples/sec; 2.430 sec/batch)
2018-10-15 09:35:14.682267: step 607, loss = 2.65 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 09:35:17.016042: step 608, loss = 2.80 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 09:35:19.414883: step 609, loss = 2.81 (53.6 examples/sec; 2.390 sec/batch)
2018-10-15 09:35:21.773880: step 610, loss = 2.79 (54.4 examples/sec; 2.354 sec/batch)
2018-10-15 09:35:24.086251: step 611, loss = 2.74 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:35:26.426870: step 612, loss = 3.09 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 09:35:28.719033: step 613, loss = 2.84 (56.0 examples/sec; 2.284 sec/batch)
2018-10-15 09:35:31.072682: step 614, loss = 2.84 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 09:35:33.504875: step 615, loss = 2.94 (52.8 examples/sec; 2.423 sec/batch)
2018-10-15 09:35:35.860625: step 616, loss = 2.95 (54.5 examples/sec; 2.347 sec/batch)
2018-10-15 09:35:38.191084: step 617, loss = 2.96 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:35:40.497971: step 618, loss = 2.75 (55.6 examples/sec; 2.300 sec/batch)
2018-10-15 09:35:42.865293: step 619, loss = 2.81 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 09:35:45.241780: step 620, loss = 2.95 (54.1 examples/sec; 2.368 sec/batch)
2018-10-15 09:35:47.563740: step 621, loss = 2.83 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:35:49.827536: step 622, loss = 3.14 (56.8 examples/sec; 2.255 sec/batch)
2018-10-15 09:35:52.146147: step 623, loss = 3.00 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:35:54.470507: step 624, loss = 2.83 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:35:56.900315: step 625, loss = 2.80 (52.9 examples/sec; 2.422 sec/batch)
2018-10-15 09:35:59.221091: step 626, loss = 2.97 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:36:01.555563: step 627, loss = 2.84 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 09:36:03.833537: step 628, loss = 2.85 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 09:36:06.184561: step 629, loss = 2.83 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:36:08.558974: step 630, loss = 2.94 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 09:36:10.921765: step 631, loss = 2.79 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 09:36:13.215810: step 632, loss = 2.99 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:36:15.490905: step 633, loss = 3.12 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 09:36:17.841552: step 634, loss = 2.86 (54.6 examples/sec; 2.342 sec/batch)
2018-10-15 09:36:20.149236: step 635, loss = 2.79 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 09:36:22.456581: step 636, loss = 2.74 (55.5 examples/sec; 2.304 sec/batch)
2018-10-15 09:36:24.871114: step 637, loss = 2.82 (53.2 examples/sec; 2.406 sec/batch)
2018-10-15 09:36:27.168250: step 638, loss = 2.80 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:36:29.460151: step 639, loss = 2.89 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 09:36:31.733790: step 640, loss = 2.78 (56.5 examples/sec; 2.267 sec/batch)
2018-10-15 09:36:33.998516: step 641, loss = 2.84 (56.7 examples/sec; 2.257 sec/batch)
2018-10-15 09:36:36.316889: step 642, loss = 2.79 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:36:38.631545: step 643, loss = 3.10 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:36:41.033710: step 644, loss = 2.97 (53.4 examples/sec; 2.395 sec/batch)
2018-10-15 09:36:43.350580: step 645, loss = 3.05 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:36:45.783718: step 646, loss = 2.78 (52.7 examples/sec; 2.428 sec/batch)
2018-10-15 09:36:48.138780: step 647, loss = 2.87 (54.6 examples/sec; 2.346 sec/batch)
2018-10-15 09:36:50.498010: step 648, loss = 2.82 (54.5 examples/sec; 2.351 sec/batch)
2018-10-15 09:36:52.813155: step 649, loss = 2.73 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:36:55.143871: step 650, loss = 2.93 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 09:36:57.420075: step 651, loss = 2.79 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 09:36:59.804468: step 652, loss = 2.68 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 09:37:02.204497: step 653, loss = 2.88 (53.5 examples/sec; 2.391 sec/batch)
2018-10-15 09:37:04.659746: step 654, loss = 2.80 (52.3 examples/sec; 2.447 sec/batch)
2018-10-15 09:37:06.935880: step 655, loss = 2.77 (56.4 examples/sec; 2.268 sec/batch)
2018-10-15 09:37:09.276444: step 656, loss = 2.57 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 09:37:11.613270: step 657, loss = 2.80 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 09:37:13.968108: step 658, loss = 2.86 (54.6 examples/sec; 2.346 sec/batch)
2018-10-15 09:37:16.271344: step 659, loss = 2.70 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 09:37:18.619090: step 660, loss = 2.79 (54.7 examples/sec; 2.338 sec/batch)
2018-10-15 09:37:20.937298: step 661, loss = 2.81 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:37:23.377869: step 662, loss = 2.72 (52.6 examples/sec; 2.434 sec/batch)
2018-10-15 09:37:25.679563: step 663, loss = 2.88 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 09:37:27.979423: step 664, loss = 2.74 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:37:30.247186: step 665, loss = 2.45 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 09:37:32.518751: step 666, loss = 2.64 (56.5 examples/sec; 2.265 sec/batch)
2018-10-15 09:37:34.825294: step 667, loss = 2.72 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 09:37:37.112411: step 668, loss = 2.72 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:37:39.347592: step 669, loss = 2.81 (57.5 examples/sec; 2.226 sec/batch)
2018-10-15 09:37:41.644970: step 670, loss = 2.70 (55.8 examples/sec; 2.292 sec/batch)
2018-10-15 09:37:43.906348: step 671, loss = 2.60 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 09:37:46.258133: step 672, loss = 2.74 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:37:48.556672: step 673, loss = 2.88 (55.9 examples/sec; 2.290 sec/batch)
2018-10-15 09:37:50.798663: step 674, loss = 2.67 (57.3 examples/sec; 2.234 sec/batch)
2018-10-15 09:37:53.057249: step 675, loss = 2.88 (56.9 examples/sec; 2.250 sec/batch)
2018-10-15 09:37:55.324206: step 676, loss = 2.68 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 09:37:57.634511: step 677, loss = 2.70 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 09:37:59.928707: step 678, loss = 2.75 (56.0 examples/sec; 2.286 sec/batch)
2018-10-15 09:38:02.239129: step 679, loss = 2.63 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:38:04.456106: step 680, loss = 3.00 (57.9 examples/sec; 2.212 sec/batch)
2018-10-15 09:38:06.766737: step 681, loss = 2.86 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:38:09.052938: step 682, loss = 2.76 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:38:11.367568: step 683, loss = 2.89 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:38:13.650612: step 684, loss = 2.79 (56.3 examples/sec; 2.275 sec/batch)
2018-10-15 09:38:15.900128: step 685, loss = 2.90 (57.1 examples/sec; 2.243 sec/batch)
2018-10-15 09:38:18.224943: step 686, loss = 2.86 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 09:38:20.489413: step 687, loss = 2.83 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 09:38:22.774840: step 688, loss = 2.74 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 09:38:25.036133: step 689, loss = 2.74 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 09:38:27.336647: step 690, loss = 2.76 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 09:38:29.593327: step 691, loss = 2.68 (57.0 examples/sec; 2.248 sec/batch)
2018-10-15 09:38:31.874455: step 692, loss = 2.98 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 09:38:34.162177: step 693, loss = 2.82 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:38:36.565319: step 694, loss = 2.90 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 09:38:38.936293: step 695, loss = 2.90 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 09:38:41.213301: step 696, loss = 2.90 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 09:38:43.434087: step 697, loss = 2.75 (57.9 examples/sec; 2.212 sec/batch)
2018-10-15 09:38:45.848946: step 698, loss = 2.95 (53.2 examples/sec; 2.405 sec/batch)
2018-10-15 09:38:48.278482: step 699, loss = 2.74 (52.9 examples/sec; 2.420 sec/batch)
2018-10-15 09:38:50.651586: step 700, loss = 2.59 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 09:38:53.599814: step 701, loss = 2.92 (57.5 examples/sec; 2.225 sec/batch)
2018-10-15 09:38:55.883226: step 702, loss = 2.68 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:38:58.120398: step 703, loss = 2.95 (57.4 examples/sec; 2.229 sec/batch)
2018-10-15 09:39:00.466900: step 704, loss = 3.01 (54.7 examples/sec; 2.338 sec/batch)
2018-10-15 09:39:02.777638: step 705, loss = 2.97 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 09:39:05.030776: step 706, loss = 2.94 (57.0 examples/sec; 2.245 sec/batch)
2018-10-15 09:39:07.238914: step 707, loss = 2.76 (58.3 examples/sec; 2.196 sec/batch)
2018-10-15 09:39:09.606021: step 708, loss = 2.75 (54.3 examples/sec; 2.356 sec/batch)
2018-10-15 09:39:11.905726: step 709, loss = 2.75 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:39:14.208216: step 710, loss = 2.76 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 09:39:16.500660: step 711, loss = 2.83 (56.1 examples/sec; 2.284 sec/batch)
2018-10-15 09:39:18.815261: step 712, loss = 2.68 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:39:21.159635: step 713, loss = 2.89 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 09:39:23.450049: step 714, loss = 2.60 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 09:39:25.836285: step 715, loss = 2.78 (53.8 examples/sec; 2.381 sec/batch)
2018-10-15 09:39:28.184403: step 716, loss = 2.65 (54.7 examples/sec; 2.342 sec/batch)
2018-10-15 09:39:30.470351: step 717, loss = 2.97 (56.2 examples/sec; 2.277 sec/batch)
2018-10-15 09:39:32.752097: step 718, loss = 2.69 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:39:35.029985: step 719, loss = 2.95 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 09:39:37.308751: step 720, loss = 2.64 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 09:39:39.573494: step 721, loss = 2.78 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 09:39:41.889845: step 722, loss = 2.90 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:39:44.162369: step 723, loss = 2.86 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 09:39:46.546168: step 724, loss = 2.71 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 09:39:48.894402: step 725, loss = 2.84 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:39:51.331102: step 726, loss = 2.92 (52.7 examples/sec; 2.428 sec/batch)
2018-10-15 09:39:53.623548: step 727, loss = 2.73 (56.0 examples/sec; 2.284 sec/batch)
2018-10-15 09:39:55.928547: step 728, loss = 2.77 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 09:39:58.208926: step 729, loss = 2.74 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 09:40:00.496414: step 730, loss = 2.76 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:40:02.791467: step 731, loss = 2.79 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 09:40:05.055278: step 732, loss = 2.71 (56.8 examples/sec; 2.255 sec/batch)
2018-10-15 09:40:07.409758: step 733, loss = 2.68 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 09:40:09.757973: step 734, loss = 2.84 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 09:40:12.062640: step 735, loss = 2.59 (55.7 examples/sec; 2.296 sec/batch)
2018-10-15 09:40:14.465787: step 736, loss = 2.87 (53.4 examples/sec; 2.398 sec/batch)
2018-10-15 09:40:16.889910: step 737, loss = 2.86 (53.0 examples/sec; 2.415 sec/batch)
2018-10-15 09:40:19.231486: step 738, loss = 2.73 (54.8 examples/sec; 2.334 sec/batch)
2018-10-15 09:40:21.613656: step 739, loss = 2.62 (53.9 examples/sec; 2.374 sec/batch)
2018-10-15 09:40:23.888528: step 740, loss = 2.90 (56.5 examples/sec; 2.266 sec/batch)
2018-10-15 09:40:26.207871: step 741, loss = 2.62 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 09:40:28.640292: step 742, loss = 2.78 (52.8 examples/sec; 2.424 sec/batch)
2018-10-15 09:40:30.941855: step 743, loss = 2.83 (55.8 examples/sec; 2.294 sec/batch)
2018-10-15 09:40:33.244002: step 744, loss = 2.64 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 09:40:35.664181: step 745, loss = 2.85 (53.1 examples/sec; 2.411 sec/batch)
2018-10-15 09:40:37.961410: step 746, loss = 2.80 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:40:40.332281: step 747, loss = 2.64 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 09:40:42.650000: step 748, loss = 2.61 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:40:44.936288: step 749, loss = 2.65 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 09:40:47.258948: step 750, loss = 3.05 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 09:40:49.530868: step 751, loss = 3.09 (56.5 examples/sec; 2.264 sec/batch)
2018-10-15 09:40:51.841153: step 752, loss = 2.96 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 09:40:54.086369: step 753, loss = 2.68 (57.2 examples/sec; 2.236 sec/batch)
2018-10-15 09:40:56.303553: step 754, loss = 2.95 (57.9 examples/sec; 2.212 sec/batch)
2018-10-15 09:40:58.625407: step 755, loss = 2.57 (55.3 examples/sec; 2.314 sec/batch)
2018-10-15 09:41:00.944142: step 756, loss = 2.73 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:41:03.280705: step 757, loss = 2.81 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 09:41:05.631010: step 758, loss = 2.65 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 09:41:07.900970: step 759, loss = 2.66 (56.6 examples/sec; 2.262 sec/batch)
2018-10-15 09:41:10.233627: step 760, loss = 2.63 (55.1 examples/sec; 2.324 sec/batch)
2018-10-15 09:41:12.578211: step 761, loss = 2.87 (54.8 examples/sec; 2.336 sec/batch)
2018-10-15 09:41:14.973206: step 762, loss = 2.78 (53.6 examples/sec; 2.386 sec/batch)
2018-10-15 09:41:17.306946: step 763, loss = 2.64 (55.1 examples/sec; 2.325 sec/batch)
2018-10-15 09:41:19.610564: step 764, loss = 2.70 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 09:41:21.936595: step 765, loss = 2.67 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 09:41:24.273541: step 766, loss = 2.62 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 09:41:26.699547: step 767, loss = 2.82 (52.9 examples/sec; 2.421 sec/batch)
2018-10-15 09:41:29.029465: step 768, loss = 2.74 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:41:31.535816: step 769, loss = 2.43 (51.2 examples/sec; 2.500 sec/batch)
2018-10-15 09:41:33.853868: step 770, loss = 2.56 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:41:36.069663: step 771, loss = 2.48 (58.0 examples/sec; 2.207 sec/batch)
2018-10-15 09:41:38.438132: step 772, loss = 2.80 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 09:41:40.796765: step 773, loss = 2.60 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 09:41:43.104581: step 774, loss = 2.71 (55.7 examples/sec; 2.300 sec/batch)
2018-10-15 09:41:45.417665: step 775, loss = 2.66 (55.5 examples/sec; 2.308 sec/batch)
2018-10-15 09:41:47.741280: step 776, loss = 2.62 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:41:50.121537: step 777, loss = 2.52 (54.0 examples/sec; 2.370 sec/batch)
2018-10-15 09:41:52.549360: step 778, loss = 2.55 (52.9 examples/sec; 2.420 sec/batch)
2018-10-15 09:41:54.937171: step 779, loss = 2.72 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 09:41:57.234747: step 780, loss = 2.66 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:41:59.608673: step 781, loss = 2.78 (54.1 examples/sec; 2.366 sec/batch)
2018-10-15 09:42:02.012939: step 782, loss = 2.70 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 09:42:04.441425: step 783, loss = 2.63 (52.9 examples/sec; 2.420 sec/batch)
2018-10-15 09:42:06.846005: step 784, loss = 2.69 (53.3 examples/sec; 2.399 sec/batch)
2018-10-15 09:42:09.160567: step 785, loss = 2.76 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:42:11.507757: step 786, loss = 2.82 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:42:13.851395: step 787, loss = 2.84 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 09:42:16.132791: step 788, loss = 2.74 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 09:42:18.538778: step 789, loss = 2.65 (53.4 examples/sec; 2.397 sec/batch)
2018-10-15 09:42:20.846627: step 790, loss = 2.78 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 09:42:23.048415: step 791, loss = 2.79 (58.4 examples/sec; 2.193 sec/batch)
2018-10-15 09:42:25.416147: step 792, loss = 2.80 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 09:42:27.688350: step 793, loss = 2.72 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:42:30.040523: step 794, loss = 2.67 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 09:42:32.421956: step 795, loss = 2.73 (53.9 examples/sec; 2.373 sec/batch)
2018-10-15 09:42:34.756592: step 796, loss = 2.79 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 09:42:37.139357: step 797, loss = 2.98 (53.9 examples/sec; 2.375 sec/batch)
2018-10-15 09:42:39.520079: step 798, loss = 2.62 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 09:42:41.947448: step 799, loss = 2.76 (52.9 examples/sec; 2.419 sec/batch)
2018-10-15 09:42:44.267288: step 800, loss = 2.62 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:42:47.265009: step 801, loss = 2.59 (55.2 examples/sec; 2.321 sec/batch)
2018-10-15 09:42:49.612047: step 802, loss = 2.48 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:42:52.026927: step 803, loss = 2.69 (53.1 examples/sec; 2.410 sec/batch)
2018-10-15 09:42:54.420978: step 804, loss = 3.01 (53.7 examples/sec; 2.386 sec/batch)
2018-10-15 09:42:56.781101: step 805, loss = 2.72 (54.4 examples/sec; 2.355 sec/batch)
2018-10-15 09:42:59.102544: step 806, loss = 2.74 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:43:01.482259: step 807, loss = 2.57 (54.0 examples/sec; 2.371 sec/batch)
2018-10-15 09:43:03.719748: step 808, loss = 2.68 (57.4 examples/sec; 2.229 sec/batch)
2018-10-15 09:43:06.086720: step 809, loss = 2.65 (54.3 examples/sec; 2.359 sec/batch)
2018-10-15 09:43:08.513673: step 810, loss = 2.82 (52.9 examples/sec; 2.419 sec/batch)
2018-10-15 09:43:10.810706: step 811, loss = 2.74 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:43:13.167901: step 812, loss = 2.82 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 09:43:15.491032: step 813, loss = 2.66 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 09:43:17.941028: step 814, loss = 2.66 (52.4 examples/sec; 2.442 sec/batch)
2018-10-15 09:43:20.284110: step 815, loss = 2.63 (54.8 examples/sec; 2.335 sec/batch)
2018-10-15 09:43:22.616442: step 816, loss = 2.59 (55.1 examples/sec; 2.324 sec/batch)
2018-10-15 09:43:24.904358: step 817, loss = 2.41 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 09:43:27.280000: step 818, loss = 2.57 (54.0 examples/sec; 2.370 sec/batch)
2018-10-15 09:43:29.601139: step 819, loss = 2.51 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:43:32.001986: step 820, loss = 2.77 (53.4 examples/sec; 2.396 sec/batch)
2018-10-15 09:43:34.314166: step 821, loss = 2.74 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:43:36.654295: step 822, loss = 2.56 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:43:38.977717: step 823, loss = 2.88 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 09:43:41.236359: step 824, loss = 2.63 (56.8 examples/sec; 2.254 sec/batch)
2018-10-15 09:43:43.512118: step 825, loss = 2.73 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 09:43:45.988250: step 826, loss = 2.87 (51.9 examples/sec; 2.468 sec/batch)
2018-10-15 09:43:48.340811: step 827, loss = 2.66 (54.6 examples/sec; 2.345 sec/batch)
2018-10-15 09:43:50.662592: step 828, loss = 2.58 (55.3 examples/sec; 2.317 sec/batch)
2018-10-15 09:43:52.991041: step 829, loss = 2.74 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:43:55.385587: step 830, loss = 2.99 (53.6 examples/sec; 2.386 sec/batch)
2018-10-15 09:43:57.756001: step 831, loss = 2.78 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 09:44:00.045636: step 832, loss = 2.69 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:44:02.418823: step 833, loss = 2.56 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:44:04.775110: step 834, loss = 2.65 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 09:44:07.126727: step 835, loss = 2.75 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:44:09.541789: step 836, loss = 2.76 (53.2 examples/sec; 2.407 sec/batch)
2018-10-15 09:44:11.913273: step 837, loss = 2.60 (54.2 examples/sec; 2.363 sec/batch)
2018-10-15 09:44:14.230914: step 838, loss = 2.67 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:44:16.589642: step 839, loss = 2.75 (54.5 examples/sec; 2.350 sec/batch)
2018-10-15 09:44:18.992308: step 840, loss = 2.47 (53.5 examples/sec; 2.394 sec/batch)
2018-10-15 09:44:21.305113: step 841, loss = 2.61 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:44:23.690783: step 842, loss = 2.83 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 09:44:26.058778: step 843, loss = 3.05 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 09:44:28.431691: step 844, loss = 2.56 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 09:44:30.818701: step 845, loss = 2.74 (53.8 examples/sec; 2.380 sec/batch)
2018-10-15 09:44:33.141410: step 846, loss = 2.68 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 09:44:35.526849: step 847, loss = 2.70 (53.9 examples/sec; 2.376 sec/batch)
2018-10-15 09:44:37.873167: step 848, loss = 2.69 (54.8 examples/sec; 2.338 sec/batch)
2018-10-15 09:44:40.212417: step 849, loss = 2.77 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:44:42.600730: step 850, loss = 2.81 (53.8 examples/sec; 2.381 sec/batch)
2018-10-15 09:44:45.012841: step 851, loss = 2.56 (53.2 examples/sec; 2.405 sec/batch)
2018-10-15 09:44:47.328266: step 852, loss = 2.81 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:44:49.676162: step 853, loss = 2.80 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 09:44:51.964699: step 854, loss = 2.89 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 09:44:54.367538: step 855, loss = 2.71 (53.5 examples/sec; 2.394 sec/batch)
2018-10-15 09:44:56.664879: step 856, loss = 2.93 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:44:58.924187: step 857, loss = 2.72 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 09:45:01.185250: step 858, loss = 2.56 (56.7 examples/sec; 2.258 sec/batch)
2018-10-15 09:45:03.445511: step 859, loss = 2.73 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 09:45:05.790699: step 860, loss = 2.72 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 09:45:08.147497: step 861, loss = 2.78 (54.5 examples/sec; 2.348 sec/batch)
2018-10-15 09:45:10.468334: step 862, loss = 2.54 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:45:12.725460: step 863, loss = 2.80 (56.9 examples/sec; 2.248 sec/batch)
2018-10-15 09:45:15.003279: step 864, loss = 2.65 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 09:45:17.318659: step 865, loss = 2.69 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:45:19.666769: step 866, loss = 2.52 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:45:22.004759: step 867, loss = 2.68 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 09:45:24.327121: step 868, loss = 2.81 (55.2 examples/sec; 2.317 sec/batch)
2018-10-15 09:45:26.605366: step 869, loss = 2.68 (56.4 examples/sec; 2.270 sec/batch)
2018-10-15 09:45:28.882350: step 870, loss = 2.76 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 09:45:31.223023: step 871, loss = 2.66 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 09:45:33.534019: step 872, loss = 2.39 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 09:45:35.812519: step 873, loss = 2.61 (56.4 examples/sec; 2.271 sec/batch)
2018-10-15 09:45:38.082686: step 874, loss = 2.48 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:45:40.370454: step 875, loss = 2.64 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 09:45:42.706496: step 876, loss = 2.48 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 09:45:45.037356: step 877, loss = 2.47 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 09:45:47.350797: step 878, loss = 2.60 (55.6 examples/sec; 2.301 sec/batch)
2018-10-15 09:45:49.651527: step 879, loss = 2.59 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:45:51.937850: step 880, loss = 2.48 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 09:45:54.253434: step 881, loss = 2.53 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:45:56.560210: step 882, loss = 2.65 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 09:45:58.862458: step 883, loss = 2.64 (55.8 examples/sec; 2.295 sec/batch)
2018-10-15 09:46:01.197751: step 884, loss = 2.61 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 09:46:03.485167: step 885, loss = 2.60 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:46:05.742146: step 886, loss = 2.59 (56.9 examples/sec; 2.249 sec/batch)
2018-10-15 09:46:08.029956: step 887, loss = 2.67 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 09:46:10.414128: step 888, loss = 2.77 (53.8 examples/sec; 2.379 sec/batch)
2018-10-15 09:46:12.765031: step 889, loss = 2.37 (54.6 examples/sec; 2.342 sec/batch)
2018-10-15 09:46:15.047886: step 890, loss = 2.68 (56.2 examples/sec; 2.278 sec/batch)
2018-10-15 09:46:17.295351: step 891, loss = 2.65 (57.2 examples/sec; 2.239 sec/batch)
2018-10-15 09:46:19.625012: step 892, loss = 2.87 (55.1 examples/sec; 2.321 sec/batch)
2018-10-15 09:46:21.992095: step 893, loss = 2.68 (54.3 examples/sec; 2.358 sec/batch)
2018-10-15 09:46:24.434225: step 894, loss = 2.52 (52.6 examples/sec; 2.434 sec/batch)
2018-10-15 09:46:26.670814: step 895, loss = 2.67 (57.5 examples/sec; 2.228 sec/batch)
2018-10-15 09:46:28.912647: step 896, loss = 2.64 (57.3 examples/sec; 2.236 sec/batch)
2018-10-15 09:46:31.238979: step 897, loss = 2.66 (55.2 examples/sec; 2.320 sec/batch)
2018-10-15 09:46:33.554195: step 898, loss = 2.86 (55.5 examples/sec; 2.307 sec/batch)
2018-10-15 09:46:35.832474: step 899, loss = 2.74 (56.4 examples/sec; 2.269 sec/batch)
2018-10-15 09:46:38.139669: step 900, loss = 2.68 (55.7 examples/sec; 2.298 sec/batch)
2018-10-15 09:46:40.921012: step 901, loss = 2.66 (56.9 examples/sec; 2.248 sec/batch)
2018-10-15 09:46:43.183866: step 902, loss = 2.69 (56.7 examples/sec; 2.259 sec/batch)
2018-10-15 09:46:45.444042: step 903, loss = 2.80 (56.8 examples/sec; 2.252 sec/batch)
2018-10-15 09:46:47.733093: step 904, loss = 2.78 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:46:50.135180: step 905, loss = 2.55 (53.5 examples/sec; 2.393 sec/batch)
2018-10-15 09:46:52.422578: step 906, loss = 2.58 (56.1 examples/sec; 2.282 sec/batch)
2018-10-15 09:46:54.687188: step 907, loss = 2.88 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 09:46:57.022395: step 908, loss = 2.65 (55.0 examples/sec; 2.326 sec/batch)
2018-10-15 09:46:59.346581: step 909, loss = 2.71 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:47:01.694369: step 910, loss = 2.67 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:47:04.065587: step 911, loss = 2.81 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 09:47:06.388962: step 912, loss = 2.68 (55.3 examples/sec; 2.315 sec/batch)
2018-10-15 09:47:08.648181: step 913, loss = 2.74 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 09:47:10.933800: step 914, loss = 2.61 (56.2 examples/sec; 2.277 sec/batch)
2018-10-15 09:47:13.357411: step 915, loss = 2.64 (53.0 examples/sec; 2.415 sec/batch)
2018-10-15 09:47:15.678970: step 916, loss = 2.59 (55.3 examples/sec; 2.313 sec/batch)
2018-10-15 09:47:18.015814: step 917, loss = 2.71 (54.9 examples/sec; 2.333 sec/batch)
2018-10-15 09:47:20.305606: step 918, loss = 2.73 (56.1 examples/sec; 2.281 sec/batch)
2018-10-15 09:47:22.672766: step 919, loss = 2.88 (54.2 examples/sec; 2.362 sec/batch)
2018-10-15 09:47:25.034181: step 920, loss = 2.72 (54.4 examples/sec; 2.355 sec/batch)
2018-10-15 09:47:27.354104: step 921, loss = 2.65 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:47:29.700679: step 922, loss = 2.76 (54.7 examples/sec; 2.339 sec/batch)
2018-10-15 09:47:31.993941: step 923, loss = 2.64 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 09:47:34.285412: step 924, loss = 2.47 (56.1 examples/sec; 2.283 sec/batch)
2018-10-15 09:47:36.584904: step 925, loss = 2.64 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:47:38.957397: step 926, loss = 2.60 (54.1 examples/sec; 2.364 sec/batch)
2018-10-15 09:47:41.306338: step 927, loss = 2.82 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:47:43.654851: step 928, loss = 2.52 (54.7 examples/sec; 2.342 sec/batch)
2018-10-15 09:47:45.956916: step 929, loss = 2.60 (55.8 examples/sec; 2.293 sec/batch)
2018-10-15 09:47:48.262138: step 930, loss = 2.65 (55.7 examples/sec; 2.297 sec/batch)
2018-10-15 09:47:50.691907: step 931, loss = 2.91 (52.9 examples/sec; 2.421 sec/batch)
2018-10-15 09:47:53.074013: step 932, loss = 2.85 (53.9 examples/sec; 2.377 sec/batch)
2018-10-15 09:47:55.506114: step 933, loss = 2.76 (52.8 examples/sec; 2.424 sec/batch)
2018-10-15 09:47:57.774899: step 934, loss = 2.48 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:48:00.062617: step 935, loss = 2.74 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 09:48:02.395660: step 936, loss = 2.83 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 09:48:04.749595: step 937, loss = 2.72 (54.5 examples/sec; 2.347 sec/batch)
2018-10-15 09:48:07.127916: step 938, loss = 2.67 (54.0 examples/sec; 2.371 sec/batch)
2018-10-15 09:48:09.466026: step 939, loss = 2.59 (55.0 examples/sec; 2.329 sec/batch)
2018-10-15 09:48:11.746230: step 940, loss = 2.51 (56.3 examples/sec; 2.273 sec/batch)
2018-10-15 09:48:14.025106: step 941, loss = 2.56 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 09:48:16.363283: step 942, loss = 2.63 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:48:18.699912: step 943, loss = 2.73 (55.0 examples/sec; 2.328 sec/batch)
2018-10-15 09:48:21.029455: step 944, loss = 3.02 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:48:23.261553: step 945, loss = 2.67 (57.5 examples/sec; 2.225 sec/batch)
2018-10-15 09:48:25.543669: step 946, loss = 2.43 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:48:27.783469: step 947, loss = 2.55 (57.3 examples/sec; 2.233 sec/batch)
2018-10-15 09:48:30.159315: step 948, loss = 2.66 (54.1 examples/sec; 2.368 sec/batch)
2018-10-15 09:48:32.510240: step 949, loss = 2.64 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:48:34.795275: step 950, loss = 2.59 (56.1 examples/sec; 2.280 sec/batch)
2018-10-15 09:48:37.051153: step 951, loss = 2.60 (56.9 examples/sec; 2.248 sec/batch)
2018-10-15 09:48:39.355021: step 952, loss = 2.63 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 09:48:41.652018: step 953, loss = 2.48 (55.9 examples/sec; 2.292 sec/batch)
2018-10-15 09:48:43.991784: step 954, loss = 2.65 (54.9 examples/sec; 2.332 sec/batch)
2018-10-15 09:48:46.310303: step 955, loss = 2.53 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:48:48.551267: step 956, loss = 2.66 (57.3 examples/sec; 2.234 sec/batch)
2018-10-15 09:48:50.884206: step 957, loss = 2.72 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 09:48:53.143851: step 958, loss = 3.03 (56.8 examples/sec; 2.253 sec/batch)
2018-10-15 09:48:55.494246: step 959, loss = 2.82 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:48:57.804453: step 960, loss = 2.66 (55.5 examples/sec; 2.304 sec/batch)
2018-10-15 09:49:00.053467: step 961, loss = 2.80 (57.0 examples/sec; 2.244 sec/batch)
2018-10-15 09:49:02.372070: step 962, loss = 2.66 (55.4 examples/sec; 2.311 sec/batch)
2018-10-15 09:49:04.689135: step 963, loss = 2.63 (55.4 examples/sec; 2.312 sec/batch)
2018-10-15 09:49:06.971047: step 964, loss = 2.61 (56.3 examples/sec; 2.274 sec/batch)
2018-10-15 09:49:09.378298: step 965, loss = 2.60 (53.3 examples/sec; 2.400 sec/batch)
2018-10-15 09:49:11.734984: step 966, loss = 2.61 (54.4 examples/sec; 2.351 sec/batch)
2018-10-15 09:49:13.962636: step 967, loss = 2.83 (57.6 examples/sec; 2.221 sec/batch)
2018-10-15 09:49:16.280264: step 968, loss = 2.65 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:49:18.618832: step 969, loss = 2.67 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:49:21.021073: step 970, loss = 2.52 (53.5 examples/sec; 2.394 sec/batch)
2018-10-15 09:49:23.396817: step 971, loss = 2.71 (54.1 examples/sec; 2.367 sec/batch)
2018-10-15 09:49:25.647255: step 972, loss = 2.53 (57.1 examples/sec; 2.242 sec/batch)
2018-10-15 09:49:27.911346: step 973, loss = 2.46 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 09:49:30.182354: step 974, loss = 2.72 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:49:32.530750: step 975, loss = 2.60 (54.7 examples/sec; 2.341 sec/batch)
2018-10-15 09:49:34.872624: step 976, loss = 2.48 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 09:49:37.286254: step 977, loss = 2.51 (53.2 examples/sec; 2.406 sec/batch)
2018-10-15 09:49:39.567180: step 978, loss = 2.54 (56.3 examples/sec; 2.272 sec/batch)
2018-10-15 09:49:41.973764: step 979, loss = 2.56 (53.4 examples/sec; 2.399 sec/batch)
2018-10-15 09:49:44.304187: step 980, loss = 2.53 (55.1 examples/sec; 2.322 sec/batch)
2018-10-15 09:49:46.655956: step 981, loss = 2.39 (54.6 examples/sec; 2.344 sec/batch)
2018-10-15 09:49:49.042205: step 982, loss = 2.60 (53.8 examples/sec; 2.378 sec/batch)
2018-10-15 09:49:51.311836: step 983, loss = 2.49 (56.6 examples/sec; 2.261 sec/batch)
2018-10-15 09:49:53.583565: step 984, loss = 2.41 (56.5 examples/sec; 2.265 sec/batch)
2018-10-15 09:49:55.832606: step 985, loss = 2.38 (57.1 examples/sec; 2.242 sec/batch)
2018-10-15 09:49:58.115878: step 986, loss = 2.71 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 09:50:00.374183: step 987, loss = 2.47 (56.9 examples/sec; 2.251 sec/batch)
2018-10-15 09:50:02.692687: step 988, loss = 2.50 (55.4 examples/sec; 2.310 sec/batch)
2018-10-15 09:50:04.942208: step 989, loss = 2.55 (57.0 examples/sec; 2.246 sec/batch)
2018-10-15 09:50:07.257867: step 990, loss = 2.62 (55.4 examples/sec; 2.309 sec/batch)
2018-10-15 09:50:09.572225: step 991, loss = 2.45 (55.5 examples/sec; 2.306 sec/batch)
2018-10-15 09:50:11.943776: step 992, loss = 2.66 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:50:14.322644: step 993, loss = 2.76 (54.0 examples/sec; 2.370 sec/batch)
2018-10-15 09:50:16.632081: step 994, loss = 2.79 (55.6 examples/sec; 2.302 sec/batch)
2018-10-15 09:50:18.916355: step 995, loss = 2.60 (56.2 examples/sec; 2.279 sec/batch)
2018-10-15 09:50:21.142660: step 996, loss = 2.91 (57.6 examples/sec; 2.221 sec/batch)
2018-10-15 09:50:23.463824: step 997, loss = 2.68 (55.3 examples/sec; 2.316 sec/batch)
2018-10-15 09:50:25.777494: step 998, loss = 2.56 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:50:28.139941: step 999, loss = 2.78 (54.4 examples/sec; 2.354 sec/batch)
2018-10-15 09:50:30.369255: step 1000, loss = 2.59 (57.6 examples/sec; 2.221 sec/batch)
2018-10-15 09:50:33.137686: step 1001, loss = 2.60 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 09:50:35.467167: step 1002, loss = 2.70 (55.1 examples/sec; 2.321 sec/batch)
2018-10-15 09:50:37.779167: step 1003, loss = 2.66 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:50:40.062353: step 1004, loss = 2.47 (56.2 examples/sec; 2.276 sec/batch)
2018-10-15 09:50:42.411085: step 1005, loss = 2.62 (54.6 examples/sec; 2.343 sec/batch)
2018-10-15 09:50:44.781701: step 1006, loss = 2.66 (54.1 examples/sec; 2.365 sec/batch)
2018-10-15 09:50:47.074456: step 1007, loss = 2.67 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 09:50:49.406961: step 1008, loss = 2.81 (55.0 examples/sec; 2.327 sec/batch)
2018-10-15 09:50:51.705710: step 1009, loss = 2.56 (55.9 examples/sec; 2.291 sec/batch)
2018-10-15 09:50:54.102661: step 1010, loss = 2.48 (53.6 examples/sec; 2.389 sec/batch)
2018-10-15 09:50:56.493191: step 1011, loss = 2.79 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 09:50:58.841582: step 1012, loss = 2.64 (54.7 examples/sec; 2.341 sec/batch)
2018-10-15 09:51:01.102601: step 1013, loss = 2.55 (56.8 examples/sec; 2.253 sec/batch)
2018-10-15 09:51:03.491243: step 1014, loss = 2.60 (53.8 examples/sec; 2.380 sec/batch)
2018-10-15 09:51:05.785948: step 1015, loss = 2.67 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:51:08.094204: step 1016, loss = 2.74 (55.7 examples/sec; 2.299 sec/batch)
2018-10-15 09:51:10.316044: step 1017, loss = 2.46 (57.8 examples/sec; 2.215 sec/batch)
2018-10-15 09:51:12.610675: step 1018, loss = 2.85 (56.0 examples/sec; 2.287 sec/batch)
2018-10-15 09:51:14.882064: step 1019, loss = 2.51 (56.6 examples/sec; 2.263 sec/batch)
2018-10-15 09:51:17.273938: step 1020, loss = 2.56 (53.7 examples/sec; 2.384 sec/batch)
2018-10-15 09:51:19.642593: step 1021, loss = 2.63 (54.2 examples/sec; 2.360 sec/batch)
2018-10-15 09:51:21.954211: step 1022, loss = 2.46 (55.6 examples/sec; 2.304 sec/batch)
2018-10-15 09:51:24.207970: step 1023, loss = 2.60 (56.9 examples/sec; 2.248 sec/batch)
2018-10-15 09:51:26.518779: step 1024, loss = 2.79 (55.5 examples/sec; 2.305 sec/batch)
2018-10-15 09:51:28.815061: step 1025, loss = 2.52 (55.9 examples/sec; 2.289 sec/batch)
2018-10-15 09:51:31.211300: step 1026, loss = 2.55 (53.6 examples/sec; 2.389 sec/batch)
2018-10-15 09:51:33.556932: step 1027, loss = 2.39 (54.7 examples/sec; 2.340 sec/batch)
2018-10-15 09:51:35.861621: step 1028, loss = 2.73 (55.8 examples/sec; 2.296 sec/batch)
2018-10-15 09:51:38.186996: step 1029, loss = 2.45 (55.2 examples/sec; 2.318 sec/batch)
2018-10-15 09:51:40.532106: step 1030, loss = 2.74 (54.8 examples/sec; 2.337 sec/batch)
2018-10-15 09:51:42.992533: step 1031, loss = 2.61 (52.1 examples/sec; 2.455 sec/batch)
2018-10-15 09:51:45.383382: step 1032, loss = 2.35 (53.7 examples/sec; 2.382 sec/batch)
2018-10-15 09:51:47.772533: step 1033, loss = 2.47 (53.7 examples/sec; 2.384 sec/batch)
2018-10-15 09:51:50.016762: step 1034, loss = 2.57 (57.2 examples/sec; 2.238 sec/batch)
2018-10-15 09:51:52.401553: step 1035, loss = 2.50 (53.8 examples/sec; 2.377 sec/batch)
2018-10-15 09:51:54.739729: step 1036, loss = 2.65 (54.9 examples/sec; 2.331 sec/batch)
2018-10-15 09:51:57.003596: step 1037, loss = 2.64 (56.7 examples/sec; 2.256 sec/batch)
2018-10-15 09:51:59.295850: step 1038, loss = 2.59 (56.0 examples/sec; 2.285 sec/batch)
2018-10-15 09:52:01.626115: step 1039, loss = 2.56 (55.1 examples/sec; 2.323 sec/batch)
2018-10-15 09:52:03.923327: step 1040, loss = 2.59 (55.9 examples/sec; 2.289 sec/batch)
Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 267, in <module>
    benchmark_name=args.dataset, batch_num=args.batch_num, batch_size=args.batch_size)
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 184, in train
    _, loss_value, summary = sess.run([train_op, total_loss, summary_op])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.OutOfRangeError: RandomShuffleQueue '_1_flowers_data/parallel_read/common_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[{{node flowers_data/parallel_read/common_queue_Dequeue}} = QueueDequeueV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:ps/replica:0/task:0/device:CPU:0"](flowers_data/parallel_read/common_queue)]]
         [[{{node model/fc1000/weights/read_S1}} = _Recv[client_terminated=false, recv_device="/job:worker/replica:0/task:1/device:CPU:0", send_device="/job:ps/replica:0/task:0/device:CPU:0", send_device_incarnation=-66666841808767008, tensor_name="edge_48_model/fc1000/weights/read", tensor_type=DT_FLOAT, _device="/job:worker/replica:0/task:1/device:CPU:0"]()]]

Caused by op u'flowers_data/parallel_read/common_queue_Dequeue', defined at:
  File "/usr/lib/python2.7/runpy.py", line 174, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 267, in <module>
    benchmark_name=args.dataset, batch_num=args.batch_num, batch_size=args.batch_size)
  File "/users/jayan/alexnet/AlexNet/scripts/train.py", line 135, in train
    images, labels, num_classes = input_data(batch_size, batch_num)
  File "AlexNet/datasets/__init__.py", line 54, in flowers_data
    num_epochs=num_epochs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py", line 96, in __init__
    scope=scope)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py", line 262, in parallel_read
    reader_kwargs=reader_kwargs).read(filename_queue)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/data/parallel_reader.py", line 131, in read
    return self._common_queue.dequeue(name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/data_flow_ops.py", line 433, in dequeue
    self._queue_ref, self._dtypes, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_data_flow_ops.py", line 3735, in queue_dequeue_v2
    timeout_ms=timeout_ms, name=name)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 3272, in create_op
    op_def=op_def)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

OutOfRangeError (see above for traceback): RandomShuffleQueue '_1_flowers_data/parallel_read/common_queue' is closed and has insufficient elements (requested 1, current size 0)
         [[{{node flowers_data/parallel_read/common_queue_Dequeue}} = QueueDequeueV2[component_types=[DT_STRING, DT_STRING], timeout_ms=-1, _device="/job:ps/replica:0/task:0/device:CPU:0"](flowers_data/parallel_read/common_queue)]]
         [[{{node model/fc1000/weights/read_S1}} = _Recv[client_terminated=false, recv_device="/job:worker/replica:0/task:1/device:CPU:0", send_device="/job:ps/replica:0/task:0/device:CPU:0", send_device_incarnation=-66666841808767008, tensor_name="edge_48_model/fc1000/weights/read", tensor_type=DT_FLOAT, _device="/job:worker/replica:0/task:1/device:CPU:0"]()]
