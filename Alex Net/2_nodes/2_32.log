
jayan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode cluster2 --dataset flowers --batch_num 10000
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:ps/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:ps/task:0)
Input batch shape: images: (128, 256, 256, 3) labels: (128,)
num_classes: 5
total_num_examples: 1280000
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 19:07:12.922165: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 94a9947f6041ae52 with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 19:07:22.350810: step 0, loss = 3.98 (18.6 examples/sec; 6.882 sec/batch)
2018-10-15 19:07:28.690712: step 1, loss = 4.96 (25.9 examples/sec; 4.946 sec/batch)
2018-10-15 19:07:33.866633: step 2, loss = 6.66 (24.8 examples/sec; 5.170 sec/batch)
2018-10-15 19:07:38.998776: step 3, loss = 5.81 (25.0 examples/sec; 5.127 sec/batch)
2018-10-15 19:07:43.938661: step 4, loss = 8.83 (26.0 examples/sec; 4.931 sec/batch)
2018-10-15 19:07:48.782765: step 5, loss = 5.85 (26.4 examples/sec; 4.839 sec/batch)
2018-10-15 19:07:53.721508: step 6, loss = 6.06 (26.0 examples/sec; 4.931 sec/batch)
2018-10-15 19:07:58.747979: step 7, loss = 6.20 (25.5 examples/sec; 5.021 sec/batch)
2018-10-15 19:08:03.735950: step 8, loss = 7.43 (25.7 examples/sec; 4.985 sec/batch)
2018-10-15 19:08:08.974401: step 9, loss = 8.24 (24.5 examples/sec; 5.234 sec/batch)
2018-10-15 19:08:14.277668: step 10, loss = 4.68 (24.2 examples/sec; 5.297 sec/batch)
2018-10-15 19:08:19.392655: step 11, loss = 7.15 (25.0 examples/sec; 5.112 sec/batch)
2018-10-15 19:08:24.133259: step 12, loss = 6.25 (27.0 examples/sec; 4.736 sec/batch)
2018-10-15 19:08:29.236752: step 13, loss = 4.26 (25.1 examples/sec; 5.096 sec/batch)
2018-10-15 19:08:34.434452: step 14, loss = 5.23 (24.7 examples/sec; 5.192 sec/batch)
2018-10-15 19:08:39.343135: step 15, loss = 4.97 (26.1 examples/sec; 4.903 sec/batch)
2018-10-15 19:08:44.526409: step 16, loss = 4.65 (24.7 examples/sec; 5.180 sec/batch)
2018-10-15 19:08:49.701284: step 17, loss = 5.07 (24.7 examples/sec; 5.173 sec/batch)
2018-10-15 19:08:54.715798: step 18, loss = 4.19 (25.6 examples/sec; 5.009 sec/batch)
2018-10-15 19:08:59.745420: step 19, loss = 3.93 (25.5 examples/sec; 5.022 sec/batch)
2018-10-15 19:09:04.832027: step 20, loss = 4.98 (25.2 examples/sec; 5.082 sec/batch)
2018-10-15 19:09:10.348995: step 21, loss = 4.37 (23.2 examples/sec; 5.515 sec/batch)
2018-10-15 19:09:15.808604: step 22, loss = 3.62 (23.5 examples/sec; 5.457 sec/batch)
2018-10-15 19:09:21.106211: step 23, loss = 4.06 (24.2 examples/sec; 5.296 sec/batch)
2018-10-15 19:09:25.806041: step 24, loss = 4.04 (27.3 examples/sec; 4.690 sec/batch)
2018-10-15 19:09:30.326229: step 25, loss = 3.86 (28.4 examples/sec; 4.512 sec/batch)
2018-10-15 19:09:33.876356: step 26, loss = 4.51 (36.1 examples/sec; 3.545 sec/batch)
2018-10-15 19:09:37.123306: step 27, loss = 4.69 (39.5 examples/sec; 3.242 sec/batch)
2018-10-15 19:09:40.499266: step 28, loss = 3.75 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:09:43.801254: step 29, loss = 3.81 (38.8 examples/sec; 3.297 sec/batch)
2018-10-15 19:09:47.260474: step 30, loss = 4.08 (37.1 examples/sec; 3.455 sec/batch)
2018-10-15 19:09:50.562951: step 31, loss = 4.05 (38.8 examples/sec; 3.298 sec/batch)
2018-10-15 19:09:54.008869: step 32, loss = 3.99 (37.2 examples/sec; 3.441 sec/batch)
2018-10-15 19:09:57.332915: step 33, loss = 4.00 (38.6 examples/sec; 3.319 sec/batch)
2018-10-15 19:10:00.636740: step 34, loss = 3.77 (38.8 examples/sec; 3.299 sec/batch)
2018-10-15 19:10:04.040905: step 35, loss = 3.79 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:10:07.381990: step 36, loss = 3.75 (38.4 examples/sec; 3.337 sec/batch)
2018-10-15 19:10:10.760298: step 37, loss = 3.76 (37.9 examples/sec; 3.373 sec/batch)
2018-10-15 19:10:14.166148: step 38, loss = 3.69 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:10:17.508209: step 39, loss = 3.58 (38.4 examples/sec; 3.337 sec/batch)
2018-10-15 19:10:20.956388: step 40, loss = 3.65 (37.2 examples/sec; 3.443 sec/batch)
2018-10-15 19:10:24.328103: step 41, loss = 3.54 (38.0 examples/sec; 3.366 sec/batch)
2018-10-15 19:10:27.809683: step 42, loss = 3.67 (36.8 examples/sec; 3.476 sec/batch)
2018-10-15 19:10:31.132476: step 43, loss = 3.58 (38.6 examples/sec; 3.318 sec/batch)
2018-10-15 19:10:34.471447: step 44, loss = 3.56 (38.4 examples/sec; 3.334 sec/batch)
2018-10-15 19:10:37.894450: step 45, loss = 3.60 (37.5 examples/sec; 3.418 sec/batch)
2018-10-15 19:10:41.295254: step 46, loss = 3.56 (37.7 examples/sec; 3.398 sec/batch)
2018-10-15 19:10:44.723800: step 47, loss = 3.52 (37.4 examples/sec; 3.426 sec/batch)
2018-10-15 19:10:48.195431: step 48, loss = 3.73 (36.9 examples/sec; 3.469 sec/batch)
2018-10-15 19:10:51.578030: step 49, loss = 3.64 (37.9 examples/sec; 3.378 sec/batch)
2018-10-15 19:10:55.143229: step 50, loss = 3.57 (36.0 examples/sec; 3.560 sec/batch)
2018-10-15 19:10:58.427634: step 51, loss = 3.46 (39.0 examples/sec; 3.280 sec/batch)
2018-10-15 19:11:01.948473: step 52, loss = 3.52 (36.4 examples/sec; 3.516 sec/batch)
2018-10-15 19:11:05.343001: step 53, loss = 3.62 (37.8 examples/sec; 3.389 sec/batch)
2018-10-15 19:11:08.709996: step 54, loss = 3.77 (38.1 examples/sec; 3.362 sec/batch)
2018-10-15 19:11:12.072359: step 55, loss = 3.61 (38.1 examples/sec; 3.357 sec/batch)
2018-10-15 19:11:15.452270: step 56, loss = 3.48 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:11:18.750345: step 57, loss = 3.45 (38.8 examples/sec; 3.295 sec/batch)
2018-10-15 19:11:22.240863: step 58, loss = 3.57 (36.7 examples/sec; 3.488 sec/batch)
2018-10-15 19:11:25.621314: step 59, loss = 3.53 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:11:29.139772: step 60, loss = 3.60 (36.4 examples/sec; 3.513 sec/batch)
2018-10-15 19:11:32.569072: step 61, loss = 3.51 (37.4 examples/sec; 3.424 sec/batch)
2018-10-15 19:11:35.901269: step 62, loss = 3.50 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:11:39.206127: step 63, loss = 3.54 (38.8 examples/sec; 3.300 sec/batch)
2018-10-15 19:11:42.764075: step 64, loss = 3.57 (36.0 examples/sec; 3.555 sec/batch)
2018-10-15 19:11:46.184886: step 65, loss = 3.49 (37.5 examples/sec; 3.416 sec/batch)
2018-10-15 19:11:49.442523: step 66, loss = 3.49 (39.4 examples/sec; 3.253 sec/batch)
2018-10-15 19:11:52.774352: step 67, loss = 3.48 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:11:56.151564: step 68, loss = 3.47 (37.9 examples/sec; 3.374 sec/batch)
2018-10-15 19:11:59.481836: step 69, loss = 3.56 (38.5 examples/sec; 3.325 sec/batch)
2018-10-15 19:12:02.856290: step 70, loss = 3.46 (38.0 examples/sec; 3.369 sec/batch)
2018-10-15 19:12:06.346609: step 71, loss = 3.49 (36.7 examples/sec; 3.485 sec/batch)
2018-10-15 19:12:09.870656: step 72, loss = 3.51 (36.4 examples/sec; 3.519 sec/batch)
2018-10-15 19:12:13.369373: step 73, loss = 3.46 (36.6 examples/sec; 3.494 sec/batch)
2018-10-15 19:12:16.692561: step 74, loss = 3.48 (38.6 examples/sec; 3.319 sec/batch)
2018-10-15 19:12:20.024166: step 75, loss = 3.50 (38.5 examples/sec; 3.326 sec/batch)
2018-10-15 19:12:23.388563: step 76, loss = 3.51 (38.1 examples/sec; 3.359 sec/batch)
2018-10-15 19:12:26.834454: step 77, loss = 3.49 (37.2 examples/sec; 3.441 sec/batch)
2018-10-15 19:12:30.229778: step 78, loss = 3.43 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:12:33.684186: step 79, loss = 3.44 (37.1 examples/sec; 3.452 sec/batch)
2018-10-15 19:12:36.952930: step 80, loss = 3.43 (39.2 examples/sec; 3.264 sec/batch)
2018-10-15 19:12:40.496867: step 81, loss = 3.50 (36.2 examples/sec; 3.539 sec/batch)
2018-10-15 19:12:43.825372: step 82, loss = 3.57 (38.5 examples/sec; 3.326 sec/batch)
2018-10-15 19:12:47.104373: step 83, loss = 3.44 (39.1 examples/sec; 3.276 sec/batch)
2018-10-15 19:12:50.566138: step 84, loss = 3.51 (37.0 examples/sec; 3.457 sec/batch)
2018-10-15 19:12:54.071386: step 85, loss = 3.51 (36.6 examples/sec; 3.501 sec/batch)
2018-10-15 19:12:57.498791: step 86, loss = 3.50 (37.4 examples/sec; 3.423 sec/batch)
2018-10-15 19:13:00.922414: step 87, loss = 3.46 (37.4 examples/sec; 3.419 sec/batch)
2018-10-15 19:13:04.310736: step 88, loss = 3.46 (37.8 examples/sec; 3.383 sec/batch)
2018-10-15 19:13:07.612988: step 89, loss = 3.51 (38.8 examples/sec; 3.299 sec/batch)
2018-10-15 19:13:11.050236: step 90, loss = 3.54 (37.3 examples/sec; 3.433 sec/batch)
2018-10-15 19:13:14.499222: step 91, loss = 3.50 (37.2 examples/sec; 3.444 sec/batch)
2018-10-15 19:13:17.931969: step 92, loss = 3.49 (37.3 examples/sec; 3.427 sec/batch)
2018-10-15 19:13:21.302958: step 93, loss = 3.47 (38.0 examples/sec; 3.368 sec/batch)
2018-10-15 19:13:24.669909: step 94, loss = 3.47 (38.1 examples/sec; 3.362 sec/batch)
2018-10-15 19:13:28.215013: step 95, loss = 3.57 (36.2 examples/sec; 3.540 sec/batch)
2018-10-15 19:13:31.602302: step 96, loss = 3.55 (37.8 examples/sec; 3.382 sec/batch)
2018-10-15 19:13:34.898681: step 97, loss = 3.48 (38.9 examples/sec; 3.291 sec/batch)
2018-10-15 19:13:38.355270: step 98, loss = 3.44 (37.1 examples/sec; 3.452 sec/batch)
2018-10-15 19:13:41.653248: step 99, loss = 3.43 (38.9 examples/sec; 3.293 sec/batch)
2018-10-15 19:13:45.059163: step 100, loss = 3.52 (37.6 examples/sec; 3.403 sec/batch)
2018-10-15 19:13:48.938824: step 101, loss = 3.50 (37.4 examples/sec; 3.420 sec/batch)
2018-10-15 19:13:52.426049: step 102, loss = 3.59 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:13:55.857068: step 103, loss = 3.42 (37.4 examples/sec; 3.426 sec/batch)
2018-10-15 19:13:59.397988: step 104, loss = 3.47 (36.2 examples/sec; 3.536 sec/batch)
2018-10-15 19:14:02.642192: step 105, loss = 3.45 (39.5 examples/sec; 3.239 sec/batch)
2018-10-15 19:14:06.047228: step 106, loss = 3.44 (37.6 examples/sec; 3.400 sec/batch)
2018-10-15 19:14:09.620980: step 107, loss = 3.45 (35.9 examples/sec; 3.568 sec/batch)
2018-10-15 19:14:13.111302: step 108, loss = 3.53 (36.7 examples/sec; 3.485 sec/batch)
2018-10-15 19:14:16.775244: step 109, loss = 3.46 (35.0 examples/sec; 3.661 sec/batch)
2018-10-15 19:14:20.229974: step 110, loss = 3.43 (37.1 examples/sec; 3.450 sec/batch)
2018-10-15 19:14:23.547581: step 111, loss = 3.45 (38.6 examples/sec; 3.315 sec/batch)
2018-10-15 19:14:27.000029: step 112, loss = 3.40 (37.1 examples/sec; 3.450 sec/batch)
2018-10-15 19:14:30.420098: step 113, loss = 3.49 (37.5 examples/sec; 3.415 sec/batch)
2018-10-15 19:14:33.959097: step 114, loss = 3.57 (36.2 examples/sec; 3.534 sec/batch)
2018-10-15 19:14:37.335253: step 115, loss = 3.53 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:14:40.764458: step 116, loss = 3.42 (37.4 examples/sec; 3.424 sec/batch)
2018-10-15 19:14:44.068356: step 117, loss = 3.31 (38.8 examples/sec; 3.300 sec/batch)
2018-10-15 19:14:47.418746: step 118, loss = 3.44 (38.3 examples/sec; 3.346 sec/batch)
2018-10-15 19:14:50.898545: step 119, loss = 3.49 (36.8 examples/sec; 3.475 sec/batch)
2018-10-15 19:14:54.284672: step 120, loss = 3.42 (37.9 examples/sec; 3.381 sec/batch)
2018-10-15 19:14:57.605239: step 121, loss = 3.46 (38.6 examples/sec; 3.315 sec/batch)
2018-10-15 19:15:01.067163: step 122, loss = 3.40 (37.0 examples/sec; 3.456 sec/batch)
2018-10-15 19:15:04.468219: step 123, loss = 3.42 (37.7 examples/sec; 3.396 sec/batch)
2018-10-15 19:15:07.757255: step 124, loss = 3.46 (39.0 examples/sec; 3.284 sec/batch)
2018-10-15 19:15:11.039413: step 125, loss = 3.40 (39.1 examples/sec; 3.277 sec/batch)
2018-10-15 19:15:14.286357: step 126, loss = 3.35 (39.5 examples/sec; 3.242 sec/batch)
2018-10-15 19:15:17.654285: step 127, loss = 3.32 (38.1 examples/sec; 3.363 sec/batch)
2018-10-15 19:15:21.004887: step 128, loss = 3.35 (38.3 examples/sec; 3.346 sec/batch)
2018-10-15 19:15:24.301625: step 129, loss = 3.30 (38.9 examples/sec; 3.292 sec/batch)
2018-10-15 19:15:27.617988: step 130, loss = 3.36 (38.7 examples/sec; 3.311 sec/batch)
2018-10-15 19:15:31.007536: step 131, loss = 3.34 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:15:34.352887: step 132, loss = 3.41 (38.3 examples/sec; 3.341 sec/batch)
2018-10-15 19:15:37.815646: step 133, loss = 3.31 (37.0 examples/sec; 3.459 sec/batch)
2018-10-15 19:15:41.269412: step 134, loss = 3.25 (37.1 examples/sec; 3.448 sec/batch)
2018-10-15 19:15:44.641644: step 135, loss = 3.22 (38.0 examples/sec; 3.367 sec/batch)
2018-10-15 19:15:48.002170: step 136, loss = 3.24 (38.1 examples/sec; 3.356 sec/batch)
2018-10-15 19:15:51.416721: step 137, loss = 3.27 (37.5 examples/sec; 3.409 sec/batch)
2018-10-15 19:15:54.875066: step 138, loss = 3.21 (37.1 examples/sec; 3.453 sec/batch)
2018-10-15 19:15:58.128678: step 139, loss = 3.24 (39.4 examples/sec; 3.248 sec/batch)
2018-10-15 19:16:01.554450: step 140, loss = 3.21 (37.4 examples/sec; 3.421 sec/batch)
2018-10-15 19:16:05.072214: step 141, loss = 3.34 (36.4 examples/sec; 3.513 sec/batch)
2018-10-15 19:16:08.411628: step 142, loss = 3.20 (38.4 examples/sec; 3.334 sec/batch)
2018-10-15 19:16:11.928781: step 143, loss = 3.16 (36.4 examples/sec; 3.514 sec/batch)
2018-10-15 19:16:15.298034: step 144, loss = 3.23 (38.1 examples/sec; 3.361 sec/batch)
2018-10-15 19:16:18.629600: step 145, loss = 3.20 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:16:21.978369: step 146, loss = 3.24 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:16:25.421702: step 147, loss = 3.16 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:16:28.881150: step 148, loss = 3.16 (37.1 examples/sec; 3.454 sec/batch)
2018-10-15 19:16:32.471081: step 149, loss = 3.03 (35.7 examples/sec; 3.584 sec/batch)
2018-10-15 19:16:35.915316: step 150, loss = 3.13 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:16:39.455783: step 151, loss = 3.04 (36.2 examples/sec; 3.536 sec/batch)
2018-10-15 19:16:42.877743: step 152, loss = 3.14 (37.5 examples/sec; 3.417 sec/batch)
2018-10-15 19:16:46.241609: step 153, loss = 3.27 (38.1 examples/sec; 3.360 sec/batch)
2018-10-15 19:16:49.647859: step 154, loss = 3.24 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:16:53.075464: step 155, loss = 3.17 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:16:56.385069: step 156, loss = 3.12 (38.7 examples/sec; 3.304 sec/batch)
2018-10-15 19:16:59.756716: step 157, loss = 3.06 (38.0 examples/sec; 3.369 sec/batch)
2018-10-15 19:17:03.098652: step 158, loss = 3.13 (38.4 examples/sec; 3.337 sec/batch)
2018-10-15 19:17:06.576614: step 159, loss = 3.30 (36.9 examples/sec; 3.473 sec/batch)
2018-10-15 19:17:09.956382: step 160, loss = 3.10 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:17:13.503314: step 161, loss = 3.03 (36.1 examples/sec; 3.541 sec/batch)
2018-10-15 19:17:16.944558: step 162, loss = 3.10 (37.3 examples/sec; 3.436 sec/batch)
2018-10-15 19:17:20.294032: step 163, loss = 3.26 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:17:23.902392: step 164, loss = 3.27 (35.5 examples/sec; 3.605 sec/batch)
2018-10-15 19:17:27.248726: step 165, loss = 3.17 (38.3 examples/sec; 3.342 sec/batch)
2018-10-15 19:17:30.587710: step 166, loss = 3.26 (38.4 examples/sec; 3.333 sec/batch)
2018-10-15 19:17:33.932597: step 167, loss = 3.11 (38.3 examples/sec; 3.340 sec/batch)
2018-10-15 19:17:37.327180: step 168, loss = 3.15 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:17:40.645285: step 169, loss = 3.27 (38.6 examples/sec; 3.313 sec/batch)
2018-10-15 19:17:43.981160: step 170, loss = 3.16 (38.4 examples/sec; 3.331 sec/batch)
2018-10-15 19:17:47.312357: step 171, loss = 3.13 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:17:50.642162: step 172, loss = 3.26 (38.5 examples/sec; 3.325 sec/batch)
2018-10-15 19:17:53.986826: step 173, loss = 3.20 (38.3 examples/sec; 3.340 sec/batch)
2018-10-15 19:17:57.534508: step 174, loss = 3.13 (36.1 examples/sec; 3.543 sec/batch)
2018-10-15 19:18:00.933105: step 175, loss = 3.12 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:18:04.290570: step 176, loss = 3.28 (38.2 examples/sec; 3.352 sec/batch)
2018-10-15 19:18:07.687536: step 177, loss = 3.22 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:18:11.124048: step 178, loss = 3.15 (37.3 examples/sec; 3.431 sec/batch)
2018-10-15 19:18:14.498842: step 179, loss = 3.08 (38.0 examples/sec; 3.370 sec/batch)
2018-10-15 19:18:17.963962: step 180, loss = 3.18 (37.0 examples/sec; 3.460 sec/batch)
2018-10-15 19:18:21.675152: step 181, loss = 3.25 (34.5 examples/sec; 3.706 sec/batch)
2018-10-15 19:18:25.033424: step 182, loss = 3.05 (38.2 examples/sec; 3.354 sec/batch)
2018-10-15 19:18:28.503631: step 183, loss = 3.10 (36.9 examples/sec; 3.466 sec/batch)
2018-10-15 19:18:31.940309: step 184, loss = 3.04 (37.3 examples/sec; 3.432 sec/batch)
2018-10-15 19:18:35.467933: step 185, loss = 3.11 (36.3 examples/sec; 3.523 sec/batch)
2018-10-15 19:18:39.038449: step 186, loss = 3.21 (35.9 examples/sec; 3.566 sec/batch)
2018-10-15 19:18:42.698939: step 187, loss = 3.39 (35.0 examples/sec; 3.656 sec/batch)
2018-10-15 19:18:46.054032: step 188, loss = 3.14 (38.2 examples/sec; 3.350 sec/batch)
2018-10-15 19:18:49.592514: step 189, loss = 3.15 (36.2 examples/sec; 3.534 sec/batch)
2018-10-15 19:18:53.000075: step 190, loss = 3.24 (37.6 examples/sec; 3.403 sec/batch)
2018-10-15 19:18:56.454163: step 191, loss = 3.11 (37.1 examples/sec; 3.449 sec/batch)
2018-10-15 19:18:59.803718: step 192, loss = 3.07 (38.3 examples/sec; 3.345 sec/batch)
2018-10-15 19:19:03.138893: step 193, loss = 3.11 (38.4 examples/sec; 3.332 sec/batch)
2018-10-15 19:19:06.514355: step 194, loss = 3.14 (38.0 examples/sec; 3.370 sec/batch)
2018-10-15 19:19:09.982046: step 195, loss = 3.17 (37.0 examples/sec; 3.463 sec/batch)
2018-10-15 19:19:13.320816: step 196, loss = 3.26 (38.4 examples/sec; 3.334 sec/batch)
2018-10-15 19:19:16.641970: step 197, loss = 3.24 (38.6 examples/sec; 3.318 sec/batch)
2018-10-15 19:19:20.081264: step 198, loss = 3.25 (37.3 examples/sec; 3.434 sec/batch)
2018-10-15 19:19:23.479293: step 199, loss = 3.02 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:19:26.769269: step 200, loss = 3.08 (38.9 examples/sec; 3.287 sec/batch)
2018-10-15 19:19:30.627812: step 201, loss = 3.32 (37.7 examples/sec; 3.394 sec/batch)
2018-10-15 19:19:34.032748: step 202, loss = 3.19 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:19:37.428433: step 203, loss = 2.96 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:19:40.806857: step 204, loss = 3.02 (37.9 examples/sec; 3.376 sec/batch)
2018-10-15 19:19:44.204927: step 205, loss = 3.05 (37.7 examples/sec; 3.394 sec/batch)
2018-10-15 19:19:47.669731: step 206, loss = 3.21 (37.0 examples/sec; 3.460 sec/batch)
2018-10-15 19:19:51.148099: step 207, loss = 3.19 (36.8 examples/sec; 3.474 sec/batch)
2018-10-15 19:19:54.562890: step 208, loss = 3.26 (37.5 examples/sec; 3.410 sec/batch)
2018-10-15 19:19:58.044063: step 209, loss = 3.19 (36.8 examples/sec; 3.476 sec/batch)
2018-10-15 19:20:01.489174: step 210, loss = 3.00 (37.2 examples/sec; 3.442 sec/batch)
2018-10-15 19:20:04.898028: step 211, loss = 3.08 (37.6 examples/sec; 3.404 sec/batch)
2018-10-15 19:20:08.407792: step 212, loss = 3.04 (36.5 examples/sec; 3.505 sec/batch)
2018-10-15 19:20:11.741638: step 213, loss = 3.03 (38.4 examples/sec; 3.329 sec/batch)
2018-10-15 19:20:15.092828: step 214, loss = 2.96 (38.2 examples/sec; 3.346 sec/batch)
2018-10-15 19:20:18.550844: step 215, loss = 3.04 (37.1 examples/sec; 3.453 sec/batch)
2018-10-15 19:20:21.916456: step 216, loss = 3.10 (38.1 examples/sec; 3.363 sec/batch)
2018-10-15 19:20:25.262971: step 217, loss = 3.23 (38.3 examples/sec; 3.342 sec/batch)
2018-10-15 19:20:28.610968: step 218, loss = 3.16 (38.3 examples/sec; 3.343 sec/batch)
2018-10-15 19:20:32.055097: step 219, loss = 3.12 (37.2 examples/sec; 3.440 sec/batch)
2018-10-15 19:20:35.417857: step 220, loss = 2.98 (38.1 examples/sec; 3.358 sec/batch)
2018-10-15 19:20:38.808321: step 221, loss = 2.97 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:20:42.169128: step 222, loss = 2.95 (38.1 examples/sec; 3.358 sec/batch)
2018-10-15 19:20:45.498567: step 223, loss = 3.15 (38.5 examples/sec; 3.327 sec/batch)
2018-10-15 19:20:48.875822: step 224, loss = 3.19 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:20:52.272196: step 225, loss = 3.07 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:20:55.620839: step 226, loss = 3.03 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:20:59.107880: step 227, loss = 3.08 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:21:02.677017: step 228, loss = 3.19 (35.9 examples/sec; 3.564 sec/batch)
2018-10-15 19:21:06.060544: step 229, loss = 3.17 (37.9 examples/sec; 3.378 sec/batch)
2018-10-15 19:21:09.409370: step 230, loss = 3.18 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:21:12.931128: step 231, loss = 3.10 (36.4 examples/sec; 3.517 sec/batch)
2018-10-15 19:21:16.437923: step 232, loss = 3.12 (36.6 examples/sec; 3.502 sec/batch)
2018-10-15 19:21:19.834629: step 233, loss = 3.20 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:21:23.342380: step 234, loss = 3.15 (36.5 examples/sec; 3.503 sec/batch)
2018-10-15 19:21:26.864459: step 235, loss = 3.18 (36.4 examples/sec; 3.517 sec/batch)
2018-10-15 19:21:30.388228: step 236, loss = 3.32 (36.4 examples/sec; 3.518 sec/batch)
2018-10-15 19:21:33.749862: step 237, loss = 3.01 (38.1 examples/sec; 3.359 sec/batch)
2018-10-15 19:21:37.332086: step 238, loss = 2.98 (35.8 examples/sec; 3.577 sec/batch)
2018-10-15 19:21:40.861604: step 239, loss = 3.15 (36.3 examples/sec; 3.525 sec/batch)
2018-10-15 19:21:44.407994: step 240, loss = 3.10 (36.1 examples/sec; 3.542 sec/batch)
2018-10-15 19:21:47.755089: step 241, loss = 3.17 (38.3 examples/sec; 3.342 sec/batch)
2018-10-15 19:21:51.120606: step 242, loss = 3.07 (38.1 examples/sec; 3.361 sec/batch)
2018-10-15 19:21:54.416727: step 243, loss = 3.11 (38.9 examples/sec; 3.290 sec/batch)
2018-10-15 19:21:57.840695: step 244, loss = 2.94 (37.4 examples/sec; 3.419 sec/batch)
2018-10-15 19:22:01.323331: step 245, loss = 3.17 (36.8 examples/sec; 3.478 sec/batch)
2018-10-15 19:22:04.658464: step 246, loss = 3.04 (38.4 examples/sec; 3.331 sec/batch)
2018-10-15 19:22:07.997985: step 247, loss = 3.09 (38.4 examples/sec; 3.335 sec/batch)
2018-10-15 19:22:11.307186: step 248, loss = 3.20 (38.7 examples/sec; 3.305 sec/batch)
2018-10-15 19:22:14.810342: step 249, loss = 3.15 (36.6 examples/sec; 3.498 sec/batch)
2018-10-15 19:22:18.216132: step 250, loss = 2.94 (37.6 examples/sec; 3.400 sec/batch)
2018-10-15 19:22:21.804676: step 251, loss = 2.97 (35.7 examples/sec; 3.583 sec/batch)
2018-10-15 19:22:25.132040: step 252, loss = 3.05 (38.5 examples/sec; 3.325 sec/batch)
2018-10-15 19:22:28.603215: step 253, loss = 2.94 (36.9 examples/sec; 3.466 sec/batch)
2018-10-15 19:22:32.028505: step 254, loss = 3.05 (37.4 examples/sec; 3.420 sec/batch)
2018-10-15 19:22:35.473644: step 255, loss = 3.09 (37.2 examples/sec; 3.440 sec/batch)
2018-10-15 19:22:38.879702: step 256, loss = 2.97 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:22:42.496770: step 257, loss = 2.92 (35.4 examples/sec; 3.613 sec/batch)
2018-10-15 19:22:45.848568: step 258, loss = 3.05 (38.2 examples/sec; 3.347 sec/batch)
2018-10-15 19:22:49.456294: step 259, loss = 3.01 (35.5 examples/sec; 3.603 sec/batch)
2018-10-15 19:22:52.900129: step 260, loss = 3.00 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:22:56.326750: step 261, loss = 2.96 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:22:59.808285: step 262, loss = 3.03 (36.8 examples/sec; 3.477 sec/batch)
2018-10-15 19:23:03.225325: step 263, loss = 3.02 (37.5 examples/sec; 3.414 sec/batch)
2018-10-15 19:23:06.685728: step 264, loss = 3.05 (37.0 examples/sec; 3.456 sec/batch)
2018-10-15 19:23:10.091894: step 265, loss = 2.99 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:23:13.419552: step 266, loss = 3.05 (38.5 examples/sec; 3.323 sec/batch)
2018-10-15 19:23:16.950055: step 267, loss = 3.06 (36.3 examples/sec; 3.525 sec/batch)
2018-10-15 19:23:20.433348: step 268, loss = 3.03 (36.8 examples/sec; 3.478 sec/batch)
2018-10-15 19:23:23.871314: step 269, loss = 2.99 (37.3 examples/sec; 3.434 sec/batch)
2018-10-15 19:23:27.303468: step 270, loss = 3.03 (37.3 examples/sec; 3.427 sec/batch)
2018-10-15 19:23:30.807387: step 271, loss = 3.02 (36.6 examples/sec; 3.497 sec/batch)
2018-10-15 19:23:34.135309: step 272, loss = 3.18 (38.5 examples/sec; 3.323 sec/batch)
2018-10-15 19:23:37.549118: step 273, loss = 3.12 (37.5 examples/sec; 3.411 sec/batch)
2018-10-15 19:23:41.116369: step 274, loss = 3.17 (35.9 examples/sec; 3.561 sec/batch)
2018-10-15 19:23:44.629165: step 275, loss = 3.03 (36.5 examples/sec; 3.507 sec/batch)
2018-10-15 19:23:47.947020: step 276, loss = 3.17 (38.6 examples/sec; 3.313 sec/batch)
2018-10-15 19:23:51.311457: step 277, loss = 3.12 (38.1 examples/sec; 3.361 sec/batch)
2018-10-15 19:23:54.846203: step 278, loss = 3.09 (36.3 examples/sec; 3.530 sec/batch)
2018-10-15 19:23:58.225981: step 279, loss = 2.99 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:24:01.602217: step 280, loss = 3.11 (38.0 examples/sec; 3.371 sec/batch)
2018-10-15 19:24:04.962278: step 281, loss = 3.07 (38.2 examples/sec; 3.355 sec/batch)
2018-10-15 19:24:08.355520: step 282, loss = 3.10 (37.8 examples/sec; 3.388 sec/batch)
2018-10-15 19:24:11.782257: step 283, loss = 3.01 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:24:15.157595: step 284, loss = 3.09 (38.0 examples/sec; 3.371 sec/batch)
2018-10-15 19:24:18.675329: step 285, loss = 3.06 (36.4 examples/sec; 3.513 sec/batch)
2018-10-15 19:24:22.063676: step 286, loss = 2.90 (37.8 examples/sec; 3.383 sec/batch)
2018-10-15 19:24:25.429425: step 287, loss = 2.93 (38.1 examples/sec; 3.363 sec/batch)
2018-10-15 19:24:28.806767: step 288, loss = 3.06 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:24:32.183558: step 289, loss = 3.14 (37.9 examples/sec; 3.374 sec/batch)
2018-10-15 19:24:35.691756: step 290, loss = 3.17 (36.5 examples/sec; 3.503 sec/batch)
2018-10-15 19:24:39.009792: step 291, loss = 3.13 (38.6 examples/sec; 3.313 sec/batch)
2018-10-15 19:24:42.407347: step 292, loss = 3.03 (37.7 examples/sec; 3.395 sec/batch)
2018-10-15 19:24:45.819686: step 293, loss = 3.01 (37.6 examples/sec; 3.408 sec/batch)
2018-10-15 19:24:49.299594: step 294, loss = 2.99 (36.8 examples/sec; 3.475 sec/batch)
2018-10-15 19:24:52.683503: step 295, loss = 3.01 (37.9 examples/sec; 3.379 sec/batch)
2018-10-15 19:24:56.262525: step 296, loss = 3.15 (35.8 examples/sec; 3.574 sec/batch)
2018-10-15 19:24:59.625882: step 297, loss = 2.91 (38.1 examples/sec; 3.359 sec/batch)
2018-10-15 19:25:03.047715: step 298, loss = 2.90 (37.5 examples/sec; 3.418 sec/batch)
2018-10-15 19:25:06.663743: step 299, loss = 3.00 (35.4 examples/sec; 3.611 sec/batch)
2018-10-15 19:25:10.088841: step 300, loss = 3.01 (37.4 examples/sec; 3.420 sec/batch)
2018-10-15 19:25:13.986812: step 301, loss = 3.04 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:25:17.382248: step 302, loss = 3.02 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:25:20.837713: step 303, loss = 3.02 (37.1 examples/sec; 3.451 sec/batch)
2018-10-15 19:25:24.368141: step 304, loss = 3.10 (36.3 examples/sec; 3.526 sec/batch)
2018-10-15 19:25:27.818913: step 305, loss = 3.09 (37.1 examples/sec; 3.446 sec/batch)
2018-10-15 19:25:31.240967: step 306, loss = 2.98 (37.5 examples/sec; 3.417 sec/batch)
2018-10-15 19:25:34.648075: step 307, loss = 3.00 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:25:38.094188: step 308, loss = 3.14 (37.2 examples/sec; 3.443 sec/batch)
2018-10-15 19:25:41.582379: step 309, loss = 3.03 (36.8 examples/sec; 3.483 sec/batch)
2018-10-15 19:25:45.148724: step 310, loss = 3.05 (35.9 examples/sec; 3.562 sec/batch)
2018-10-15 19:25:48.592315: step 311, loss = 3.07 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:25:51.880030: step 312, loss = 3.01 (39.0 examples/sec; 3.283 sec/batch)
2018-10-15 19:25:55.507118: step 313, loss = 3.06 (35.3 examples/sec; 3.624 sec/batch)
2018-10-15 19:25:59.031312: step 314, loss = 3.08 (36.4 examples/sec; 3.519 sec/batch)
2018-10-15 19:26:02.593792: step 315, loss = 3.01 (36.0 examples/sec; 3.557 sec/batch)
2018-10-15 19:26:06.008047: step 316, loss = 3.00 (37.5 examples/sec; 3.410 sec/batch)
2018-10-15 19:26:09.434068: step 317, loss = 2.92 (37.4 examples/sec; 3.423 sec/batch)
2018-10-15 19:26:12.747297: step 318, loss = 2.94 (38.7 examples/sec; 3.310 sec/batch)
2018-10-15 19:26:16.185834: step 319, loss = 2.99 (37.3 examples/sec; 3.434 sec/batch)
2018-10-15 19:26:19.674788: step 320, loss = 2.99 (36.7 examples/sec; 3.484 sec/batch)
2018-10-15 19:26:23.080546: step 321, loss = 3.09 (37.6 examples/sec; 3.400 sec/batch)
2018-10-15 19:26:26.658339: step 322, loss = 3.24 (35.8 examples/sec; 3.573 sec/batch)
2018-10-15 19:26:29.965614: step 323, loss = 2.93 (38.7 examples/sec; 3.304 sec/batch)
2018-10-15 19:26:33.443196: step 324, loss = 2.99 (36.9 examples/sec; 3.472 sec/batch)
2018-10-15 19:26:36.811218: step 325, loss = 2.96 (38.1 examples/sec; 3.363 sec/batch)
2018-10-15 19:26:40.293576: step 326, loss = 3.04 (36.8 examples/sec; 3.479 sec/batch)
2018-10-15 19:26:43.680797: step 327, loss = 3.01 (37.8 examples/sec; 3.384 sec/batch)
2018-10-15 19:26:47.077493: step 328, loss = 2.98 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:26:50.581266: step 329, loss = 2.97 (36.6 examples/sec; 3.499 sec/batch)
2018-10-15 19:26:53.993342: step 330, loss = 2.99 (37.6 examples/sec; 3.407 sec/batch)
2018-10-15 19:26:57.544538: step 331, loss = 2.96 (36.1 examples/sec; 3.546 sec/batch)
2018-10-15 19:27:00.974030: step 332, loss = 3.08 (37.4 examples/sec; 3.424 sec/batch)
2018-10-15 19:27:04.438559: step 333, loss = 2.99 (37.0 examples/sec; 3.460 sec/batch)
2018-10-15 19:27:07.828740: step 334, loss = 3.16 (37.8 examples/sec; 3.387 sec/batch)
2018-10-15 19:27:11.265642: step 335, loss = 2.99 (37.3 examples/sec; 3.435 sec/batch)
2018-10-15 19:27:14.610624: step 336, loss = 3.14 (38.3 examples/sec; 3.340 sec/batch)
2018-10-15 19:27:18.115822: step 337, loss = 3.19 (36.6 examples/sec; 3.500 sec/batch)
2018-10-15 19:27:21.511609: step 338, loss = 3.21 (37.8 examples/sec; 3.391 sec/batch)
2018-10-15 19:27:24.953739: step 339, loss = 3.19 (37.2 examples/sec; 3.437 sec/batch)
2018-10-15 19:27:28.385431: step 340, loss = 3.14 (37.4 examples/sec; 3.426 sec/batch)
2018-10-15 19:27:31.841809: step 341, loss = 2.95 (37.1 examples/sec; 3.452 sec/batch)
2018-10-15 19:27:35.215240: step 342, loss = 2.90 (38.0 examples/sec; 3.368 sec/batch)
2018-10-15 19:27:38.763268: step 343, loss = 3.07 (36.1 examples/sec; 3.543 sec/batch)
2018-10-15 19:27:42.233239: step 344, loss = 3.02 (36.9 examples/sec; 3.465 sec/batch)
2018-10-15 19:27:45.670597: step 345, loss = 2.91 (37.3 examples/sec; 3.432 sec/batch)
2018-10-15 19:27:49.148143: step 346, loss = 2.90 (36.8 examples/sec; 3.475 sec/batch)
2018-10-15 19:27:52.523182: step 347, loss = 2.99 (38.0 examples/sec; 3.370 sec/batch)
2018-10-15 19:27:55.872192: step 348, loss = 3.12 (38.3 examples/sec; 3.346 sec/batch)
2018-10-15 19:27:59.198481: step 349, loss = 3.13 (38.5 examples/sec; 3.322 sec/batch)
2018-10-15 19:28:02.811529: step 350, loss = 2.86 (35.5 examples/sec; 3.608 sec/batch)
2018-10-15 19:28:06.210177: step 351, loss = 3.06 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:28:09.653451: step 352, loss = 3.01 (37.2 examples/sec; 3.437 sec/batch)
2018-10-15 19:28:13.162797: step 353, loss = 2.99 (36.5 examples/sec; 3.504 sec/batch)
2018-10-15 19:28:16.709720: step 354, loss = 2.97 (36.1 examples/sec; 3.542 sec/batch)
2018-10-15 19:28:20.055418: step 355, loss = 2.86 (38.3 examples/sec; 3.343 sec/batch)
2018-10-15 19:28:23.437012: step 356, loss = 2.86 (37.9 examples/sec; 3.376 sec/batch)
2018-10-15 19:28:26.821897: step 357, loss = 2.92 (37.9 examples/sec; 3.380 sec/batch)
2018-10-15 19:28:30.326589: step 358, loss = 2.94 (36.6 examples/sec; 3.499 sec/batch)
2018-10-15 19:28:33.721584: step 359, loss = 3.04 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:28:37.189447: step 360, loss = 2.84 (37.0 examples/sec; 3.463 sec/batch)
2018-10-15 19:28:40.604725: step 361, loss = 2.99 (37.5 examples/sec; 3.410 sec/batch)
2018-10-15 19:28:44.025011: step 362, loss = 3.01 (37.5 examples/sec; 3.415 sec/batch)
2018-10-15 19:28:47.456753: step 363, loss = 2.90 (37.4 examples/sec; 3.427 sec/batch)
2018-10-15 19:28:50.955800: step 364, loss = 2.97 (36.6 examples/sec; 3.493 sec/batch)
2018-10-15 19:28:54.363979: step 365, loss = 2.85 (37.6 examples/sec; 3.405 sec/batch)
2018-10-15 19:28:57.731811: step 366, loss = 2.85 (38.1 examples/sec; 3.362 sec/batch)
2018-10-15 19:29:01.187974: step 367, loss = 2.91 (37.1 examples/sec; 3.451 sec/batch)
2018-10-15 19:29:04.778063: step 368, loss = 2.94 (35.7 examples/sec; 3.586 sec/batch)
2018-10-15 19:29:08.167434: step 369, loss = 2.92 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:29:11.590161: step 370, loss = 3.00 (37.4 examples/sec; 3.420 sec/batch)
2018-10-15 19:29:14.978725: step 371, loss = 2.94 (37.8 examples/sec; 3.386 sec/batch)
2018-10-15 19:29:18.513530: step 372, loss = 2.99 (36.3 examples/sec; 3.530 sec/batch)
2018-10-15 19:29:21.993953: step 373, loss = 3.00 (36.8 examples/sec; 3.476 sec/batch)
2018-10-15 19:29:25.397528: step 374, loss = 2.96 (37.7 examples/sec; 3.399 sec/batch)
2018-10-15 19:29:28.906300: step 375, loss = 2.99 (36.5 examples/sec; 3.504 sec/batch)
2018-10-15 19:29:32.537155: step 376, loss = 3.01 (35.3 examples/sec; 3.627 sec/batch)
2018-10-15 19:29:35.899931: step 377, loss = 3.15 (38.1 examples/sec; 3.360 sec/batch)
2018-10-15 19:29:39.258616: step 378, loss = 2.99 (38.1 examples/sec; 3.356 sec/batch)
2018-10-15 19:29:42.847116: step 379, loss = 2.97 (35.7 examples/sec; 3.584 sec/batch)
2018-10-15 19:29:46.222597: step 380, loss = 3.03 (38.0 examples/sec; 3.371 sec/batch)
2018-10-15 19:29:49.684257: step 381, loss = 3.08 (37.0 examples/sec; 3.456 sec/batch)
2018-10-15 19:29:53.050237: step 382, loss = 3.18 (38.1 examples/sec; 3.361 sec/batch)
2018-10-15 19:29:56.673554: step 383, loss = 3.03 (35.4 examples/sec; 3.618 sec/batch)
2018-10-15 19:30:00.056417: step 384, loss = 2.97 (37.9 examples/sec; 3.378 sec/batch)
2018-10-15 19:30:03.506539: step 385, loss = 2.92 (37.1 examples/sec; 3.446 sec/batch)
2018-10-15 19:30:06.854289: step 386, loss = 2.96 (38.3 examples/sec; 3.343 sec/batch)
2018-10-15 19:30:10.242144: step 387, loss = 2.94 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:30:13.783512: step 388, loss = 2.93 (36.2 examples/sec; 3.538 sec/batch)
2018-10-15 19:30:17.280812: step 389, loss = 3.06 (36.6 examples/sec; 3.493 sec/batch)
2018-10-15 19:30:20.934924: step 390, loss = 2.93 (35.1 examples/sec; 3.649 sec/batch)
2018-10-15 19:30:24.292508: step 391, loss = 2.92 (38.2 examples/sec; 3.355 sec/batch)
2018-10-15 19:30:27.691956: step 392, loss = 2.89 (37.7 examples/sec; 3.397 sec/batch)
2018-10-15 19:30:31.038110: step 393, loss = 2.92 (38.3 examples/sec; 3.343 sec/batch)
2018-10-15 19:30:34.586189: step 394, loss = 2.85 (36.1 examples/sec; 3.545 sec/batch)
2018-10-15 19:30:38.027990: step 395, loss = 3.07 (37.2 examples/sec; 3.437 sec/batch)
2018-10-15 19:30:41.565751: step 396, loss = 3.04 (36.2 examples/sec; 3.533 sec/batch)
2018-10-15 19:30:44.987654: step 397, loss = 3.09 (37.5 examples/sec; 3.417 sec/batch)
2018-10-15 19:30:48.409028: step 398, loss = 2.97 (37.5 examples/sec; 3.416 sec/batch)
2018-10-15 19:30:51.750184: step 399, loss = 2.99 (38.3 examples/sec; 3.338 sec/batch)
2018-10-15 19:30:55.241109: step 400, loss = 3.01 (36.7 examples/sec; 3.486 sec/batch)
2018-10-15 19:30:59.194110: step 401, loss = 2.80 (37.9 examples/sec; 3.382 sec/batch)
2018-10-15 19:31:02.516487: step 402, loss = 2.97 (38.6 examples/sec; 3.320 sec/batch)
2018-10-15 19:31:05.928871: step 403, loss = 2.85 (37.6 examples/sec; 3.408 sec/batch)
2018-10-15 19:31:09.352705: step 404, loss = 2.96 (37.4 examples/sec; 3.418 sec/batch)
2018-10-15 19:31:12.880527: step 405, loss = 3.07 (36.3 examples/sec; 3.522 sec/batch)
2018-10-15 19:31:16.293798: step 406, loss = 2.86 (37.6 examples/sec; 3.408 sec/batch)
2018-10-15 19:31:19.648769: step 407, loss = 2.84 (38.2 examples/sec; 3.350 sec/batch)
2018-10-15 19:31:23.032115: step 408, loss = 2.92 (37.9 examples/sec; 3.381 sec/batch)
2018-10-15 19:31:26.617110: step 409, loss = 3.02 (35.8 examples/sec; 3.580 sec/batch)
2018-10-15 19:31:30.072980: step 410, loss = 2.99 (37.1 examples/sec; 3.453 sec/batch)
2018-10-15 19:31:33.472146: step 411, loss = 2.95 (37.7 examples/sec; 3.394 sec/batch)
2018-10-15 19:31:36.872772: step 412, loss = 3.11 (37.7 examples/sec; 3.396 sec/batch)
2018-10-15 19:31:40.270441: step 413, loss = 2.88 (37.7 examples/sec; 3.395 sec/batch)
2018-10-15 19:31:43.803436: step 414, loss = 2.96 (36.3 examples/sec; 3.528 sec/batch)
2018-10-15 19:31:47.277135: step 415, loss = 2.99 (36.9 examples/sec; 3.469 sec/batch)
2018-10-15 19:31:50.849959: step 416, loss = 2.86 (35.9 examples/sec; 3.568 sec/batch)
2018-10-15 19:31:54.320118: step 417, loss = 2.91 (36.9 examples/sec; 3.466 sec/batch)
2018-10-15 19:31:57.749261: step 418, loss = 2.93 (37.4 examples/sec; 3.424 sec/batch)
2018-10-15 19:32:01.176657: step 419, loss = 2.93 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:32:04.615450: step 420, loss = 2.88 (37.3 examples/sec; 3.434 sec/batch)
2018-10-15 19:32:07.901701: step 421, loss = 2.80 (39.0 examples/sec; 3.281 sec/batch)
2018-10-15 19:32:11.301261: step 422, loss = 2.84 (37.7 examples/sec; 3.394 sec/batch)
2018-10-15 19:32:14.651741: step 423, loss = 2.99 (38.3 examples/sec; 3.345 sec/batch)
2018-10-15 19:32:18.093728: step 424, loss = 3.05 (37.2 examples/sec; 3.439 sec/batch)
2018-10-15 19:32:21.510869: step 425, loss = 3.00 (37.5 examples/sec; 3.414 sec/batch)
2018-10-15 19:32:25.025218: step 426, loss = 3.01 (36.5 examples/sec; 3.509 sec/batch)
2018-10-15 19:32:28.460471: step 427, loss = 2.87 (37.3 examples/sec; 3.430 sec/batch)
2018-10-15 19:32:32.001251: step 428, loss = 2.93 (36.2 examples/sec; 3.538 sec/batch)
2018-10-15 19:32:35.452669: step 429, loss = 2.83 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:32:39.077174: step 430, loss = 2.90 (35.4 examples/sec; 3.620 sec/batch)
2018-10-15 19:32:42.526906: step 431, loss = 2.78 (37.2 examples/sec; 3.445 sec/batch)
2018-10-15 19:32:46.027108: step 432, loss = 2.87 (36.6 examples/sec; 3.497 sec/batch)
2018-10-15 19:32:49.417670: step 433, loss = 3.06 (37.8 examples/sec; 3.386 sec/batch)
2018-10-15 19:32:52.833720: step 434, loss = 2.91 (37.5 examples/sec; 3.411 sec/batch)
2018-10-15 19:32:56.189013: step 435, loss = 2.97 (38.2 examples/sec; 3.352 sec/batch)
2018-10-15 19:32:59.634692: step 436, loss = 2.90 (37.2 examples/sec; 3.441 sec/batch)
2018-10-15 19:33:03.186367: step 437, loss = 2.88 (36.1 examples/sec; 3.549 sec/batch)
2018-10-15 19:33:06.553343: step 438, loss = 3.03 (38.1 examples/sec; 3.362 sec/batch)
2018-10-15 19:33:10.012215: step 439, loss = 2.93 (37.1 examples/sec; 3.454 sec/batch)
2018-10-15 19:33:13.336069: step 440, loss = 2.93 (38.6 examples/sec; 3.319 sec/batch)
2018-10-15 19:33:16.847592: step 441, loss = 2.98 (36.5 examples/sec; 3.507 sec/batch)
2018-10-15 19:33:20.299707: step 442, loss = 3.07 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:33:23.798629: step 443, loss = 3.08 (36.6 examples/sec; 3.494 sec/batch)
2018-10-15 19:33:27.195700: step 444, loss = 2.95 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:33:30.680968: step 445, loss = 2.85 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:33:34.127439: step 446, loss = 2.89 (37.2 examples/sec; 3.444 sec/batch)
2018-10-15 19:33:37.541249: step 447, loss = 2.90 (37.5 examples/sec; 3.409 sec/batch)
2018-10-15 19:33:40.994476: step 448, loss = 2.86 (37.1 examples/sec; 3.450 sec/batch)
2018-10-15 19:33:44.428498: step 449, loss = 2.83 (37.3 examples/sec; 3.429 sec/batch)
2018-10-15 19:33:47.820075: step 450, loss = 2.95 (37.8 examples/sec; 3.387 sec/batch)
2018-10-15 19:33:51.358654: step 451, loss = 2.93 (36.2 examples/sec; 3.534 sec/batch)
2018-10-15 19:33:54.800083: step 452, loss = 2.87 (37.2 examples/sec; 3.437 sec/batch)
2018-10-15 19:33:58.284533: step 453, loss = 2.96 (36.8 examples/sec; 3.479 sec/batch)
2018-10-15 19:34:01.606089: step 454, loss = 2.93 (38.6 examples/sec; 3.318 sec/batch)
2018-10-15 19:34:05.012468: step 455, loss = 2.87 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:34:08.448622: step 456, loss = 3.04 (37.3 examples/sec; 3.431 sec/batch)
2018-10-15 19:34:11.926204: step 457, loss = 2.78 (36.9 examples/sec; 3.473 sec/batch)
2018-10-15 19:34:15.364346: step 458, loss = 2.81 (37.3 examples/sec; 3.433 sec/batch)
2018-10-15 19:34:18.775977: step 459, loss = 2.76 (37.6 examples/sec; 3.407 sec/batch)
2018-10-15 19:34:22.241044: step 460, loss = 2.74 (37.0 examples/sec; 3.460 sec/batch)
2018-10-15 19:34:25.796772: step 461, loss = 2.91 (36.0 examples/sec; 3.552 sec/batch)
2018-10-15 19:34:29.213370: step 462, loss = 2.86 (37.5 examples/sec; 3.411 sec/batch)
2018-10-15 19:34:32.717755: step 463, loss = 2.92 (36.6 examples/sec; 3.502 sec/batch)
2018-10-15 19:34:36.188892: step 464, loss = 2.66 (36.9 examples/sec; 3.468 sec/batch)
2018-10-15 19:34:39.774823: step 465, loss = 2.90 (35.7 examples/sec; 3.582 sec/batch)
2018-10-15 19:34:43.173205: step 466, loss = 2.75 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:34:46.770508: step 467, loss = 2.90 (35.6 examples/sec; 3.592 sec/batch)
2018-10-15 19:34:50.223607: step 468, loss = 2.75 (37.1 examples/sec; 3.448 sec/batch)
2018-10-15 19:34:53.582415: step 469, loss = 2.79 (38.2 examples/sec; 3.355 sec/batch)
2018-10-15 19:34:57.043610: step 470, loss = 2.82 (37.0 examples/sec; 3.458 sec/batch)
2018-10-15 19:35:00.439256: step 471, loss = 2.81 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:35:03.956320: step 472, loss = 2.89 (36.4 examples/sec; 3.515 sec/batch)
2018-10-15 19:35:07.442912: step 473, loss = 2.86 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:35:11.098595: step 474, loss = 2.81 (35.1 examples/sec; 3.651 sec/batch)
2018-10-15 19:35:14.497802: step 475, loss = 2.81 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:35:17.930551: step 476, loss = 2.95 (37.3 examples/sec; 3.427 sec/batch)
2018-10-15 19:35:21.331085: step 477, loss = 2.86 (37.7 examples/sec; 3.396 sec/batch)
2018-10-15 19:35:24.914545: step 478, loss = 2.91 (35.8 examples/sec; 3.578 sec/batch)
2018-10-15 19:35:28.316999: step 479, loss = 2.97 (37.7 examples/sec; 3.397 sec/batch)
2018-10-15 19:35:31.941542: step 480, loss = 2.94 (35.4 examples/sec; 3.620 sec/batch)
2018-10-15 19:35:35.360589: step 481, loss = 2.90 (37.5 examples/sec; 3.416 sec/batch)
2018-10-15 19:35:38.939035: step 482, loss = 2.83 (35.8 examples/sec; 3.576 sec/batch)
2018-10-15 19:35:42.337366: step 483, loss = 2.89 (37.7 examples/sec; 3.393 sec/batch)
2018-10-15 19:35:45.665286: step 484, loss = 2.93 (38.5 examples/sec; 3.323 sec/batch)
2018-10-15 19:35:49.209944: step 485, loss = 3.04 (36.2 examples/sec; 3.540 sec/batch)
2018-10-15 19:35:52.663987: step 486, loss = 2.93 (37.1 examples/sec; 3.449 sec/batch)
2018-10-15 19:35:56.042000: step 487, loss = 3.04 (37.9 examples/sec; 3.374 sec/batch)
2018-10-15 19:35:59.516992: step 488, loss = 3.02 (36.9 examples/sec; 3.472 sec/batch)
2018-10-15 19:36:02.870847: step 489, loss = 2.86 (38.2 examples/sec; 3.349 sec/batch)
2018-10-15 19:36:06.351877: step 490, loss = 2.79 (36.8 examples/sec; 3.476 sec/batch)
2018-10-15 19:36:09.800891: step 491, loss = 2.87 (37.1 examples/sec; 3.446 sec/batch)
2018-10-15 19:36:13.311315: step 492, loss = 2.82 (36.5 examples/sec; 3.508 sec/batch)
2018-10-15 19:36:16.716303: step 493, loss = 2.93 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:36:20.154412: step 494, loss = 2.82 (37.3 examples/sec; 3.435 sec/batch)
2018-10-15 19:36:23.603788: step 495, loss = 2.84 (37.2 examples/sec; 3.445 sec/batch)
2018-10-15 19:36:27.055824: step 496, loss = 2.84 (37.1 examples/sec; 3.446 sec/batch)
2018-10-15 19:36:30.450502: step 497, loss = 2.88 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:36:33.799114: step 498, loss = 2.91 (38.3 examples/sec; 3.344 sec/batch)
2018-10-15 19:36:37.175478: step 499, loss = 2.93 (38.0 examples/sec; 3.371 sec/batch)
2018-10-15 19:36:40.593338: step 500, loss = 3.00 (37.5 examples/sec; 3.413 sec/batch)
2018-10-15 19:36:44.519679: step 501, loss = 2.87 (37.2 examples/sec; 3.444 sec/batch)
2018-10-15 19:36:47.998623: step 502, loss = 3.00 (36.9 examples/sec; 3.473 sec/batch)
2018-10-15 19:36:51.650782: step 503, loss = 3.06 (35.1 examples/sec; 3.647 sec/batch)
2018-10-15 19:36:55.047658: step 504, loss = 2.84 (37.7 examples/sec; 3.392 sec/batch)
2018-10-15 19:36:58.495081: step 505, loss = 2.77 (37.2 examples/sec; 3.443 sec/batch)
2018-10-15 19:37:01.947056: step 506, loss = 2.91 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:37:05.432174: step 507, loss = 2.90 (36.8 examples/sec; 3.480 sec/batch)
2018-10-15 19:37:08.833884: step 508, loss = 2.75 (37.7 examples/sec; 3.398 sec/batch)
2018-10-15 19:37:12.204952: step 509, loss = 2.77 (38.0 examples/sec; 3.368 sec/batch)
2018-10-15 19:37:15.687941: step 510, loss = 2.94 (36.8 examples/sec; 3.478 sec/batch)
2018-10-15 19:37:19.069287: step 511, loss = 2.95 (37.9 examples/sec; 3.377 sec/batch)
2018-10-15 19:37:22.505388: step 512, loss = 2.93 (37.3 examples/sec; 3.431 sec/batch)
2018-10-15 19:37:25.926381: step 513, loss = 2.90 (37.4 examples/sec; 3.418 sec/batch)
2018-10-15 19:37:29.356542: step 514, loss = 2.98 (37.4 examples/sec; 3.425 sec/batch)
2018-10-15 19:37:32.767607: step 515, loss = 2.80 (37.6 examples/sec; 3.406 sec/batch)
2018-10-15 19:37:36.200183: step 516, loss = 3.28 (37.3 examples/sec; 3.428 sec/batch)
2018-10-15 19:37:39.616088: step 517, loss = 2.96 (37.5 examples/sec; 3.411 sec/batch)
2018-10-15 19:37:43.042850: step 518, loss = 2.87 (37.4 examples/sec; 3.422 sec/batch)
2018-10-15 19:37:46.454112: step 519, loss = 2.94 (37.6 examples/sec; 3.406 sec/batch)
2018-10-15 19:37:49.942755: step 520, loss = 2.85 (36.7 examples/sec; 3.484 sec/batch)
2018-10-15 19:37:53.361405: step 521, loss = 2.95 (37.5 examples/sec; 3.416 sec/batch)
2018-10-15 19:37:56.733970: step 522, loss = 2.92 (38.0 examples/sec; 3.367 sec/batch)
2018-10-15 19:38:00.245559: step 523, loss = 2.77 (36.5 examples/sec; 3.506 sec/batch)
2018-10-15 19:38:03.684195: step 524, loss = 2.89 (37.3 examples/sec; 3.436 sec/batch)
2018-10-15 19:38:07.150767: step 525, loss = 2.85 (37.0 examples/sec; 3.462 sec/batch)
2018-10-15 19:38:10.509991: step 526, loss = 2.90 (38.2 examples/sec; 3.354 sec/batch)
2018-10-15 19:38:14.001913: step 527, loss = 2.85 (36.7 examples/sec; 3.487 sec/batch)
2018-10-15 19:38:17.523200: step 528, loss = 3.02 (36.4 examples/sec; 3.516 sec/batch)
2018-10-15 19:38:21.022981: step 529, loss = 2.94 (36.6 examples/sec; 3.495 sec/batch)
2018-10-15 19:38:24.414726: step 530, loss = 2.84 (37.8 examples/sec; 3.387 sec/batch)
2018-10-15 19:38:27.804977: step 531, loss = 3.02 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:38:31.327762: step 532, loss = 2.81 (36.4 examples/sec; 3.518 sec/batch)
2018-10-15 19:38:34.703889: step 533, loss = 2.81 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:38:38.183199: step 534, loss = 2.81 (36.8 examples/sec; 3.475 sec/batch)
2018-10-15 19:38:41.678136: step 535, loss = 2.80 (36.7 examples/sec; 3.490 sec/batch)
2018-10-15 19:38:45.094445: step 536, loss = 2.88 (37.5 examples/sec; 3.413 sec/batch)
2018-10-15 19:38:48.644497: step 537, loss = 2.89 (36.1 examples/sec; 3.547 sec/batch)
2018-10-15 19:38:52.034992: step 538, loss = 2.89 (37.8 examples/sec; 3.385 sec/batch)
2018-10-15 19:38:55.505193: step 539, loss = 2.94 (36.9 examples/sec; 3.467 sec/batch)
2018-10-15 19:38:58.988265: step 540, loss = 2.92 (36.8 examples/sec; 3.478 sec/batch)
2018-10-15 19:39:02.499193: step 541, loss = 2.97 (36.5 examples/sec; 3.505 sec/batch)
2018-10-15 19:39:05.876134: step 542, loss = 2.94 (38.0 examples/sec; 3.372 sec/batch)
2018-10-15 19:39:09.327938: step 543, loss = 3.01 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:39:12.818450: step 544, loss = 2.94 (36.7 examples/sec; 3.487 sec/batch)
2018-10-15 19:39:16.288249: step 545, loss = 2.98 (36.9 examples/sec; 3.465 sec/batch)
2018-10-15 19:39:19.775263: step 546, loss = 2.97 (36.8 examples/sec; 3.482 sec/batch)
2018-10-15 19:39:23.154663: step 547, loss = 2.87 (37.9 examples/sec; 3.375 sec/batch)
2018-10-15 19:39:26.519328: step 548, loss = 2.82 (38.1 examples/sec; 3.360 sec/batch)
2018-10-15 19:39:30.102692: step 549, loss = 2.78 (35.7 examples/sec; 3.581 sec/batch)
2018-10-15 19:39:33.515103: step 550, loss = 2.99 (37.6 examples/sec; 3.407 sec/batch)
2018-10-15 19:39:37.071386: step 551, loss = 2.88 (36.0 examples/sec; 3.552 sec/batch)
2018-10-15 19:39:40.485132: step 552, loss = 2.90 (37.5 examples/sec; 3.409 sec/batch)
2018-10-15 19:39:43.911293: step 553, loss = 2.79 (37.4 examples/sec; 3.421 sec/batch)
2018-10-15 19:39:47.459079: step 554, loss = 2.83 (36.1 examples/sec; 3.543 sec/batch)
2018-10-15 19:39:50.862729: step 555, loss = 2.86 (37.7 examples/sec; 3.398 sec/batch)
2018-10-15 19:39:54.206715: step 556, loss = 2.92 (38.3 examples/sec; 3.339 sec/batch)
2018-10-15 19:39:57.763372: step 557, loss = 2.93 (36.0 examples/sec; 3.552 sec/batch)
2018-10-15 19:40:01.219078: step 558, loss = 2.89 (37.1 examples/sec; 3.453 sec/batch)
2018-10-15 19:40:04.711484: step 559, loss = 2.63 (36.7 examples/sec; 3.487 sec/batch)
2018-10-15 19:40:08.128776: step 560, loss = 2.86 (37.5 examples/sec; 3.412 sec/batch)
2018-10-15 19:40:11.481044: step 561, loss = 2.87 (38.2 examples/sec; 3.347 sec/batch)
2018-10-15 19:40:14.876097: step 562, loss = 2.75 (37.8 examples/sec; 3.391 sec/batch)
2018-10-15 19:40:18.362076: step 563, loss = 2.95 (36.8 examples/sec; 3.483 sec/batch)
2018-10-15 19:40:21.814184: step 564, loss = 2.76 (37.1 examples/sec; 3.447 sec/batch)
2018-10-15 19:40:25.215634: step 565, loss = 2.71 (37.7 examples/sec; 3.397 sec/batch)
2018-10-15 19:40:28.622672: step 566, loss = 2.70 (37.6 examples/sec; 3.402 sec/batch)
2018-10-15 19:40:32.071033: step 567, loss = 2.77 (37.2 examples/sec; 3.444 sec/batch)
2018-10-15 19:40:35.564326: step 568, loss = 2.67 (36.7 examples/sec; 3.488 sec/batch)
2018-10-15 19:40:39.032938: step 569, loss = 2.77 (37.0 examples/sec; 3.463 sec/batch)
2018-10-15 19:40:42.426169: step 570, loss = 2.72 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:40:45.898553: step 571, loss = 2.66 (36.9 examples/sec; 3.467 sec/batch)
2018-10-15 19:40:49.403182: step 572, loss = 2.75 (36.6 examples/sec; 3.499 sec/batch)
2018-10-15 19:40:52.773963: step 573, loss = 2.73 (38.0 examples/sec; 3.366 sec/batch)
2018-10-15 19:40:56.211059: step 574, loss = 2.80 (37.3 examples/sec; 3.433 sec/batch)
2018-10-15 19:40:59.607351: step 575, loss = 2.78 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:41:03.128651: step 576, loss = 2.66 (36.4 examples/sec; 3.516 sec/batch)
2018-10-15 19:41:06.556749: step 577, loss = 2.86 (37.4 examples/sec; 3.423 sec/batch)
2018-10-15 19:41:09.982297: step 578, loss = 2.90 (37.4 examples/sec; 3.421 sec/batch)
2018-10-15 19:41:13.448427: step 579, loss = 2.85 (37.0 examples/sec; 3.461 sec/batch)
2018-10-15 19:41:16.794721: step 580, loss = 2.77 (38.3 examples/sec; 3.342 sec/batch)
2018-10-15 19:41:20.128396: step 581, loss = 2.95 (38.5 examples/sec; 3.329 sec/batch)
2018-10-15 19:41:23.566550: step 582, loss = 2.68 (37.3 examples/sec; 3.433 sec/batch)
2018-10-15 19:41:27.066593: step 583, loss = 2.86 (36.6 examples/sec; 3.495 sec/batch)
2018-10-15 19:41:30.471459: step 584, loss = 2.78 (37.6 examples/sec; 3.401 sec/batch)
2018-10-15 19:41:34.039120: step 585, loss = 2.76 (35.9 examples/sec; 3.563 sec/batch)
2018-10-15 19:41:37.517558: step 586, loss = 2.81 (36.9 examples/sec; 3.474 sec/batch)
2018-10-15 19:41:41.019279: step 587, loss = 2.84 (36.6 examples/sec; 3.497 sec/batch)
2018-10-15 19:41:44.443085: step 588, loss = 2.78 (37.4 examples/sec; 3.419 sec/batch)
2018-10-15 19:41:47.878211: step 589, loss = 3.03 (37.3 examples/sec; 3.430 sec/batch)
2018-10-15 19:41:51.378990: step 590, loss = 2.79 (36.6 examples/sec; 3.498 sec/batch)
2018-10-15 19:41:54.917349: step 591, loss = 3.02 (36.2 examples/sec; 3.533 sec/batch)
2018-10-15 19:41:58.313676: step 592, loss = 2.71 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:42:01.630020: step 593, loss = 2.86 (38.7 examples/sec; 3.311 sec/batch)
2018-10-15 19:42:05.017142: step 594, loss = 2.84 (37.8 examples/sec; 3.382 sec/batch)
2018-10-15 19:42:08.404584: step 595, loss = 2.79 (37.8 examples/sec; 3.383 sec/batch)
2018-10-15 19:42:11.876447: step 596, loss = 2.84 (36.9 examples/sec; 3.469 sec/batch)
2018-10-15 19:42:15.261147: step 597, loss = 2.77 (37.9 examples/sec; 3.379 sec/batch)
2018-10-15 19:42:18.625297: step 598, loss = 2.92 (38.1 examples/sec; 3.360 sec/batch)
2018-10-15 19:42:22.286709: step 599, loss = 2.72 (35.0 examples/sec; 3.659 sec/batch)
2018-10-15 19:42:25.681828: step 600, loss = 2.80 (37.7 examples/sec; 3.391 sec/batch)
2018-10-15 19:42:29.725181: step 601, loss = 2.83 (36.2 examples/sec; 3.539 sec/batch)
2018-10-15 19:42:33.096096: step 602, loss = 2.74 (38.0 examples/sec; 3.368 sec/batch)
2018-10-15 19:42:36.415099: step 603, loss = 2.77 (38.6 examples/sec; 3.314 sec/batch)
2018-10-15 19:42:39.809710: step 604, loss = 2.78 (37.8 examples/sec; 3.390 sec/batch)
2018-10-15 19:42:43.255704: step 605, loss = 3.03 (37.2 examples/sec; 3.441 sec/batch)
2018-10-15 19:42:46.755388: step 606, loss = 2.79 (36.6 examples/sec; 3.495 sec/batch)
2018-10-15 19:42:50.326843: step 607, loss = 2.92 (35.9 examples/sec; 3.567 sec/batch)
2018-10-15 19:42:53.750321: step 608, loss = 2.65 (37.4 examples/sec; 3.419 sec/batch)
Terminated
jayan@node-0:~/alexnet$ ls
1_node  2_nodes  4__32_node.csv  4_nodes  4_nodes_1.csv  AlexNet  cluster_utils.py  host  serverlog-0.out  serverlog-1.out  serverlog-ps-0.out  startserver.py  startservers.sh
[1]-  Terminated              python startserver.py -deploy_mode cluster2 -job_name ps --task_index 0
[2]+  Terminated              python startserver.py -deploy_mode cluster2 -job_name worker --task_index 0
jayan@node-0:~/alexnet$
jayan@node-0:~/alexnet$ ls
1_node  2_nodes  4__32_node.csv  4_nodes  4_nodes_1.csv  AlexNet  cluster_utils.py  host  serverlog-0.out  serverlog-1.out  serverlog-ps-0.out  startserver.py  startservers.sh
jayan@node-0:~/alexnet$ vi 2_nodes/
jayan@node-0:~/alexnet$ cd 2_nodes/
jayan@node-0:~/alexnet/2_nodes$ ls
2_nodes_1.csv  2_nodes.csv  serverlog-2-servers.txt
jayan@node-0:~/alexnet/2_nodes$ vi server_2_32.log
jayan@node-0:~/alexnet/2_nodes$ cd ..
jayan@node-0:~/alexnet$ python startserver.py -deploy_mode cluster -job_name ps --task_index 0 & python startserver.py -deploy_mode cluster -job_name worker --task_index 0 &
[1] 37131
[2] 37132
jayan@node-0:~/alexnet$
jayan@node-0:~/alexnet$ 2018-10-15 19:46:10.720463: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2018-10-15 19:46:10.724277: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2223}
2018-10-15 19:46:10.724295: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> node-0:2222, 1 -> node-1:2222}
2018-10-15 19:46:10.726444: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2223
2018-10-15 19:46:10.729688: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2018-10-15 19:46:10.734929: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> node-0:2223}
2018-10-15 19:46:10.734959: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2222, 1 -> node-1:2222}
2018-10-15 19:46:10.738521: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2222

jayan@node-0:~/alexnet$
jayan@node-0:~/alexnet$ python -m AlexNet.scripts.train --mode cluster --dataset flowers --batch_num 10000
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/input.py:187: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
Tensor("Size:0", shape=(), dtype=int32, device=/job:ps/task:0)
Tensor("Size_1:0", shape=(), dtype=int32, device=/job:ps/task:0)
Input batch shape: images: (64, 256, 256, 3) labels: (64,)
num_classes: 5
total_num_examples: 640000
flowers_data/parallel_read/filenames
flowers_data/parallel_read/common_queue
flowers_data/batch/fifo_queue
2018-10-15 19:47:07.129622: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session 17dade53b8ee2d6f with config: allow_soft_placement: true
WARNING:tensorflow:From /users/jayan/alexnet/AlexNet/scripts/train.py:160: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
44 threads started for queue
2018-10-15 19:47:14.055072: step 0, loss = 4.68 (14.7 examples/sec; 4.353 sec/batch)
2018-10-15 19:47:17.935112: step 1, loss = 6.09 (19.5 examples/sec; 3.286 sec/batch)
2018-10-15 19:47:20.722689: step 2, loss = 6.17 (23.0 examples/sec; 2.783 sec/batch)
2018-10-15 19:47:23.606117: step 3, loss = 6.84 (22.2 examples/sec; 2.879 sec/batch)
2018-10-15 19:47:26.311237: step 4, loss = 9.16 (23.7 examples/sec; 2.700 sec/batch)
2018-10-15 19:47:29.072761: step 5, loss = 9.09 (23.2 examples/sec; 2.758 sec/batch)
2018-10-15 19:47:31.683034: step 6, loss = 6.37 (24.6 examples/sec; 2.605 sec/batch)
2018-10-15 19:47:34.411623: step 7, loss = 5.85 (23.5 examples/sec; 2.726 sec/batch)
2018-10-15 19:47:37.165662: step 8, loss = 6.04 (23.3 examples/sec; 2.747 sec/batch)
2018-10-15 19:47:39.941172: step 9, loss = 7.00 (23.1 examples/sec; 2.770 sec/batch)
2018-10-15 19:47:42.751949: step 10, loss = 6.09 (22.8 examples/sec; 2.805 sec/batch)
2018-10-15 19:47:45.511942: step 11, loss = 5.52 (23.2 examples/sec; 2.755 sec/batch)
2018-10-15 19:47:48.300649: step 12, loss = 5.15 (23.0 examples/sec; 2.784 sec/batch)
2018-10-15 19:47:51.094310: step 13, loss = 5.36 (23.0 examples/sec; 2.788 sec/batch)
2018-10-15 19:47:53.971749: step 14, loss = 3.90 (22.3 examples/sec; 2.872 sec/batch)
2018-10-15 19:47:56.787101: step 15, loss = 4.38 (22.8 examples/sec; 2.812 sec/batch)
2018-10-15 19:47:59.814769: step 16, loss = 4.19 (21.2 examples/sec; 3.024 sec/batch)
2018-10-15 19:48:02.501315: step 17, loss = 3.90 (23.9 examples/sec; 2.680 sec/batch)
2018-10-15 19:48:05.204084: step 18, loss = 4.79 (23.7 examples/sec; 2.698 sec/batch)
2018-10-15 19:48:07.988955: step 19, loss = 3.86 (23.0 examples/sec; 2.777 sec/batch)
2018-10-15 19:48:10.824563: step 20, loss = 4.28 (22.6 examples/sec; 2.831 sec/batch)
2018-10-15 19:48:13.737344: step 21, loss = 4.09 (22.0 examples/sec; 2.907 sec/batch)
2018-10-15 19:48:16.856437: step 22, loss = 4.61 (20.5 examples/sec; 3.116 sec/batch)
2018-10-15 19:48:19.637971: step 23, loss = 4.39 (23.0 examples/sec; 2.779 sec/batch)
2018-10-15 19:48:22.067660: step 24, loss = 3.43 (26.4 examples/sec; 2.425 sec/batch)
2018-10-15 19:48:24.434450: step 25, loss = 3.97 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 19:48:26.756891: step 26, loss = 3.93 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 19:48:29.015042: step 27, loss = 3.95 (28.4 examples/sec; 2.253 sec/batch)
2018-10-15 19:48:31.342075: step 28, loss = 3.83 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 19:48:33.678429: step 29, loss = 3.91 (27.4 examples/sec; 2.332 sec/batch)
2018-10-15 19:48:35.998550: step 30, loss = 4.40 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 19:48:38.304193: step 31, loss = 3.80 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 19:48:40.566687: step 32, loss = 3.89 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 19:48:42.944354: step 33, loss = 4.21 (27.0 examples/sec; 2.373 sec/batch)
2018-10-15 19:48:45.268221: step 34, loss = 3.91 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 19:48:47.658832: step 35, loss = 3.57 (26.8 examples/sec; 2.385 sec/batch)
2018-10-15 19:48:50.028913: step 36, loss = 3.84 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 19:48:52.287580: step 37, loss = 3.93 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 19:48:54.637332: step 38, loss = 3.62 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 19:48:56.997424: step 39, loss = 3.53 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 19:48:59.464375: step 40, loss = 4.12 (26.0 examples/sec; 2.462 sec/batch)
2018-10-15 19:49:01.790441: step 41, loss = 4.35 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 19:49:04.045921: step 42, loss = 4.10 (28.4 examples/sec; 2.250 sec/batch)
2018-10-15 19:49:06.457777: step 43, loss = 3.57 (26.6 examples/sec; 2.407 sec/batch)
2018-10-15 19:49:08.877993: step 44, loss = 3.53 (26.5 examples/sec; 2.417 sec/batch)
2018-10-15 19:49:11.173558: step 45, loss = 3.77 (27.9 examples/sec; 2.291 sec/batch)
2018-10-15 19:49:13.493935: step 46, loss = 3.68 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 19:49:15.850688: step 47, loss = 3.71 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 19:49:18.317897: step 48, loss = 3.39 (26.0 examples/sec; 2.463 sec/batch)
2018-10-15 19:49:20.569107: step 49, loss = 3.50 (28.5 examples/sec; 2.246 sec/batch)
2018-10-15 19:49:22.849669: step 50, loss = 3.82 (28.1 examples/sec; 2.276 sec/batch)
2018-10-15 19:49:25.221555: step 51, loss = 3.83 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 19:49:27.629922: step 52, loss = 3.64 (26.6 examples/sec; 2.406 sec/batch)
2018-10-15 19:49:30.029999: step 53, loss = 3.51 (26.7 examples/sec; 2.395 sec/batch)
2018-10-15 19:49:32.341408: step 54, loss = 3.38 (27.7 examples/sec; 2.306 sec/batch)
2018-10-15 19:49:34.613700: step 55, loss = 3.66 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 19:49:36.972049: step 56, loss = 3.61 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 19:49:39.387336: step 57, loss = 3.74 (26.5 examples/sec; 2.413 sec/batch)
2018-10-15 19:49:41.737314: step 58, loss = 3.47 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 19:49:43.993862: step 59, loss = 3.45 (28.4 examples/sec; 2.252 sec/batch)
2018-10-15 19:49:46.319946: step 60, loss = 3.54 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 19:49:48.801559: step 61, loss = 3.52 (25.8 examples/sec; 2.477 sec/batch)
2018-10-15 19:49:51.062819: step 62, loss = 3.62 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 19:49:53.355201: step 63, loss = 3.47 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 19:49:55.808730: step 64, loss = 3.40 (26.1 examples/sec; 2.449 sec/batch)
2018-10-15 19:49:58.159064: step 65, loss = 3.55 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 19:50:00.433418: step 66, loss = 3.51 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 19:50:02.760149: step 67, loss = 3.58 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 19:50:05.265379: step 68, loss = 3.54 (25.6 examples/sec; 2.501 sec/batch)
2018-10-15 19:50:07.576966: step 69, loss = 3.42 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 19:50:09.800846: step 70, loss = 3.37 (28.8 examples/sec; 2.219 sec/batch)
2018-10-15 19:50:12.185642: step 71, loss = 3.44 (26.9 examples/sec; 2.380 sec/batch)
2018-10-15 19:50:14.614747: step 72, loss = 3.67 (26.4 examples/sec; 2.424 sec/batch)
2018-10-15 19:50:17.034797: step 73, loss = 3.52 (26.5 examples/sec; 2.415 sec/batch)
2018-10-15 19:50:19.430436: step 74, loss = 3.48 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 19:50:21.843702: step 75, loss = 3.45 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 19:50:24.305334: step 76, loss = 3.23 (26.0 examples/sec; 2.457 sec/batch)
2018-10-15 19:50:26.688171: step 77, loss = 3.27 (26.9 examples/sec; 2.378 sec/batch)
2018-10-15 19:50:28.947559: step 78, loss = 3.27 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 19:50:31.420086: step 79, loss = 3.28 (25.9 examples/sec; 2.468 sec/batch)
2018-10-15 19:50:33.974089: step 80, loss = 3.36 (25.1 examples/sec; 2.545 sec/batch)
2018-10-15 19:50:36.290562: step 81, loss = 3.35 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 19:50:38.658095: step 82, loss = 3.22 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 19:50:41.055617: step 83, loss = 3.32 (26.8 examples/sec; 2.392 sec/batch)
2018-10-15 19:50:43.461980: step 84, loss = 3.21 (26.6 examples/sec; 2.402 sec/batch)
2018-10-15 19:50:45.785129: step 85, loss = 3.23 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 19:50:48.147132: step 86, loss = 3.01 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 19:50:50.489808: step 87, loss = 3.44 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 19:50:52.830086: step 88, loss = 3.42 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 19:50:55.150668: step 89, loss = 3.38 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 19:50:57.379698: step 90, loss = 3.09 (28.8 examples/sec; 2.224 sec/batch)
2018-10-15 19:50:59.804186: step 91, loss = 3.10 (26.5 examples/sec; 2.420 sec/batch)
2018-10-15 19:51:02.198681: step 92, loss = 3.29 (26.8 examples/sec; 2.390 sec/batch)
2018-10-15 19:51:04.522826: step 93, loss = 3.13 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 19:51:06.940175: step 94, loss = 3.33 (26.5 examples/sec; 2.412 sec/batch)
2018-10-15 19:51:09.429347: step 95, loss = 3.27 (25.8 examples/sec; 2.484 sec/batch)
2018-10-15 19:51:11.833608: step 96, loss = 3.33 (26.7 examples/sec; 2.399 sec/batch)
2018-10-15 19:51:14.069869: step 97, loss = 3.07 (28.7 examples/sec; 2.231 sec/batch)
2018-10-15 19:51:16.485257: step 98, loss = 3.27 (26.6 examples/sec; 2.410 sec/batch)
2018-10-15 19:51:18.959279: step 99, loss = 3.27 (25.9 examples/sec; 2.470 sec/batch)
2018-10-15 19:51:21.305969: step 100, loss = 3.18 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 19:51:24.090432: step 101, loss = 3.18 (27.0 examples/sec; 2.366 sec/batch)
2018-10-15 19:51:26.623708: step 102, loss = 3.22 (25.3 examples/sec; 2.528 sec/batch)
2018-10-15 19:51:29.069158: step 103, loss = 3.09 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 19:51:31.384127: step 104, loss = 3.44 (27.7 examples/sec; 2.311 sec/batch)
2018-10-15 19:51:33.736972: step 105, loss = 3.27 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 19:51:36.116130: step 106, loss = 3.32 (27.0 examples/sec; 2.374 sec/batch)
2018-10-15 19:51:38.377876: step 107, loss = 3.55 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 19:51:40.811515: step 108, loss = 3.35 (26.4 examples/sec; 2.428 sec/batch)
2018-10-15 19:51:43.348175: step 109, loss = 3.27 (25.3 examples/sec; 2.532 sec/batch)
2018-10-15 19:51:45.764484: step 110, loss = 3.38 (26.5 examples/sec; 2.412 sec/batch)
2018-10-15 19:51:48.112322: step 111, loss = 3.49 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 19:51:50.590397: step 112, loss = 3.34 (25.9 examples/sec; 2.473 sec/batch)
2018-10-15 19:51:53.004568: step 113, loss = 3.17 (26.6 examples/sec; 2.409 sec/batch)
2018-10-15 19:51:55.302950: step 114, loss = 3.18 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 19:51:57.616795: step 115, loss = 3.25 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 19:52:00.053743: step 116, loss = 3.35 (26.3 examples/sec; 2.432 sec/batch)
2018-10-15 19:52:02.492853: step 117, loss = 3.25 (26.3 examples/sec; 2.434 sec/batch)
2018-10-15 19:52:04.817672: step 118, loss = 3.18 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 19:52:07.167403: step 119, loss = 3.13 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 19:52:09.654578: step 120, loss = 3.49 (25.8 examples/sec; 2.483 sec/batch)
2018-10-15 19:52:12.004919: step 121, loss = 3.18 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 19:52:14.271615: step 122, loss = 3.15 (28.3 examples/sec; 2.263 sec/batch)
2018-10-15 19:52:16.672995: step 123, loss = 3.09 (26.7 examples/sec; 2.397 sec/batch)
2018-10-15 19:52:19.286594: step 124, loss = 3.29 (24.5 examples/sec; 2.609 sec/batch)
2018-10-15 19:52:21.654524: step 125, loss = 3.10 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 19:52:24.045215: step 126, loss = 3.15 (26.8 examples/sec; 2.386 sec/batch)
2018-10-15 19:52:26.509466: step 127, loss = 3.31 (26.0 examples/sec; 2.460 sec/batch)
2018-10-15 19:52:28.897583: step 128, loss = 3.06 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 19:52:31.230699: step 129, loss = 3.21 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 19:52:33.564030: step 130, loss = 3.17 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 19:52:36.099137: step 131, loss = 3.16 (25.3 examples/sec; 2.530 sec/batch)
2018-10-15 19:52:38.542292: step 132, loss = 3.07 (26.2 examples/sec; 2.438 sec/batch)
2018-10-15 19:52:40.817271: step 133, loss = 3.14 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 19:52:43.182917: step 134, loss = 3.27 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 19:52:45.648321: step 135, loss = 3.34 (26.0 examples/sec; 2.461 sec/batch)
2018-10-15 19:52:48.021927: step 136, loss = 3.25 (27.0 examples/sec; 2.371 sec/batch)
2018-10-15 19:52:50.336749: step 137, loss = 3.23 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 19:52:52.839741: step 138, loss = 3.08 (25.6 examples/sec; 2.498 sec/batch)
2018-10-15 19:52:55.293067: step 139, loss = 3.15 (26.1 examples/sec; 2.448 sec/batch)
2018-10-15 19:52:57.605565: step 140, loss = 3.35 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 19:52:59.885489: step 141, loss = 3.26 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 19:53:02.418412: step 142, loss = 3.00 (25.3 examples/sec; 2.528 sec/batch)
2018-10-15 19:53:04.669920: step 143, loss = 3.31 (28.5 examples/sec; 2.247 sec/batch)
2018-10-15 19:53:07.008288: step 144, loss = 3.05 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 19:53:09.386521: step 145, loss = 3.11 (27.0 examples/sec; 2.373 sec/batch)
2018-10-15 19:53:11.749020: step 146, loss = 3.22 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 19:53:14.193332: step 147, loss = 3.06 (26.2 examples/sec; 2.440 sec/batch)
2018-10-15 19:53:16.506575: step 148, loss = 3.42 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 19:53:19.061232: step 149, loss = 3.35 (25.1 examples/sec; 2.550 sec/batch)
2018-10-15 19:53:21.472180: step 150, loss = 3.18 (26.6 examples/sec; 2.406 sec/batch)
2018-10-15 19:53:23.785714: step 151, loss = 3.03 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 19:53:26.106440: step 152, loss = 3.23 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 19:53:28.549314: step 153, loss = 3.11 (26.3 examples/sec; 2.438 sec/batch)
2018-10-15 19:53:30.913585: step 154, loss = 3.10 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 19:53:33.249765: step 155, loss = 3.03 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 19:53:35.754029: step 156, loss = 3.09 (25.6 examples/sec; 2.500 sec/batch)
2018-10-15 19:53:38.172525: step 157, loss = 2.98 (26.5 examples/sec; 2.414 sec/batch)
2018-10-15 19:53:40.477737: step 158, loss = 3.04 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 19:53:42.867071: step 159, loss = 2.98 (26.8 examples/sec; 2.385 sec/batch)
2018-10-15 19:53:45.218864: step 160, loss = 3.24 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 19:53:47.525256: step 161, loss = 2.88 (27.8 examples/sec; 2.302 sec/batch)
2018-10-15 19:53:49.856688: step 162, loss = 3.08 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 19:53:52.321999: step 163, loss = 3.03 (26.0 examples/sec; 2.461 sec/batch)
2018-10-15 19:53:54.613682: step 164, loss = 3.27 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 19:53:56.896676: step 165, loss = 3.41 (28.1 examples/sec; 2.278 sec/batch)
2018-10-15 19:53:59.171927: step 166, loss = 3.20 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 19:54:01.589104: step 167, loss = 3.22 (26.5 examples/sec; 2.411 sec/batch)
2018-10-15 19:54:04.007770: step 168, loss = 2.96 (26.5 examples/sec; 2.413 sec/batch)
2018-10-15 19:54:06.300725: step 169, loss = 3.08 (28.0 examples/sec; 2.288 sec/batch)
2018-10-15 19:54:08.605558: step 170, loss = 2.98 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 19:54:11.028936: step 171, loss = 3.01 (26.5 examples/sec; 2.419 sec/batch)
2018-10-15 19:54:13.436601: step 172, loss = 3.10 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 19:54:15.706729: step 173, loss = 3.04 (28.2 examples/sec; 2.267 sec/batch)
2018-10-15 19:54:18.081451: step 174, loss = 3.35 (27.0 examples/sec; 2.370 sec/batch)
2018-10-15 19:54:20.520591: step 175, loss = 3.10 (26.3 examples/sec; 2.434 sec/batch)
2018-10-15 19:54:22.819990: step 176, loss = 3.20 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 19:54:25.163575: step 177, loss = 3.18 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 19:54:27.608616: step 178, loss = 3.10 (26.2 examples/sec; 2.440 sec/batch)
2018-10-15 19:54:29.968824: step 179, loss = 3.05 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 19:54:32.253893: step 180, loss = 3.19 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 19:54:34.517421: step 181, loss = 2.98 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 19:54:36.970212: step 182, loss = 3.07 (26.1 examples/sec; 2.450 sec/batch)
2018-10-15 19:54:39.260119: step 183, loss = 2.97 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 19:54:41.559718: step 184, loss = 2.95 (27.9 examples/sec; 2.298 sec/batch)
2018-10-15 19:54:43.803942: step 185, loss = 3.15 (28.6 examples/sec; 2.239 sec/batch)
2018-10-15 19:54:46.253478: step 186, loss = 3.19 (26.2 examples/sec; 2.445 sec/batch)
2018-10-15 19:54:48.657739: step 187, loss = 3.13 (26.7 examples/sec; 2.400 sec/batch)
2018-10-15 19:54:51.015917: step 188, loss = 3.24 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 19:54:53.345323: step 189, loss = 3.00 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 19:54:55.792666: step 190, loss = 3.10 (26.2 examples/sec; 2.443 sec/batch)
2018-10-15 19:54:58.239312: step 191, loss = 3.31 (26.2 examples/sec; 2.442 sec/batch)
2018-10-15 19:55:00.542944: step 192, loss = 3.23 (27.8 examples/sec; 2.299 sec/batch)
2018-10-15 19:55:02.892033: step 193, loss = 3.07 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 19:55:05.191020: step 194, loss = 3.04 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 19:55:07.656618: step 195, loss = 3.15 (26.0 examples/sec; 2.460 sec/batch)
2018-10-15 19:55:09.942257: step 196, loss = 3.12 (28.1 examples/sec; 2.281 sec/batch)
2018-10-15 19:55:12.276275: step 197, loss = 3.19 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 19:55:14.722963: step 198, loss = 3.03 (26.2 examples/sec; 2.442 sec/batch)
2018-10-15 19:55:17.063564: step 199, loss = 3.00 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 19:55:19.355298: step 200, loss = 3.04 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 19:55:22.287348: step 201, loss = 3.03 (25.8 examples/sec; 2.483 sec/batch)
2018-10-15 19:55:24.643979: step 202, loss = 3.18 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 19:55:26.895999: step 203, loss = 3.27 (28.5 examples/sec; 2.249 sec/batch)
2018-10-15 19:55:29.192078: step 204, loss = 3.12 (27.9 examples/sec; 2.291 sec/batch)
2018-10-15 19:55:31.694294: step 205, loss = 3.25 (25.6 examples/sec; 2.497 sec/batch)
2018-10-15 19:55:34.051866: step 206, loss = 3.04 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 19:55:36.331647: step 207, loss = 3.07 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 19:55:38.670375: step 208, loss = 3.30 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 19:55:41.089700: step 209, loss = 3.24 (26.5 examples/sec; 2.415 sec/batch)
2018-10-15 19:55:43.476314: step 210, loss = 3.27 (26.9 examples/sec; 2.382 sec/batch)
2018-10-15 19:55:45.782444: step 211, loss = 3.09 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 19:55:48.192727: step 212, loss = 3.04 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 19:55:50.458747: step 213, loss = 3.04 (28.3 examples/sec; 2.261 sec/batch)
2018-10-15 19:55:52.739177: step 214, loss = 3.10 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 19:55:55.009722: step 215, loss = 3.01 (28.2 examples/sec; 2.266 sec/batch)
2018-10-15 19:55:57.352810: step 216, loss = 3.07 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 19:55:59.799881: step 217, loss = 2.88 (26.2 examples/sec; 2.442 sec/batch)
2018-10-15 19:56:02.083014: step 218, loss = 3.02 (28.1 examples/sec; 2.278 sec/batch)
2018-10-15 19:56:04.365075: step 219, loss = 2.93 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 19:56:06.916417: step 220, loss = 2.92 (25.1 examples/sec; 2.546 sec/batch)
2018-10-15 19:56:09.230452: step 221, loss = 2.98 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 19:56:11.538732: step 222, loss = 3.13 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 19:56:14.042098: step 223, loss = 3.07 (25.6 examples/sec; 2.500 sec/batch)
2018-10-15 19:56:16.492894: step 224, loss = 3.26 (26.1 examples/sec; 2.448 sec/batch)
2018-10-15 19:56:18.762916: step 225, loss = 3.06 (28.3 examples/sec; 2.265 sec/batch)
2018-10-15 19:56:21.019466: step 226, loss = 3.23 (28.4 examples/sec; 2.252 sec/batch)
2018-10-15 19:56:23.495394: step 227, loss = 3.36 (25.9 examples/sec; 2.471 sec/batch)
2018-10-15 19:56:25.929984: step 228, loss = 3.19 (26.3 examples/sec; 2.432 sec/batch)
2018-10-15 19:56:28.213392: step 229, loss = 3.05 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 19:56:30.598080: step 230, loss = 3.16 (26.9 examples/sec; 2.380 sec/batch)
2018-10-15 19:56:33.051256: step 231, loss = 3.13 (26.1 examples/sec; 2.448 sec/batch)
2018-10-15 19:56:35.378550: step 232, loss = 3.16 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 19:56:37.671897: step 233, loss = 2.98 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 19:56:40.041129: step 234, loss = 2.90 (27.0 examples/sec; 2.366 sec/batch)
2018-10-15 19:56:42.455380: step 235, loss = 2.99 (26.6 examples/sec; 2.409 sec/batch)
2018-10-15 19:56:44.743708: step 236, loss = 3.05 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 19:56:47.056639: step 237, loss = 2.94 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 19:56:49.568133: step 238, loss = 3.01 (25.5 examples/sec; 2.506 sec/batch)
2018-10-15 19:56:52.027503: step 239, loss = 3.06 (26.1 examples/sec; 2.455 sec/batch)
2018-10-15 19:56:54.345626: step 240, loss = 2.81 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 19:56:56.833006: step 241, loss = 3.05 (25.8 examples/sec; 2.482 sec/batch)
2018-10-15 19:56:59.260409: step 242, loss = 3.16 (26.4 examples/sec; 2.423 sec/batch)
2018-10-15 19:57:01.549682: step 243, loss = 2.94 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 19:57:03.887326: step 244, loss = 2.96 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 19:57:06.277973: step 245, loss = 2.95 (26.8 examples/sec; 2.385 sec/batch)
2018-10-15 19:57:08.634278: step 246, loss = 2.94 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 19:57:10.913824: step 247, loss = 2.98 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 19:57:13.297659: step 248, loss = 3.08 (26.9 examples/sec; 2.381 sec/batch)
2018-10-15 19:57:15.857169: step 249, loss = 3.30 (25.1 examples/sec; 2.554 sec/batch)
2018-10-15 19:57:18.167687: step 250, loss = 3.17 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 19:57:20.365819: step 251, loss = 3.11 (29.2 examples/sec; 2.193 sec/batch)
2018-10-15 19:57:22.790781: step 252, loss = 3.32 (26.4 examples/sec; 2.420 sec/batch)
2018-10-15 19:57:25.204114: step 253, loss = 3.27 (26.6 examples/sec; 2.410 sec/batch)
2018-10-15 19:57:27.446201: step 254, loss = 2.98 (28.6 examples/sec; 2.236 sec/batch)
2018-10-15 19:57:29.718617: step 255, loss = 3.12 (28.2 examples/sec; 2.268 sec/batch)
2018-10-15 19:57:32.052981: step 256, loss = 3.13 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 19:57:34.458229: step 257, loss = 3.16 (26.7 examples/sec; 2.400 sec/batch)
2018-10-15 19:57:36.811241: step 258, loss = 3.08 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 19:57:39.154707: step 259, loss = 3.25 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 19:57:41.543209: step 260, loss = 3.32 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 19:57:43.939387: step 261, loss = 3.30 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 19:57:46.268402: step 262, loss = 3.12 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 19:57:48.658547: step 263, loss = 3.14 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 19:57:51.169288: step 264, loss = 3.11 (25.5 examples/sec; 2.508 sec/batch)
2018-10-15 19:57:53.530522: step 265, loss = 3.12 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 19:57:55.938394: step 266, loss = 3.15 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 19:57:58.369354: step 267, loss = 2.97 (26.4 examples/sec; 2.426 sec/batch)
2018-10-15 19:58:00.784440: step 268, loss = 3.06 (26.5 examples/sec; 2.412 sec/batch)
2018-10-15 19:58:03.224982: step 269, loss = 3.07 (26.3 examples/sec; 2.435 sec/batch)
2018-10-15 19:58:05.668655: step 270, loss = 3.13 (26.2 examples/sec; 2.439 sec/batch)
2018-10-15 19:58:08.116288: step 271, loss = 3.03 (26.2 examples/sec; 2.443 sec/batch)
2018-10-15 19:58:10.477876: step 272, loss = 3.07 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 19:58:12.811569: step 273, loss = 3.10 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 19:58:15.277054: step 274, loss = 2.93 (26.0 examples/sec; 2.461 sec/batch)
2018-10-15 19:58:17.671230: step 275, loss = 3.07 (26.8 examples/sec; 2.390 sec/batch)
2018-10-15 19:58:19.975088: step 276, loss = 2.91 (27.8 examples/sec; 2.299 sec/batch)
2018-10-15 19:58:22.268145: step 277, loss = 2.95 (28.0 examples/sec; 2.288 sec/batch)
2018-10-15 19:58:24.785266: step 278, loss = 2.88 (25.5 examples/sec; 2.512 sec/batch)
2018-10-15 19:58:27.238429: step 279, loss = 2.94 (26.1 examples/sec; 2.448 sec/batch)
2018-10-15 19:58:29.559630: step 280, loss = 3.19 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 19:58:32.061550: step 281, loss = 3.27 (25.6 examples/sec; 2.497 sec/batch)
2018-10-15 19:58:34.454219: step 282, loss = 3.18 (26.8 examples/sec; 2.388 sec/batch)
2018-10-15 19:58:36.797026: step 283, loss = 3.24 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 19:58:39.232212: step 284, loss = 3.00 (26.3 examples/sec; 2.431 sec/batch)
2018-10-15 19:58:41.671723: step 285, loss = 3.04 (26.3 examples/sec; 2.435 sec/batch)
2018-10-15 19:58:44.068145: step 286, loss = 3.01 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 19:58:46.419349: step 287, loss = 3.02 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 19:58:48.901892: step 288, loss = 3.01 (25.8 examples/sec; 2.480 sec/batch)
2018-10-15 19:58:51.456065: step 289, loss = 2.91 (25.1 examples/sec; 2.549 sec/batch)
2018-10-15 19:58:53.753380: step 290, loss = 2.92 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 19:58:56.152695: step 291, loss = 3.02 (26.7 examples/sec; 2.398 sec/batch)
2018-10-15 19:58:58.557890: step 292, loss = 2.91 (26.7 examples/sec; 2.400 sec/batch)
2018-10-15 19:59:00.903125: step 293, loss = 2.94 (27.3 examples/sec; 2.340 sec/batch)
2018-10-15 19:59:03.302046: step 294, loss = 3.07 (26.7 examples/sec; 2.394 sec/batch)
2018-10-15 19:59:05.771524: step 295, loss = 2.98 (26.0 examples/sec; 2.465 sec/batch)
2018-10-15 19:59:08.085822: step 296, loss = 2.84 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 19:59:10.479517: step 297, loss = 2.86 (26.8 examples/sec; 2.389 sec/batch)
2018-10-15 19:59:13.079802: step 298, loss = 3.03 (24.7 examples/sec; 2.595 sec/batch)
2018-10-15 19:59:15.432343: step 299, loss = 2.94 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 19:59:17.763706: step 300, loss = 2.85 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 19:59:20.705218: step 301, loss = 2.98 (26.0 examples/sec; 2.463 sec/batch)
2018-10-15 19:59:23.124106: step 302, loss = 2.92 (26.5 examples/sec; 2.416 sec/batch)
2018-10-15 19:59:25.468058: step 303, loss = 2.67 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 19:59:27.815027: step 304, loss = 3.03 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 19:59:30.267275: step 305, loss = 2.81 (26.1 examples/sec; 2.448 sec/batch)
2018-10-15 19:59:32.617296: step 306, loss = 2.93 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 19:59:34.887737: step 307, loss = 2.86 (28.2 examples/sec; 2.268 sec/batch)
2018-10-15 19:59:37.342811: step 308, loss = 3.02 (26.1 examples/sec; 2.452 sec/batch)
2018-10-15 19:59:39.768910: step 309, loss = 2.99 (26.4 examples/sec; 2.421 sec/batch)
2018-10-15 19:59:42.057354: step 310, loss = 2.94 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 19:59:44.400358: step 311, loss = 2.88 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 19:59:46.796487: step 312, loss = 2.70 (26.8 examples/sec; 2.392 sec/batch)
2018-10-15 19:59:49.240473: step 313, loss = 3.09 (26.2 examples/sec; 2.439 sec/batch)
2018-10-15 19:59:51.609554: step 314, loss = 3.26 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 19:59:53.855630: step 315, loss = 2.84 (28.6 examples/sec; 2.241 sec/batch)
2018-10-15 19:59:56.327766: step 316, loss = 3.10 (25.9 examples/sec; 2.467 sec/batch)
2018-10-15 19:59:58.661997: step 317, loss = 2.97 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 20:00:01.019661: step 318, loss = 3.26 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 20:00:03.305321: step 319, loss = 2.88 (28.1 examples/sec; 2.281 sec/batch)
2018-10-15 20:00:05.697666: step 320, loss = 2.78 (26.8 examples/sec; 2.387 sec/batch)
2018-10-15 20:00:08.158366: step 321, loss = 3.09 (26.0 examples/sec; 2.458 sec/batch)
2018-10-15 20:00:10.448041: step 322, loss = 2.95 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 20:00:12.855147: step 323, loss = 3.00 (26.6 examples/sec; 2.402 sec/batch)
2018-10-15 20:00:15.281998: step 324, loss = 2.89 (26.4 examples/sec; 2.424 sec/batch)
2018-10-15 20:00:17.583837: step 325, loss = 3.17 (27.9 examples/sec; 2.297 sec/batch)
2018-10-15 20:00:19.896358: step 326, loss = 3.21 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 20:00:22.215086: step 327, loss = 2.86 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 20:00:24.776571: step 328, loss = 2.88 (25.0 examples/sec; 2.557 sec/batch)
2018-10-15 20:00:27.015076: step 329, loss = 3.03 (28.7 examples/sec; 2.234 sec/batch)
2018-10-15 20:00:29.304002: step 330, loss = 3.28 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 20:00:31.653163: step 331, loss = 2.98 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 20:00:34.020804: step 332, loss = 3.08 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 20:00:36.373992: step 333, loss = 2.93 (27.3 examples/sec; 2.349 sec/batch)
2018-10-15 20:00:38.736962: step 334, loss = 2.94 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 20:00:41.088854: step 335, loss = 3.11 (27.2 examples/sec; 2.349 sec/batch)
2018-10-15 20:00:43.563747: step 336, loss = 3.13 (25.9 examples/sec; 2.470 sec/batch)
2018-10-15 20:00:46.143188: step 337, loss = 3.02 (24.9 examples/sec; 2.575 sec/batch)
2018-10-15 20:00:48.473738: step 338, loss = 3.11 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 20:00:50.730139: step 339, loss = 3.13 (28.4 examples/sec; 2.251 sec/batch)
2018-10-15 20:00:53.141982: step 340, loss = 2.95 (26.6 examples/sec; 2.407 sec/batch)
2018-10-15 20:00:55.607589: step 341, loss = 2.95 (26.0 examples/sec; 2.461 sec/batch)
2018-10-15 20:00:57.946913: step 342, loss = 2.97 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 20:01:00.227067: step 343, loss = 3.24 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 20:01:02.558729: step 344, loss = 3.07 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 20:01:05.045205: step 345, loss = 3.03 (25.8 examples/sec; 2.483 sec/batch)
2018-10-15 20:01:07.408892: step 346, loss = 3.05 (27.1 examples/sec; 2.359 sec/batch)
2018-10-15 20:01:09.734222: step 347, loss = 3.24 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 20:01:12.147202: step 348, loss = 3.26 (26.6 examples/sec; 2.408 sec/batch)
2018-10-15 20:01:14.575993: step 349, loss = 3.12 (26.4 examples/sec; 2.424 sec/batch)
2018-10-15 20:01:16.879969: step 350, loss = 3.15 (27.8 examples/sec; 2.299 sec/batch)
2018-10-15 20:01:19.142535: step 351, loss = 3.06 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 20:01:21.475675: step 352, loss = 3.00 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 20:01:23.948738: step 353, loss = 3.18 (25.9 examples/sec; 2.468 sec/batch)
2018-10-15 20:01:26.213427: step 354, loss = 3.10 (28.3 examples/sec; 2.260 sec/batch)
2018-10-15 20:01:28.541533: step 355, loss = 3.02 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 20:01:30.987546: step 356, loss = 2.96 (26.2 examples/sec; 2.441 sec/batch)
2018-10-15 20:01:33.285035: step 357, loss = 3.05 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 20:01:35.615258: step 358, loss = 3.01 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 20:01:37.945490: step 359, loss = 3.08 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 20:01:40.436299: step 360, loss = 2.85 (25.7 examples/sec; 2.488 sec/batch)
2018-10-15 20:01:42.860310: step 361, loss = 2.98 (26.4 examples/sec; 2.421 sec/batch)
2018-10-15 20:01:45.194325: step 362, loss = 2.92 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 20:01:47.603945: step 363, loss = 2.92 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 20:01:50.023338: step 364, loss = 3.03 (26.5 examples/sec; 2.414 sec/batch)
2018-10-15 20:01:52.282587: step 365, loss = 2.90 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 20:01:54.500410: step 366, loss = 3.13 (28.9 examples/sec; 2.214 sec/batch)
2018-10-15 20:01:56.842938: step 367, loss = 2.85 (27.4 examples/sec; 2.338 sec/batch)
2018-10-15 20:01:59.344243: step 368, loss = 2.99 (25.6 examples/sec; 2.496 sec/batch)
2018-10-15 20:02:01.659128: step 369, loss = 3.08 (27.7 examples/sec; 2.311 sec/batch)
2018-10-15 20:02:03.952568: step 370, loss = 2.93 (28.0 examples/sec; 2.288 sec/batch)
2018-10-15 20:02:06.237929: step 371, loss = 3.10 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 20:02:08.747791: step 372, loss = 3.41 (25.5 examples/sec; 2.505 sec/batch)
2018-10-15 20:02:11.101231: step 373, loss = 3.13 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 20:02:13.392625: step 374, loss = 3.17 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 20:02:15.913440: step 375, loss = 3.03 (25.4 examples/sec; 2.516 sec/batch)
2018-10-15 20:02:18.332843: step 376, loss = 3.10 (26.5 examples/sec; 2.415 sec/batch)
2018-10-15 20:02:20.607531: step 377, loss = 3.06 (28.2 examples/sec; 2.271 sec/batch)
2018-10-15 20:02:22.867208: step 378, loss = 3.23 (28.4 examples/sec; 2.255 sec/batch)
2018-10-15 20:02:25.331145: step 379, loss = 3.06 (26.0 examples/sec; 2.461 sec/batch)
2018-10-15 20:02:27.650543: step 380, loss = 3.00 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 20:02:29.953077: step 381, loss = 3.19 (27.9 examples/sec; 2.298 sec/batch)
2018-10-15 20:02:32.311580: step 382, loss = 2.86 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 20:02:34.818834: step 383, loss = 2.99 (25.6 examples/sec; 2.502 sec/batch)
2018-10-15 20:02:37.183951: step 384, loss = 3.19 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 20:02:39.478547: step 385, loss = 3.04 (27.9 examples/sec; 2.290 sec/batch)
2018-10-15 20:02:41.984219: step 386, loss = 2.98 (25.6 examples/sec; 2.500 sec/batch)
2018-10-15 20:02:44.352958: step 387, loss = 2.96 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 20:02:46.648229: step 388, loss = 2.87 (27.9 examples/sec; 2.290 sec/batch)
2018-10-15 20:02:48.956548: step 389, loss = 2.83 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 20:02:51.414174: step 390, loss = 2.79 (26.1 examples/sec; 2.452 sec/batch)
2018-10-15 20:02:53.677302: step 391, loss = 3.01 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 20:02:55.936722: step 392, loss = 3.02 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 20:02:58.396971: step 393, loss = 3.05 (26.1 examples/sec; 2.455 sec/batch)
2018-10-15 20:03:00.746606: step 394, loss = 2.77 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 20:03:03.070062: step 395, loss = 3.08 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 20:03:05.358854: step 396, loss = 3.26 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 20:03:07.820972: step 397, loss = 3.00 (26.0 examples/sec; 2.457 sec/batch)
2018-10-15 20:03:10.139066: step 398, loss = 2.92 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 20:03:12.479018: step 399, loss = 2.84 (27.4 examples/sec; 2.335 sec/batch)
2018-10-15 20:03:14.797607: step 400, loss = 2.78 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 20:03:17.772061: step 401, loss = 3.16 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 20:03:20.119999: step 402, loss = 3.10 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 20:03:22.416727: step 403, loss = 3.01 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 20:03:24.833085: step 404, loss = 2.88 (26.5 examples/sec; 2.414 sec/batch)
2018-10-15 20:03:27.135162: step 405, loss = 2.88 (27.9 examples/sec; 2.297 sec/batch)
2018-10-15 20:03:29.491963: step 406, loss = 3.03 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 20:03:31.826821: step 407, loss = 3.07 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 20:03:34.226321: step 408, loss = 2.96 (26.7 examples/sec; 2.395 sec/batch)
2018-10-15 20:03:36.612552: step 409, loss = 2.87 (26.9 examples/sec; 2.382 sec/batch)
2018-10-15 20:03:39.084969: step 410, loss = 3.06 (25.9 examples/sec; 2.467 sec/batch)
2018-10-15 20:03:41.430295: step 411, loss = 2.99 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 20:03:43.817121: step 412, loss = 3.12 (26.9 examples/sec; 2.382 sec/batch)
2018-10-15 20:03:46.275042: step 413, loss = 3.05 (26.1 examples/sec; 2.453 sec/batch)
2018-10-15 20:03:48.575796: step 414, loss = 3.10 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 20:03:50.892403: step 415, loss = 3.03 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 20:03:53.228317: step 416, loss = 3.02 (27.5 examples/sec; 2.331 sec/batch)
2018-10-15 20:03:55.526960: step 417, loss = 2.88 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 20:03:57.847240: step 418, loss = 3.08 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 20:04:00.167490: step 419, loss = 3.11 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 20:04:02.720872: step 420, loss = 3.01 (25.1 examples/sec; 2.551 sec/batch)
2018-10-15 20:04:05.065910: step 421, loss = 3.05 (27.3 examples/sec; 2.340 sec/batch)
2018-10-15 20:04:07.383169: step 422, loss = 2.97 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 20:04:09.731891: step 423, loss = 2.87 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 20:04:12.122187: step 424, loss = 2.90 (26.8 examples/sec; 2.386 sec/batch)
2018-10-15 20:04:14.421139: step 425, loss = 2.79 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 20:04:16.657426: step 426, loss = 2.92 (28.7 examples/sec; 2.233 sec/batch)
2018-10-15 20:04:18.977870: step 427, loss = 2.90 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 20:04:21.286370: step 428, loss = 2.81 (27.8 examples/sec; 2.304 sec/batch)
2018-10-15 20:04:23.674168: step 429, loss = 2.80 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 20:04:25.927319: step 430, loss = 2.96 (28.4 examples/sec; 2.250 sec/batch)
2018-10-15 20:04:28.252997: step 431, loss = 2.93 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 20:04:30.695908: step 432, loss = 2.89 (26.2 examples/sec; 2.439 sec/batch)
2018-10-15 20:04:32.968648: step 433, loss = 2.89 (28.2 examples/sec; 2.267 sec/batch)
2018-10-15 20:04:35.272665: step 434, loss = 3.01 (27.8 examples/sec; 2.299 sec/batch)
2018-10-15 20:04:37.563387: step 435, loss = 3.04 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 20:04:39.928229: step 436, loss = 3.05 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 20:04:42.284111: step 437, loss = 3.03 (27.2 examples/sec; 2.351 sec/batch)
2018-10-15 20:04:44.553251: step 438, loss = 2.83 (28.3 examples/sec; 2.264 sec/batch)
2018-10-15 20:04:46.794272: step 439, loss = 2.97 (28.6 examples/sec; 2.236 sec/batch)
2018-10-15 20:04:49.219487: step 440, loss = 3.00 (26.4 examples/sec; 2.420 sec/batch)
2018-10-15 20:04:51.650556: step 441, loss = 3.00 (26.4 examples/sec; 2.425 sec/batch)
2018-10-15 20:04:53.994891: step 442, loss = 2.90 (27.4 examples/sec; 2.339 sec/batch)
2018-10-15 20:04:56.237459: step 443, loss = 2.86 (28.6 examples/sec; 2.237 sec/batch)
2018-10-15 20:04:58.557119: step 444, loss = 2.78 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 20:05:00.977680: step 445, loss = 2.89 (26.5 examples/sec; 2.416 sec/batch)
2018-10-15 20:05:03.254302: step 446, loss = 3.13 (28.2 examples/sec; 2.272 sec/batch)
2018-10-15 20:05:05.602542: step 447, loss = 2.89 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 20:05:08.079999: step 448, loss = 3.00 (25.9 examples/sec; 2.473 sec/batch)
2018-10-15 20:05:10.449604: step 449, loss = 2.96 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 20:05:12.765229: step 450, loss = 2.78 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 20:05:15.060399: step 451, loss = 2.98 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 20:05:17.546419: step 452, loss = 2.89 (25.8 examples/sec; 2.481 sec/batch)
2018-10-15 20:05:19.976722: step 453, loss = 2.83 (26.4 examples/sec; 2.425 sec/batch)
2018-10-15 20:05:22.252692: step 454, loss = 3.01 (28.2 examples/sec; 2.271 sec/batch)
2018-10-15 20:05:24.646242: step 455, loss = 2.93 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 20:05:27.023388: step 456, loss = 2.92 (27.0 examples/sec; 2.373 sec/batch)
2018-10-15 20:05:29.335444: step 457, loss = 3.02 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 20:05:31.596664: step 458, loss = 3.00 (28.4 examples/sec; 2.256 sec/batch)
2018-10-15 20:05:34.071962: step 459, loss = 3.03 (25.9 examples/sec; 2.471 sec/batch)
2018-10-15 20:05:36.427531: step 460, loss = 2.97 (27.2 examples/sec; 2.351 sec/batch)
2018-10-15 20:05:38.777242: step 461, loss = 3.13 (27.3 examples/sec; 2.345 sec/batch)
2018-10-15 20:05:41.056548: step 462, loss = 2.89 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 20:05:43.413265: step 463, loss = 3.04 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 20:05:45.843932: step 464, loss = 2.91 (26.4 examples/sec; 2.426 sec/batch)
2018-10-15 20:05:48.255575: step 465, loss = 2.89 (26.6 examples/sec; 2.406 sec/batch)
2018-10-15 20:05:50.496935: step 466, loss = 3.08 (28.6 examples/sec; 2.238 sec/batch)
2018-10-15 20:05:52.812766: step 467, loss = 3.11 (27.7 examples/sec; 2.311 sec/batch)
2018-10-15 20:05:55.215549: step 468, loss = 3.04 (26.7 examples/sec; 2.400 sec/batch)
2018-10-15 20:05:57.478910: step 469, loss = 2.97 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 20:05:59.900734: step 470, loss = 2.93 (26.5 examples/sec; 2.417 sec/batch)
2018-10-15 20:06:02.323956: step 471, loss = 2.92 (26.5 examples/sec; 2.418 sec/batch)
2018-10-15 20:06:04.850904: step 472, loss = 2.93 (25.4 examples/sec; 2.522 sec/batch)
2018-10-15 20:06:07.203718: step 473, loss = 2.89 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 20:06:09.636025: step 474, loss = 2.97 (26.4 examples/sec; 2.428 sec/batch)
2018-10-15 20:06:11.968492: step 475, loss = 2.96 (27.5 examples/sec; 2.328 sec/batch)
2018-10-15 20:06:14.424788: step 476, loss = 2.88 (26.1 examples/sec; 2.451 sec/batch)
2018-10-15 20:06:16.770033: step 477, loss = 2.87 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 20:06:19.177839: step 478, loss = 2.94 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 20:06:21.661497: step 479, loss = 3.18 (25.8 examples/sec; 2.479 sec/batch)
2018-10-15 20:06:24.071652: step 480, loss = 2.82 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 20:06:26.467156: step 481, loss = 2.93 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 20:06:29.014411: step 482, loss = 2.91 (25.2 examples/sec; 2.542 sec/batch)
2018-10-15 20:06:31.439081: step 483, loss = 2.86 (26.5 examples/sec; 2.419 sec/batch)
2018-10-15 20:06:33.766953: step 484, loss = 2.80 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 20:06:36.237471: step 485, loss = 2.70 (26.0 examples/sec; 2.466 sec/batch)
2018-10-15 20:06:38.764866: step 486, loss = 2.88 (25.4 examples/sec; 2.523 sec/batch)
2018-10-15 20:06:41.185179: step 487, loss = 2.88 (26.5 examples/sec; 2.416 sec/batch)
2018-10-15 20:06:43.614770: step 488, loss = 3.02 (26.4 examples/sec; 2.425 sec/batch)
2018-10-15 20:06:46.026384: step 489, loss = 2.78 (26.6 examples/sec; 2.407 sec/batch)
2018-10-15 20:06:48.379437: step 490, loss = 2.82 (27.2 examples/sec; 2.349 sec/batch)
2018-10-15 20:06:50.726130: step 491, loss = 3.01 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 20:06:53.089332: step 492, loss = 2.96 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 20:06:55.545092: step 493, loss = 2.91 (26.1 examples/sec; 2.451 sec/batch)
2018-10-15 20:06:57.830113: step 494, loss = 2.94 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 20:07:00.101865: step 495, loss = 2.92 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 20:07:02.520117: step 496, loss = 2.75 (26.5 examples/sec; 2.415 sec/batch)
2018-10-15 20:07:05.096122: step 497, loss = 2.84 (24.9 examples/sec; 2.571 sec/batch)
2018-10-15 20:07:07.463239: step 498, loss = 2.94 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 20:07:09.797636: step 499, loss = 2.83 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 20:07:12.423815: step 500, loss = 2.67 (24.4 examples/sec; 2.621 sec/batch)
2018-10-15 20:07:15.305881: step 501, loss = 2.83 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 20:07:17.643604: step 502, loss = 2.92 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 20:07:20.129309: step 503, loss = 2.61 (25.8 examples/sec; 2.483 sec/batch)
2018-10-15 20:07:22.613339: step 504, loss = 2.78 (25.8 examples/sec; 2.479 sec/batch)
2018-10-15 20:07:24.925935: step 505, loss = 2.81 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 20:07:27.252029: step 506, loss = 2.73 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 20:07:29.775559: step 507, loss = 2.83 (25.4 examples/sec; 2.519 sec/batch)
2018-10-15 20:07:32.091813: step 508, loss = 2.82 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 20:07:34.413126: step 509, loss = 2.73 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 20:07:36.791871: step 510, loss = 2.84 (26.9 examples/sec; 2.375 sec/batch)
2018-10-15 20:07:39.259645: step 511, loss = 2.85 (26.0 examples/sec; 2.463 sec/batch)
2018-10-15 20:07:41.673416: step 512, loss = 2.90 (26.6 examples/sec; 2.409 sec/batch)
2018-10-15 20:07:44.030811: step 513, loss = 2.77 (27.2 examples/sec; 2.352 sec/batch)
2018-10-15 20:07:46.621803: step 514, loss = 2.80 (24.7 examples/sec; 2.586 sec/batch)
2018-10-15 20:07:48.932595: step 515, loss = 2.77 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 20:07:51.278694: step 516, loss = 2.90 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 20:07:53.696999: step 517, loss = 2.73 (26.5 examples/sec; 2.413 sec/batch)
2018-10-15 20:07:56.141544: step 518, loss = 2.71 (26.2 examples/sec; 2.439 sec/batch)
2018-10-15 20:07:58.507944: step 519, loss = 2.85 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 20:08:00.883478: step 520, loss = 2.94 (27.0 examples/sec; 2.371 sec/batch)
2018-10-15 20:08:03.318155: step 521, loss = 2.81 (26.3 examples/sec; 2.430 sec/batch)
2018-10-15 20:08:05.745954: step 522, loss = 2.94 (26.4 examples/sec; 2.423 sec/batch)
2018-10-15 20:08:08.003421: step 523, loss = 3.04 (28.4 examples/sec; 2.252 sec/batch)
2018-10-15 20:08:10.431645: step 524, loss = 2.79 (26.4 examples/sec; 2.423 sec/batch)
2018-10-15 20:08:12.813474: step 525, loss = 2.69 (26.9 examples/sec; 2.377 sec/batch)
2018-10-15 20:08:15.242703: step 526, loss = 2.85 (26.4 examples/sec; 2.424 sec/batch)
2018-10-15 20:08:17.498370: step 527, loss = 2.80 (28.4 examples/sec; 2.250 sec/batch)
2018-10-15 20:08:19.760539: step 528, loss = 2.92 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 20:08:22.171226: step 529, loss = 2.89 (26.6 examples/sec; 2.406 sec/batch)
2018-10-15 20:08:24.501547: step 530, loss = 3.03 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 20:08:27.004625: step 531, loss = 2.79 (25.6 examples/sec; 2.498 sec/batch)
2018-10-15 20:08:29.363100: step 532, loss = 2.84 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 20:08:31.756117: step 533, loss = 3.15 (26.8 examples/sec; 2.388 sec/batch)
2018-10-15 20:08:34.150816: step 534, loss = 2.92 (26.8 examples/sec; 2.390 sec/batch)
2018-10-15 20:08:36.437459: step 535, loss = 2.93 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 20:08:38.810846: step 536, loss = 2.94 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 20:08:41.239832: step 537, loss = 2.67 (26.4 examples/sec; 2.424 sec/batch)
2018-10-15 20:08:43.561102: step 538, loss = 2.83 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 20:08:45.840781: step 539, loss = 3.01 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 20:08:48.222973: step 540, loss = 2.80 (26.9 examples/sec; 2.378 sec/batch)
2018-10-15 20:08:50.613269: step 541, loss = 2.81 (26.8 examples/sec; 2.386 sec/batch)
2018-10-15 20:08:53.036523: step 542, loss = 2.82 (26.5 examples/sec; 2.418 sec/batch)
2018-10-15 20:08:55.437067: step 543, loss = 3.00 (26.7 examples/sec; 2.396 sec/batch)
2018-10-15 20:08:57.806516: step 544, loss = 2.94 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 20:09:00.225662: step 545, loss = 3.00 (26.5 examples/sec; 2.414 sec/batch)
2018-10-15 20:09:02.593185: step 546, loss = 2.90 (27.1 examples/sec; 2.362 sec/batch)
2018-10-15 20:09:04.959714: step 547, loss = 2.80 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 20:09:07.335739: step 548, loss = 2.89 (27.0 examples/sec; 2.371 sec/batch)
2018-10-15 20:09:09.639516: step 549, loss = 2.86 (27.8 examples/sec; 2.298 sec/batch)
2018-10-15 20:09:11.957577: step 550, loss = 2.91 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 20:09:14.273022: step 551, loss = 3.19 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 20:09:16.623919: step 552, loss = 3.10 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 20:09:19.090112: step 553, loss = 3.12 (26.0 examples/sec; 2.461 sec/batch)
2018-10-15 20:09:21.468431: step 554, loss = 2.92 (27.0 examples/sec; 2.373 sec/batch)
2018-10-15 20:09:23.789347: step 555, loss = 2.73 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 20:09:26.207214: step 556, loss = 2.89 (26.5 examples/sec; 2.412 sec/batch)
2018-10-15 20:09:28.571632: step 557, loss = 2.95 (27.1 examples/sec; 2.360 sec/batch)
2018-10-15 20:09:30.896794: step 558, loss = 2.91 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 20:09:33.318974: step 559, loss = 2.95 (26.5 examples/sec; 2.417 sec/batch)
2018-10-15 20:09:35.727373: step 560, loss = 2.90 (26.6 examples/sec; 2.404 sec/batch)
2018-10-15 20:09:38.039159: step 561, loss = 2.75 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 20:09:40.353930: step 562, loss = 2.75 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 20:09:42.812756: step 563, loss = 2.92 (26.1 examples/sec; 2.453 sec/batch)
2018-10-15 20:09:45.213280: step 564, loss = 2.95 (26.7 examples/sec; 2.399 sec/batch)
2018-10-15 20:09:47.662640: step 565, loss = 2.76 (26.2 examples/sec; 2.445 sec/batch)
2018-10-15 20:09:50.045022: step 566, loss = 2.81 (26.9 examples/sec; 2.378 sec/batch)
2018-10-15 20:09:52.395761: step 567, loss = 2.84 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 20:09:54.951976: step 568, loss = 2.69 (25.1 examples/sec; 2.551 sec/batch)
2018-10-15 20:09:57.308101: step 569, loss = 2.82 (27.2 examples/sec; 2.351 sec/batch)
2018-10-15 20:09:59.576141: step 570, loss = 2.76 (28.3 examples/sec; 2.264 sec/batch)
2018-10-15 20:10:01.952057: step 571, loss = 2.78 (27.0 examples/sec; 2.371 sec/batch)
2018-10-15 20:10:04.520692: step 572, loss = 2.85 (24.9 examples/sec; 2.566 sec/batch)
2018-10-15 20:10:06.895151: step 573, loss = 2.90 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 20:10:09.225775: step 574, loss = 2.83 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 20:10:11.565016: step 575, loss = 2.64 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 20:10:13.973498: step 576, loss = 2.96 (26.6 examples/sec; 2.404 sec/batch)
2018-10-15 20:10:16.425838: step 577, loss = 2.80 (26.2 examples/sec; 2.447 sec/batch)
2018-10-15 20:10:18.754452: step 578, loss = 2.79 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 20:10:21.101150: step 579, loss = 2.80 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 20:10:23.598370: step 580, loss = 2.72 (25.7 examples/sec; 2.494 sec/batch)
2018-10-15 20:10:25.998482: step 581, loss = 2.97 (26.7 examples/sec; 2.395 sec/batch)
2018-10-15 20:10:28.404654: step 582, loss = 2.64 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 20:10:30.789868: step 583, loss = 3.01 (26.9 examples/sec; 2.381 sec/batch)
2018-10-15 20:10:33.234707: step 584, loss = 2.89 (26.2 examples/sec; 2.440 sec/batch)
2018-10-15 20:10:35.673978: step 585, loss = 2.69 (26.3 examples/sec; 2.434 sec/batch)
2018-10-15 20:10:38.018603: step 586, loss = 2.97 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 20:10:40.246396: step 587, loss = 2.93 (28.8 examples/sec; 2.223 sec/batch)
2018-10-15 20:10:42.709130: step 588, loss = 2.78 (26.0 examples/sec; 2.460 sec/batch)
2018-10-15 20:10:45.101560: step 589, loss = 2.85 (26.8 examples/sec; 2.389 sec/batch)
2018-10-15 20:10:47.393428: step 590, loss = 2.88 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 20:10:49.675522: step 591, loss = 2.85 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 20:10:52.153557: step 592, loss = 2.77 (25.9 examples/sec; 2.473 sec/batch)
2018-10-15 20:10:54.609295: step 593, loss = 2.68 (26.1 examples/sec; 2.453 sec/batch)
2018-10-15 20:10:56.890657: step 594, loss = 2.79 (28.1 examples/sec; 2.276 sec/batch)
2018-10-15 20:10:59.310975: step 595, loss = 2.82 (26.5 examples/sec; 2.416 sec/batch)
2018-10-15 20:11:01.677202: step 596, loss = 2.70 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 20:11:04.018618: step 597, loss = 2.85 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 20:11:06.270102: step 598, loss = 2.85 (28.5 examples/sec; 2.246 sec/batch)
2018-10-15 20:11:08.546458: step 599, loss = 2.91 (28.2 examples/sec; 2.271 sec/batch)
2018-10-15 20:11:10.870618: step 600, loss = 2.72 (27.6 examples/sec; 2.321 sec/batch)
2018-10-15 20:11:13.661924: step 601, loss = 2.65 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 20:11:15.913917: step 602, loss = 2.91 (28.5 examples/sec; 2.247 sec/batch)
2018-10-15 20:11:18.298248: step 603, loss = 2.91 (26.9 examples/sec; 2.379 sec/batch)
2018-10-15 20:11:20.700390: step 604, loss = 2.93 (26.7 examples/sec; 2.397 sec/batch)
2018-10-15 20:11:22.999128: step 605, loss = 2.88 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 20:11:25.262967: step 606, loss = 2.74 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 20:11:27.744430: step 607, loss = 2.72 (25.9 examples/sec; 2.476 sec/batch)
2018-10-15 20:11:30.143454: step 608, loss = 3.11 (26.7 examples/sec; 2.396 sec/batch)
2018-10-15 20:11:32.453844: step 609, loss = 2.79 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 20:11:34.811207: step 610, loss = 2.87 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 20:11:37.149701: step 611, loss = 2.91 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 20:11:39.387896: step 612, loss = 3.03 (28.7 examples/sec; 2.234 sec/batch)
2018-10-15 20:11:41.735569: step 613, loss = 2.92 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 20:11:44.026682: step 614, loss = 2.98 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 20:11:46.458969: step 615, loss = 2.82 (26.4 examples/sec; 2.428 sec/batch)
2018-10-15 20:11:48.914833: step 616, loss = 2.83 (26.1 examples/sec; 2.450 sec/batch)
2018-10-15 20:11:51.249544: step 617, loss = 2.93 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 20:11:53.616276: step 618, loss = 2.95 (27.1 examples/sec; 2.364 sec/batch)
2018-10-15 20:11:56.101674: step 619, loss = 2.99 (25.8 examples/sec; 2.484 sec/batch)
2018-10-15 20:11:58.513769: step 620, loss = 2.85 (26.6 examples/sec; 2.407 sec/batch)
2018-10-15 20:12:00.855592: step 621, loss = 3.07 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 20:12:03.371945: step 622, loss = 2.99 (25.5 examples/sec; 2.512 sec/batch)
2018-10-15 20:12:05.898600: step 623, loss = 2.86 (25.4 examples/sec; 2.522 sec/batch)
2018-10-15 20:12:08.240076: step 624, loss = 2.98 (27.4 examples/sec; 2.340 sec/batch)
2018-10-15 20:12:10.586926: step 625, loss = 2.70 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 20:12:12.889690: step 626, loss = 2.99 (27.9 examples/sec; 2.298 sec/batch)
2018-10-15 20:12:15.281714: step 627, loss = 2.98 (26.8 examples/sec; 2.389 sec/batch)
2018-10-15 20:12:17.678598: step 628, loss = 2.77 (26.8 examples/sec; 2.392 sec/batch)
2018-10-15 20:12:20.021079: step 629, loss = 2.82 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 20:12:22.661810: step 630, loss = 2.64 (24.3 examples/sec; 2.636 sec/batch)
2018-10-15 20:12:24.930236: step 631, loss = 2.96 (28.3 examples/sec; 2.263 sec/batch)
2018-10-15 20:12:27.275390: step 632, loss = 2.70 (27.3 examples/sec; 2.340 sec/batch)
2018-10-15 20:12:29.549012: step 633, loss = 2.83 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 20:12:31.947119: step 634, loss = 2.73 (26.7 examples/sec; 2.393 sec/batch)
2018-10-15 20:12:34.354645: step 635, loss = 2.79 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 20:12:36.638470: step 636, loss = 2.67 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 20:12:39.041117: step 637, loss = 2.66 (26.7 examples/sec; 2.397 sec/batch)
2018-10-15 20:12:41.386780: step 638, loss = 2.90 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 20:12:43.914372: step 639, loss = 2.98 (25.4 examples/sec; 2.522 sec/batch)
2018-10-15 20:12:46.278379: step 640, loss = 3.04 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 20:12:48.598762: step 641, loss = 2.83 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 20:12:51.071466: step 642, loss = 2.77 (25.9 examples/sec; 2.467 sec/batch)
2018-10-15 20:12:53.528194: step 643, loss = 2.90 (26.1 examples/sec; 2.452 sec/batch)
2018-10-15 20:12:55.879502: step 644, loss = 2.78 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 20:12:58.454128: step 645, loss = 2.89 (24.9 examples/sec; 2.570 sec/batch)
2018-10-15 20:13:00.749239: step 646, loss = 3.11 (27.9 examples/sec; 2.290 sec/batch)
2018-10-15 20:13:03.090604: step 647, loss = 2.65 (27.4 examples/sec; 2.337 sec/batch)
2018-10-15 20:13:05.467251: step 648, loss = 2.76 (27.0 examples/sec; 2.372 sec/batch)
2018-10-15 20:13:07.853679: step 649, loss = 2.63 (26.9 examples/sec; 2.382 sec/batch)
2018-10-15 20:13:10.344138: step 650, loss = 2.68 (25.7 examples/sec; 2.485 sec/batch)
2018-10-15 20:13:12.639133: step 651, loss = 2.79 (27.9 examples/sec; 2.291 sec/batch)
2018-10-15 20:13:15.098839: step 652, loss = 2.82 (26.1 examples/sec; 2.455 sec/batch)
2018-10-15 20:13:17.507050: step 653, loss = 2.60 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 20:13:19.887588: step 654, loss = 2.76 (26.9 examples/sec; 2.375 sec/batch)
2018-10-15 20:13:22.184003: step 655, loss = 2.68 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 20:13:24.580686: step 656, loss = 2.70 (26.7 examples/sec; 2.394 sec/batch)
2018-10-15 20:13:27.003415: step 657, loss = 2.67 (26.5 examples/sec; 2.418 sec/batch)
2018-10-15 20:13:29.335037: step 658, loss = 2.63 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 20:13:31.661590: step 659, loss = 2.77 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 20:13:34.068375: step 660, loss = 2.96 (26.6 examples/sec; 2.402 sec/batch)
2018-10-15 20:13:36.431373: step 661, loss = 2.77 (27.2 examples/sec; 2.355 sec/batch)
2018-10-15 20:13:38.803873: step 662, loss = 2.69 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 20:13:41.093970: step 663, loss = 3.02 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 20:13:43.496366: step 664, loss = 2.77 (26.7 examples/sec; 2.396 sec/batch)
2018-10-15 20:13:45.981330: step 665, loss = 2.84 (25.8 examples/sec; 2.480 sec/batch)
2018-10-15 20:13:48.312833: step 666, loss = 2.74 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 20:13:50.620241: step 667, loss = 2.94 (27.8 examples/sec; 2.302 sec/batch)
2018-10-15 20:13:52.938859: step 668, loss = 2.80 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 20:13:55.268086: step 669, loss = 2.84 (27.5 examples/sec; 2.323 sec/batch)
2018-10-15 20:13:57.528523: step 670, loss = 2.83 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 20:13:59.857819: step 671, loss = 2.74 (27.5 examples/sec; 2.324 sec/batch)
2018-10-15 20:14:02.267488: step 672, loss = 2.90 (26.6 examples/sec; 2.406 sec/batch)
2018-10-15 20:14:04.656222: step 673, loss = 2.85 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 20:14:07.067074: step 674, loss = 2.91 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 20:14:09.395419: step 675, loss = 2.88 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 20:14:11.873606: step 676, loss = 2.93 (25.9 examples/sec; 2.473 sec/batch)
2018-10-15 20:14:14.145675: step 677, loss = 3.00 (28.2 examples/sec; 2.267 sec/batch)
2018-10-15 20:14:16.401170: step 678, loss = 2.72 (28.4 examples/sec; 2.251 sec/batch)
2018-10-15 20:14:18.793491: step 679, loss = 2.77 (26.8 examples/sec; 2.387 sec/batch)
2018-10-15 20:14:21.083477: step 680, loss = 2.76 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 20:14:23.463826: step 681, loss = 2.91 (26.9 examples/sec; 2.375 sec/batch)
2018-10-15 20:14:25.841421: step 682, loss = 2.76 (27.0 examples/sec; 2.372 sec/batch)
2018-10-15 20:14:28.169819: step 683, loss = 3.01 (27.5 examples/sec; 2.326 sec/batch)
2018-10-15 20:14:30.516366: step 684, loss = 3.01 (27.3 examples/sec; 2.342 sec/batch)
2018-10-15 20:14:32.762951: step 685, loss = 2.74 (28.6 examples/sec; 2.241 sec/batch)
2018-10-15 20:14:35.039735: step 686, loss = 2.66 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 20:14:37.465423: step 687, loss = 2.71 (26.4 examples/sec; 2.421 sec/batch)
2018-10-15 20:14:39.904264: step 688, loss = 2.92 (26.3 examples/sec; 2.434 sec/batch)
2018-10-15 20:14:42.164369: step 689, loss = 2.81 (28.4 examples/sec; 2.254 sec/batch)
2018-10-15 20:14:44.438466: step 690, loss = 2.81 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 20:14:46.789007: step 691, loss = 2.81 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 20:14:49.101684: step 692, loss = 2.67 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 20:14:51.319684: step 693, loss = 2.82 (28.9 examples/sec; 2.212 sec/batch)
2018-10-15 20:14:53.616443: step 694, loss = 2.80 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 20:14:55.984802: step 695, loss = 2.93 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 20:14:58.343312: step 696, loss = 3.09 (27.2 examples/sec; 2.353 sec/batch)
2018-10-15 20:15:00.815037: step 697, loss = 2.99 (25.9 examples/sec; 2.467 sec/batch)
2018-10-15 20:15:03.108769: step 698, loss = 2.88 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 20:15:05.518967: step 699, loss = 2.90 (26.6 examples/sec; 2.405 sec/batch)
2018-10-15 20:15:07.887173: step 700, loss = 2.85 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 20:15:10.899220: step 701, loss = 2.69 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 20:15:13.238490: step 702, loss = 2.65 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 20:15:15.838862: step 703, loss = 2.81 (24.7 examples/sec; 2.595 sec/batch)
2018-10-15 20:15:18.099240: step 704, loss = 2.88 (28.4 examples/sec; 2.256 sec/batch)
2018-10-15 20:15:20.453738: step 705, loss = 2.69 (27.2 examples/sec; 2.349 sec/batch)
2018-10-15 20:15:22.820046: step 706, loss = 2.59 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 20:15:25.243132: step 707, loss = 2.72 (26.5 examples/sec; 2.419 sec/batch)
2018-10-15 20:15:27.501134: step 708, loss = 2.65 (28.4 examples/sec; 2.253 sec/batch)
2018-10-15 20:15:29.791691: step 709, loss = 2.43 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 20:15:32.110188: step 710, loss = 2.76 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 20:15:34.465503: step 711, loss = 2.82 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 20:15:36.837376: step 712, loss = 2.67 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 20:15:39.103476: step 713, loss = 2.60 (28.3 examples/sec; 2.262 sec/batch)
2018-10-15 20:15:41.558722: step 714, loss = 2.61 (26.1 examples/sec; 2.452 sec/batch)
2018-10-15 20:15:44.034873: step 715, loss = 2.71 (25.9 examples/sec; 2.471 sec/batch)
2018-10-15 20:15:46.329797: step 716, loss = 2.70 (28.0 examples/sec; 2.290 sec/batch)
2018-10-15 20:15:48.622974: step 717, loss = 2.81 (28.0 examples/sec; 2.288 sec/batch)
2018-10-15 20:15:51.024462: step 718, loss = 2.66 (26.7 examples/sec; 2.397 sec/batch)
2018-10-15 20:15:53.345770: step 719, loss = 2.77 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 20:15:55.684346: step 720, loss = 2.55 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 20:15:58.072581: step 721, loss = 2.64 (26.9 examples/sec; 2.383 sec/batch)
2018-10-15 20:16:00.581294: step 722, loss = 2.91 (25.5 examples/sec; 2.506 sec/batch)
2018-10-15 20:16:02.978666: step 723, loss = 2.66 (26.7 examples/sec; 2.393 sec/batch)
2018-10-15 20:16:05.297555: step 724, loss = 2.54 (27.7 examples/sec; 2.314 sec/batch)
2018-10-15 20:16:07.564519: step 725, loss = 2.66 (28.3 examples/sec; 2.262 sec/batch)
2018-10-15 20:16:09.899026: step 726, loss = 2.64 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 20:16:12.246679: step 727, loss = 2.78 (27.3 examples/sec; 2.343 sec/batch)
2018-10-15 20:16:14.608160: step 728, loss = 2.79 (27.2 examples/sec; 2.356 sec/batch)
2018-10-15 20:16:16.970693: step 729, loss = 2.59 (27.1 examples/sec; 2.359 sec/batch)
2018-10-15 20:16:19.373767: step 730, loss = 2.71 (26.7 examples/sec; 2.397 sec/batch)
2018-10-15 20:16:21.635396: step 731, loss = 2.78 (28.4 examples/sec; 2.257 sec/batch)
2018-10-15 20:16:24.073483: step 732, loss = 2.65 (26.3 examples/sec; 2.433 sec/batch)
2018-10-15 20:16:26.494965: step 733, loss = 2.82 (26.5 examples/sec; 2.419 sec/batch)
2018-10-15 20:16:28.773783: step 734, loss = 2.66 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 20:16:31.064982: step 735, loss = 2.67 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 20:16:33.530236: step 736, loss = 2.63 (26.0 examples/sec; 2.461 sec/batch)
2018-10-15 20:16:35.889674: step 737, loss = 2.79 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 20:16:38.169107: step 738, loss = 2.85 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 20:16:40.430328: step 739, loss = 2.87 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 20:16:42.852168: step 740, loss = 2.97 (26.5 examples/sec; 2.417 sec/batch)
2018-10-15 20:16:45.179941: step 741, loss = 2.71 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 20:16:47.475292: step 742, loss = 2.66 (27.9 examples/sec; 2.290 sec/batch)
2018-10-15 20:16:49.830008: step 743, loss = 2.66 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 20:16:52.368841: step 744, loss = 2.85 (25.3 examples/sec; 2.534 sec/batch)
2018-10-15 20:16:54.660146: step 745, loss = 2.67 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 20:16:57.011014: step 746, loss = 2.90 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 20:16:59.469129: step 747, loss = 2.96 (26.1 examples/sec; 2.453 sec/batch)
2018-10-15 20:17:01.950386: step 748, loss = 2.97 (25.8 examples/sec; 2.476 sec/batch)
2018-10-15 20:17:04.251081: step 749, loss = 2.84 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 20:17:06.556752: step 750, loss = 2.74 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 20:17:08.999498: step 751, loss = 2.81 (26.2 examples/sec; 2.440 sec/batch)
2018-10-15 20:17:11.348715: step 752, loss = 2.84 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 20:17:13.601044: step 753, loss = 2.73 (28.4 examples/sec; 2.250 sec/batch)
2018-10-15 20:17:15.995251: step 754, loss = 2.61 (26.8 examples/sec; 2.389 sec/batch)
2018-10-15 20:17:18.506206: step 755, loss = 2.66 (25.5 examples/sec; 2.506 sec/batch)
2018-10-15 20:17:20.821681: step 756, loss = 2.70 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 20:17:23.156653: step 757, loss = 2.81 (27.5 examples/sec; 2.330 sec/batch)
2018-10-15 20:17:25.535665: step 758, loss = 2.79 (27.0 examples/sec; 2.374 sec/batch)
2018-10-15 20:17:27.791609: step 759, loss = 2.82 (28.4 examples/sec; 2.251 sec/batch)
2018-10-15 20:17:30.074935: step 760, loss = 2.69 (28.1 examples/sec; 2.279 sec/batch)
2018-10-15 20:17:32.412906: step 761, loss = 2.98 (27.4 examples/sec; 2.333 sec/batch)
2018-10-15 20:17:34.771903: step 762, loss = 2.88 (27.2 examples/sec; 2.354 sec/batch)
2018-10-15 20:17:37.143590: step 763, loss = 2.84 (27.0 examples/sec; 2.366 sec/batch)
2018-10-15 20:17:39.518399: step 764, loss = 2.84 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 20:17:41.804712: step 765, loss = 2.85 (28.0 examples/sec; 2.282 sec/batch)
2018-10-15 20:17:44.299857: step 766, loss = 2.80 (25.7 examples/sec; 2.490 sec/batch)
2018-10-15 20:17:46.666939: step 767, loss = 2.94 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 20:17:48.930704: step 768, loss = 2.68 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 20:17:51.331169: step 769, loss = 2.67 (26.7 examples/sec; 2.398 sec/batch)
2018-10-15 20:17:53.684360: step 770, loss = 2.55 (27.2 examples/sec; 2.349 sec/batch)
2018-10-15 20:17:56.123596: step 771, loss = 2.77 (26.3 examples/sec; 2.434 sec/batch)
2018-10-15 20:17:58.385958: step 772, loss = 2.56 (28.3 examples/sec; 2.258 sec/batch)
2018-10-15 20:18:00.725166: step 773, loss = 2.72 (27.4 examples/sec; 2.336 sec/batch)
2018-10-15 20:18:03.104970: step 774, loss = 2.78 (26.9 examples/sec; 2.377 sec/batch)
2018-10-15 20:18:05.443836: step 775, loss = 2.73 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 20:18:07.744060: step 776, loss = 2.90 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 20:18:10.124133: step 777, loss = 2.80 (26.9 examples/sec; 2.375 sec/batch)
2018-10-15 20:18:12.496935: step 778, loss = 2.78 (27.0 examples/sec; 2.368 sec/batch)
2018-10-15 20:18:14.784676: step 779, loss = 2.64 (28.0 examples/sec; 2.283 sec/batch)
2018-10-15 20:18:17.094629: step 780, loss = 2.65 (27.8 examples/sec; 2.305 sec/batch)
2018-10-15 20:18:19.532663: step 781, loss = 2.59 (26.3 examples/sec; 2.433 sec/batch)
2018-10-15 20:18:21.927808: step 782, loss = 2.73 (26.8 examples/sec; 2.392 sec/batch)
2018-10-15 20:18:24.310459: step 783, loss = 2.86 (26.9 examples/sec; 2.378 sec/batch)
2018-10-15 20:18:26.612745: step 784, loss = 2.90 (27.9 examples/sec; 2.298 sec/batch)
2018-10-15 20:18:29.004473: step 785, loss = 2.95 (26.8 examples/sec; 2.387 sec/batch)
2018-10-15 20:18:31.353772: step 786, loss = 2.65 (27.3 examples/sec; 2.344 sec/batch)
2018-10-15 20:18:33.623242: step 787, loss = 2.72 (28.3 examples/sec; 2.265 sec/batch)
2018-10-15 20:18:36.050903: step 788, loss = 2.65 (26.4 examples/sec; 2.424 sec/batch)
2018-10-15 20:18:38.344647: step 789, loss = 2.52 (28.0 examples/sec; 2.289 sec/batch)
2018-10-15 20:18:40.605841: step 790, loss = 2.54 (28.3 examples/sec; 2.259 sec/batch)
2018-10-15 20:18:42.912359: step 791, loss = 2.99 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 20:18:45.227834: step 792, loss = 2.67 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 20:18:47.687598: step 793, loss = 2.92 (26.0 examples/sec; 2.457 sec/batch)
2018-10-15 20:18:50.107778: step 794, loss = 2.72 (26.5 examples/sec; 2.415 sec/batch)
2018-10-15 20:18:52.387229: step 795, loss = 2.67 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 20:18:54.613039: step 796, loss = 2.66 (28.8 examples/sec; 2.221 sec/batch)
2018-10-15 20:18:57.051267: step 797, loss = 2.85 (26.3 examples/sec; 2.433 sec/batch)
2018-10-15 20:18:59.428246: step 798, loss = 2.72 (27.0 examples/sec; 2.374 sec/batch)
2018-10-15 20:19:01.734402: step 799, loss = 2.62 (27.8 examples/sec; 2.301 sec/batch)
2018-10-15 20:19:04.040295: step 800, loss = 2.71 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 20:19:06.808927: step 801, loss = 2.72 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 20:19:09.131126: step 802, loss = 2.54 (27.6 examples/sec; 2.317 sec/batch)
2018-10-15 20:19:11.562459: step 803, loss = 2.66 (26.4 examples/sec; 2.429 sec/batch)
2018-10-15 20:19:13.915473: step 804, loss = 2.63 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 20:19:16.232384: step 805, loss = 2.96 (27.7 examples/sec; 2.312 sec/batch)
2018-10-15 20:19:18.636150: step 806, loss = 2.69 (26.7 examples/sec; 2.399 sec/batch)
2018-10-15 20:19:20.925666: step 807, loss = 2.50 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 20:19:23.235556: step 808, loss = 2.74 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 20:19:25.488218: step 809, loss = 2.64 (28.5 examples/sec; 2.248 sec/batch)
2018-10-15 20:19:27.788087: step 810, loss = 3.14 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 20:19:30.241109: step 811, loss = 2.80 (26.1 examples/sec; 2.450 sec/batch)
2018-10-15 20:19:32.595152: step 812, loss = 2.51 (27.3 examples/sec; 2.348 sec/batch)
2018-10-15 20:19:34.973212: step 813, loss = 2.74 (26.9 examples/sec; 2.375 sec/batch)
2018-10-15 20:19:37.387635: step 814, loss = 2.51 (26.6 examples/sec; 2.409 sec/batch)
2018-10-15 20:19:39.738693: step 815, loss = 2.71 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 20:19:41.963396: step 816, loss = 3.02 (28.8 examples/sec; 2.222 sec/batch)
2018-10-15 20:19:44.293819: step 817, loss = 2.79 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 20:19:46.659822: step 818, loss = 2.92 (27.1 examples/sec; 2.361 sec/batch)
2018-10-15 20:19:48.929910: step 819, loss = 2.73 (28.3 examples/sec; 2.265 sec/batch)
2018-10-15 20:19:51.228488: step 820, loss = 2.63 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 20:19:53.614925: step 821, loss = 2.77 (26.8 examples/sec; 2.384 sec/batch)
2018-10-15 20:19:56.076643: step 822, loss = 2.78 (26.0 examples/sec; 2.457 sec/batch)
2018-10-15 20:19:58.439767: step 823, loss = 2.81 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 20:20:00.747897: step 824, loss = 2.65 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 20:20:03.240960: step 825, loss = 2.73 (25.7 examples/sec; 2.488 sec/batch)
2018-10-15 20:20:05.610608: step 826, loss = 2.85 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 20:20:07.954573: step 827, loss = 2.76 (27.3 examples/sec; 2.341 sec/batch)
2018-10-15 20:20:10.252763: step 828, loss = 2.96 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 20:20:12.727943: step 829, loss = 2.74 (25.9 examples/sec; 2.472 sec/batch)
2018-10-15 20:20:15.029175: step 830, loss = 2.70 (27.9 examples/sec; 2.297 sec/batch)
2018-10-15 20:20:17.303682: step 831, loss = 2.84 (28.2 examples/sec; 2.270 sec/batch)
2018-10-15 20:20:19.587212: step 832, loss = 2.69 (28.1 examples/sec; 2.280 sec/batch)
2018-10-15 20:20:22.057110: step 833, loss = 2.62 (26.0 examples/sec; 2.465 sec/batch)
2018-10-15 20:20:24.318712: step 834, loss = 2.97 (28.4 examples/sec; 2.256 sec/batch)
2018-10-15 20:20:26.586925: step 835, loss = 2.83 (28.3 examples/sec; 2.265 sec/batch)
2018-10-15 20:20:28.886023: step 836, loss = 2.63 (27.9 examples/sec; 2.296 sec/batch)
2018-10-15 20:20:31.235497: step 837, loss = 2.75 (27.3 examples/sec; 2.346 sec/batch)
2018-10-15 20:20:33.565348: step 838, loss = 2.74 (27.5 examples/sec; 2.325 sec/batch)
2018-10-15 20:20:35.844251: step 839, loss = 2.57 (28.1 examples/sec; 2.274 sec/batch)
2018-10-15 20:20:38.122756: step 840, loss = 2.69 (28.2 examples/sec; 2.273 sec/batch)
2018-10-15 20:20:40.527107: step 841, loss = 2.55 (26.6 examples/sec; 2.403 sec/batch)
2018-10-15 20:20:42.890253: step 842, loss = 2.54 (27.1 examples/sec; 2.358 sec/batch)
2018-10-15 20:20:45.144423: step 843, loss = 2.54 (28.5 examples/sec; 2.249 sec/batch)
2018-10-15 20:20:47.398783: step 844, loss = 2.65 (28.5 examples/sec; 2.248 sec/batch)
2018-10-15 20:20:49.760969: step 845, loss = 2.54 (27.2 examples/sec; 2.357 sec/batch)
2018-10-15 20:20:52.075553: step 846, loss = 2.76 (27.7 examples/sec; 2.310 sec/batch)
2018-10-15 20:20:54.366766: step 847, loss = 2.70 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 20:20:56.655151: step 848, loss = 2.87 (28.0 examples/sec; 2.284 sec/batch)
2018-10-15 20:20:59.040623: step 849, loss = 2.67 (26.9 examples/sec; 2.380 sec/batch)
2018-10-15 20:21:01.436340: step 850, loss = 2.74 (26.8 examples/sec; 2.391 sec/batch)
2018-10-15 20:21:03.728445: step 851, loss = 2.81 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 20:21:06.033936: step 852, loss = 2.88 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 20:21:08.512743: step 853, loss = 2.65 (25.9 examples/sec; 2.474 sec/batch)
2018-10-15 20:21:10.837209: step 854, loss = 2.78 (27.6 examples/sec; 2.319 sec/batch)
2018-10-15 20:21:13.133742: step 855, loss = 2.79 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 20:21:15.435172: step 856, loss = 2.84 (27.9 examples/sec; 2.297 sec/batch)
2018-10-15 20:21:17.812278: step 857, loss = 2.87 (27.0 examples/sec; 2.372 sec/batch)
2018-10-15 20:21:20.104838: step 858, loss = 2.66 (28.0 examples/sec; 2.290 sec/batch)
2018-10-15 20:21:22.394551: step 859, loss = 2.59 (28.0 examples/sec; 2.287 sec/batch)
2018-10-15 20:21:24.663801: step 860, loss = 2.65 (28.3 examples/sec; 2.264 sec/batch)
2018-10-15 20:21:27.090309: step 861, loss = 2.58 (26.4 examples/sec; 2.422 sec/batch)
2018-10-15 20:21:29.531044: step 862, loss = 2.57 (26.3 examples/sec; 2.438 sec/batch)
2018-10-15 20:21:31.857762: step 863, loss = 2.69 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 20:21:34.259862: step 864, loss = 2.93 (26.7 examples/sec; 2.397 sec/batch)
2018-10-15 20:21:36.691315: step 865, loss = 2.65 (26.4 examples/sec; 2.426 sec/batch)
2018-10-15 20:21:38.965464: step 866, loss = 2.75 (28.2 examples/sec; 2.269 sec/batch)
2018-10-15 20:21:41.277377: step 867, loss = 2.39 (27.7 examples/sec; 2.307 sec/batch)
2018-10-15 20:21:43.609691: step 868, loss = 2.85 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 20:21:46.063029: step 869, loss = 2.57 (26.1 examples/sec; 2.448 sec/batch)
2018-10-15 20:21:48.331229: step 870, loss = 2.59 (28.3 examples/sec; 2.263 sec/batch)
2018-10-15 20:21:50.737661: step 871, loss = 2.68 (26.7 examples/sec; 2.401 sec/batch)
2018-10-15 20:21:53.179542: step 872, loss = 2.88 (26.3 examples/sec; 2.438 sec/batch)
2018-10-15 20:21:55.468820: step 873, loss = 2.59 (28.0 examples/sec; 2.285 sec/batch)
2018-10-15 20:21:57.716139: step 874, loss = 2.74 (28.5 examples/sec; 2.242 sec/batch)
2018-10-15 20:22:00.015674: step 875, loss = 2.86 (27.9 examples/sec; 2.294 sec/batch)
2018-10-15 20:22:02.333633: step 876, loss = 2.73 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 20:22:04.710713: step 877, loss = 2.67 (27.0 examples/sec; 2.369 sec/batch)
2018-10-15 20:22:07.011558: step 878, loss = 2.69 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 20:22:09.345141: step 879, loss = 2.82 (27.5 examples/sec; 2.329 sec/batch)
2018-10-15 20:22:11.655799: step 880, loss = 2.79 (27.8 examples/sec; 2.306 sec/batch)
2018-10-15 20:22:14.073834: step 881, loss = 2.55 (26.5 examples/sec; 2.413 sec/batch)
2018-10-15 20:22:16.371245: step 882, loss = 2.86 (27.9 examples/sec; 2.292 sec/batch)
2018-10-15 20:22:18.605892: step 883, loss = 2.92 (28.7 examples/sec; 2.230 sec/batch)
2018-10-15 20:22:20.885888: step 884, loss = 2.82 (28.1 examples/sec; 2.275 sec/batch)
2018-10-15 20:22:23.314285: step 885, loss = 2.78 (26.4 examples/sec; 2.423 sec/batch)
2018-10-15 20:22:25.697733: step 886, loss = 2.66 (26.9 examples/sec; 2.379 sec/batch)
2018-10-15 20:22:27.995149: step 887, loss = 2.78 (27.9 examples/sec; 2.293 sec/batch)
2018-10-15 20:22:30.312243: step 888, loss = 2.74 (27.7 examples/sec; 2.313 sec/batch)
2018-10-15 20:22:32.632151: step 889, loss = 2.74 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 20:22:35.169716: step 890, loss = 2.68 (25.3 examples/sec; 2.534 sec/batch)
2018-10-15 20:22:37.460845: step 891, loss = 2.58 (28.0 examples/sec; 2.286 sec/batch)
2018-10-15 20:22:39.705836: step 892, loss = 2.67 (28.6 examples/sec; 2.240 sec/batch)
2018-10-15 20:22:41.974198: step 893, loss = 2.55 (28.2 examples/sec; 2.266 sec/batch)
2018-10-15 20:22:44.489812: step 894, loss = 2.85 (25.5 examples/sec; 2.511 sec/batch)
2018-10-15 20:22:46.814298: step 895, loss = 2.68 (27.6 examples/sec; 2.322 sec/batch)
2018-10-15 20:22:49.122889: step 896, loss = 2.82 (27.8 examples/sec; 2.304 sec/batch)
2018-10-15 20:22:51.443585: step 897, loss = 2.55 (27.6 examples/sec; 2.316 sec/batch)
2018-10-15 20:22:53.877078: step 898, loss = 2.76 (26.3 examples/sec; 2.431 sec/batch)
2018-10-15 20:22:56.197067: step 899, loss = 2.69 (27.6 examples/sec; 2.315 sec/batch)
2018-10-15 20:22:58.511374: step 900, loss = 2.66 (27.7 examples/sec; 2.309 sec/batch)
2018-10-15 20:23:01.337033: step 901, loss = 2.72 (26.9 examples/sec; 2.379 sec/batch)
2018-10-15 20:23:03.726490: step 902, loss = 2.87 (26.8 examples/sec; 2.385 sec/batch)
2018-10-15 20:23:06.065326: step 903, loss = 2.77 (27.4 examples/sec; 2.334 sec/batch)
2018-10-15 20:23:08.364749: step 904, loss = 2.58 (27.9 examples/sec; 2.295 sec/batch)
2018-10-15 20:23:10.696696: step 905, loss = 2.86 (27.5 examples/sec; 2.327 sec/batch)
2018-10-15 20:23:13.051867: step 906, loss = 2.63 (27.2 examples/sec; 2.350 sec/batch)
2018-10-15 20:23:15.375234: step 907, loss = 2.65 (27.6 examples/sec; 2.318 sec/batch)
2018-10-15 20:23:17.680207: step 908, loss = 2.76 (27.8 examples/sec; 2.300 sec/batch)
2018-10-15 20:23:20.177021: step 909, loss = 2.69 (25.7 examples/sec; 2.494 sec/batch)
2018-10-15 20:23:22.547599: step 910, loss = 2.57 (27.1 examples/sec; 2.365 sec/batch)
2018-10-15 20:23:24.855720: step 911, loss = 2.62 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 20:23:27.163356: step 912, loss = 2.69 (27.8 examples/sec; 2.303 sec/batch)
2018-10-15 20:23:29.529554: step 913, loss = 2.81 (27.1 examples/sec; 2.363 sec/batch)
2018-10-15 20:23:31.881008: step 914, loss = 2.59 (27.3 examples/sec; 2.347 sec/batch)
2018-10-15 20:23:34.206350: step 915, loss = 2.70 (27.6 examples/sec; 2.320 sec/batch)
2018-10-15 20:23:36.517945: step 916, loss = 2.53 (27.7 examples/sec; 2.308 sec/batch)
2018-10-15 20:23:38.845728: step 917, loss = 2.71 (27.6 examples/sec; 2.323 sec/batch)
2018-10-15 20:23:41.238441: step 918, loss = 2.56 (26.8 examples/sec; 2.387 sec/batch)
2018-10-15 20:23:43.499145: step 919, loss = 2.55 (28.4 examples/sec; 2.256 sec/batch)
2018-10-15 20:23:45.874325: step 920, loss = 2.53 (27.0 examples/sec; 2.371 sec/batch)
2018-10-15 20:23:48.266795: step 921, loss = 2.78 (26.8 examples/sec; 2.387 sec/batch)
2018-10-15 20:23:50.663082: step 922, loss = 2.65 (26.8 examples/sec; 2.392 sec/batch)
2018-10-15 20:23:52.944993: step 923, loss = 2.61 (28.1 examples/sec; 2.277 sec/batch)
2018-10-15 20:23:55.189989: step 924, loss = 2.57 (28.6 examples/sec; 2.239 sec/batch)
2018-10-15 20:23:57.561573: step 925, loss = 2.62 (27.0 examples/sec; 2.367 sec/batch)
2018-10-15 20:23:59.862474: step 926, loss = 2.52 (27.9 examples/sec; 2.298 sec/batch)
Terminated
